---
title: "Yang-Mills Lattice Gauge Theory - William's Explorations"

bibliography: references.bib

# format:
#   html:
#     code-fold: true
# jupyter: python3
---

### January 16, 2026

Let $x,y,v,w \in B_n$ such that 
$(x,y) \in E_n^1$ and $(v,w) \in E_n^1$, 
and so that these are distinct edges.
As in section 9 of @Chatterjee2016, 
for a configuration $U \in U(B_n)$ drawn from the product
Haar measure, 
define $V(x,y) = G_U(x)U(x,y)G_U(y)^*$
and $V(v,w) = G_U(v)U(v,w)G_U(w)^*$.

Thus, $V(x,y)$ is a random variable
from the probability space
$\left( \prod_{E_n} U(N), \mathcal{B}(\prod_{E_n} U(N)), du_{\phi(1)} \cdots du_{\phi(a)} du_{\psi(1)} \cdots du_{\psi(r)} \right)$.

Let $a = |E_n^0|$ and let $r = |E_n^1|$. 
Let $\phi: \{1,\cdots, a\} \to E_n^0$ be a bijection
and let $\psi: \{1, \cdots, r\} \to E_n^1$
be a bijection as well. 
For the vertex $x$, let 
$$
i_1(x), \cdots, i_{|x|_1}(x)
$$
be the unique list of integers from the set
$\{1, \cdots, a\}$ such that 
$$
\phi(i_1(x)), \cdots, \phi(i_{|x|_1}(x))
$$
is the path of bonds in $E_n^0$ leading from vertex
$0 \in B_n$ to vertex $x \in B_n$.

Similary, define the lists
$$
i_1(y), \cdots, i_{|y|_1}(y)
$$
and 
$$
i_1(v), \cdots, i_{|v|_1}(v)
$$
and 
$$
i_1(w), \cdots, i_{|w|_1}(w).
$$
Recall the definition from the January 15, 2026 post: 
$G_n: U(B_n) \times B_n \to U(N)$ is given by 
$$
G_n(U,z) = \begin{cases}
I & \text{ if }z = 0 \\
u_{\phi(i_1(z))}\cdots u_{\phi(i_{|z|_1}(z))} & \text{ else},
\end{cases}
$$
where $\phi(i_1(z)), \cdots, \phi(i_{|z|_1}(z))$
is the unique path of bonds in $E_n^0$ 
between $0 \in B_n$ and $z \in B_n$.
By the Lemma from the January 15th post as well, we have the following: 
\begin{align*}
V(x,y) &= G_U(x)U(x,y)G_U(y)^* \\
&= G_n(U,x) U(x,y) G_n(U,y)^* \\
&= U_{\phi(i_1(x))} \cdots U_{\phi(i_{|x|_1}(x))} U(x,y) U_{\phi(i_{|y|_1}(y))}^* \cdots U_{\phi(i_{1}(y))}^*,
\end{align*}
and 
\begin{align*}
V(v, w) &= U_{\phi(i_1(v))} \cdots U_{\phi(i_{|v|_1}(v))} U(v,w) U_{\phi(i_{|w|_1}(w))}^* \cdots U_{\phi(i_{1}(w))}^*.
\end{align*}


::: {.callout-note title="Lemma"}
For all 
:::


### January 15, 2026

Fix the box size $n \in \mathbb N$ and 
the dimension $d \in \mathbb N$.
Let $a = |E_n^0|$ and let $r = |E_n^1|$. 

Let $\phi_n: \{1, \cdots, a\} \to E_n^0$ be 
a bijection 
and let $\psi_n: \{1, \cdots, r\} \to E_n^1$
be a bijection as well, but 
let me suppress the $n$ from the notation, leaving
us with $\phi$ and $\psi$.

A configuration is an assignment 
$E_n \to U(N)$, or 
a sequence $(u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)} )$, 
where
$u_{\phi(l)} \in U(N)$ for all $l \in \{1, \cdots, a\}$
and $u_{\psi(l)} \in U(N)$ for all $l \in \{1, \cdots, r\}$.

Next we consider notation for axial gauge fixing. 
Recall that the set of all configurations on $(B_n, E_n)$ 
is denoted $U(B_n)$.
Let $G_n: U(B_n) \times B_n \to U(N)$ 
be defined by 
$$G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), x) = 
\begin{cases}
I & \text{if } x = 0 \\
u_{\phi(i_1(x))} \cdots u_{\phi(i_k(x))} & \text{if } x \ne 0.
\end{cases}
$$
where $k = |x|_1$ and 
$i_1(x), \cdots, i_k(x)$ is the unique sequence of integers taken from the set 
$\{1, \cdots, a\}$ such that 
$\phi(i_1(x)), \cdots, \phi(i_k(x))$ is the
path of bonds in $E_n^0$ leading from vertex $0 \in B_n$ to 
vertex $x \in B_n$.

::: {.callout-note title="Lemma"}
$G_n$ agrees with the definition of gauge fixing given in section 9 
of @Chatterjee2016.
:::

Proof. We have fixed the box size $n$. Next, fix a configuration $U \in U(B_n)$, or 
we may write $U = (u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)} )$
for some $u_{(\cdot)} \in U(N)$. 

We proceed by induction using the lexicographic ordering 
$\prec$ on the box lattice sites to show that 
$$G_U(x) = G_n(U,x)$$
 for all $x \in B_n$, where the
left hand side is Chatterjee's notation for axial 
gauge fixing.

The base case is $x = 0$. 
We indeed have
$G_U(0) = I = G_n(U,0)$. 

Now assume the induction hypothesis
that $G_U(y) = G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), y)$
for all $y \prec x$, for some $x = (x_1, \cdots, x_d) \in B_n$.

Let $j \in \{1, \cdots, d\}$ be the largest 
index such that $x_j \ne 0$. 
Let $y := x - e_j$.
Then $x \in B_n$ and $y \prec x$.
Hence, 
\begin{align*}
G_U(x) &= G_U(y)U(y,x) \\
&= G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), y) U(y,x) \\
&= u_{\phi(i_1(y))} \cdots u_{\phi(i_k(y))} U(y,x)
\end{align*}
where $k = |y|_1$ and 
$i_1(y), \cdots, i_k(y)$ is the unique sequence of integers taken from the set 
$\{1, \cdots, a\}$ such that 
$\phi(i_1(y)), \cdots, \phi(i_k(y))$ is the
path of bonds in $E_n^0$ leading from vertex $0 \in B_n$ to 
vertex $y \in B_n$. Observe that $(y,x) \in E_n^0$. 
Hence, the sequence $\phi(i_1(y)), \cdots, \phi(i_k(y)), (y,x)$
is a path from $0 \in B_n$ to $x \in B_n$ in the graph $(B_n, E_n^0)$. But
$(B_n, E_n^0)$ is a tree, so it is a unique path. 
Define $i_1(x) := i_1(y)$ and 
so forth until $i_k(x) := i_k(y)$. 
Then let $i_{k+1}(x)$ be the unique integer 
in the set $\{1, \cdots, a\} \setminus \{i_1(y), \cdots, i_k(y))\}$
such that $\phi(i_{k+1}(x)) = (y,x)$. Based on this, we conclude that
\begin{align*}
u_{\phi(i_1(y))} \cdots u_{\phi(i_k(y))} U(y,x) &= u_{\phi(i_1(x))} \cdots u_{\phi(i_k(x))} u_{\phi(i_{k+1}(x))} \\
&= G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), x)
\end{align*}
which closes the induction. $\square$

Tomorrow, I want to use the notation 
$G_n$ and the notion of paths to 
rewrite my journal entry from January 13, 2026.



### January 14, 2026

#### Literature Reflection (aka rabbit hole reflection)
I want to reflect in this post about
one rabbbit hole I went down. 

The story begins when I was looking
for notation to describe the tree that is
implicit in the $E_n^0$ edges of the box $B_n$, 
as described in @Chatterjee2016. 
So I visited first my old 
repository of pdfs from my Davidson times. In
that repository I looked 
at Bruce Sagan's "The Art of Counting," which
did not have what I needed in terms 
of describing a lattice tree, although 
I could use the definition of tree mentioned there. 
Then I visited Bonas's "A Walk through Combinatorics,"
which didn't have anything on lattices $\mathbb Z^d$, 
but had the definitions of graphs and such. 
Then I remembered the 
book Prof. Brennecke recommended, namely, 
"Statistical Mechanics of Lattice Systems" by Friedli and
Velenik. 
That book was written in 2019, and
I noticed a general difference between the 
notation in Chatterjee's 2016 paper
and the Lattice Systems book.
This got me a bit worried because 
it suggested that that notation I'm learning 
via the 2016 paper is outdated. 

Then the rabbit hole officially began because I 
was no longer looking for notation for the tree. 
I couldn't help myself and I looked up
Velenik's website because I was so impressed 
by the lattice systems book and I wondered 
if Velenik was already famous. Then, 
fueled by a lust for fame, I looked up 
Dominil-Copin, who also works at the same place 
as Velenik. I downloaded the article: 
"Marginal Triviality of the scaling limits ..."
by Copin and Aizenman. The one upshot of this 
is that I discovered what Prof. Brennecke meant 
when he said a Field's medal was awarded to 
proving the triviality of some 
quantum field model. 
Unable to stop my fame lust, 
I also looked up Chatterjee's website, and 
saw the Yang-Mills program he has concocted 
together with S. Cao. At this point
I was very burnt out and 
desperate to break the loop. 

Anyways, I think I'll continue to look
at some of the current articles for
ideas on notation, but hopefully I 
won't feel stuck in a loop like I did today.

#### Notation

I think it is better to make a list
of the possible notations for lattice gauge
theory. I'll allow myself
to write down multiple notations and see later
which ones stick. 

- Fix $\mathbb Z^d$ as the d-dimensional integer lattice.

- Let $e_1, \cdots, e_d$ be the standard basis
vectors in $\mathbb R^d$.
- Let $\mathfrak{E}$ (mathfrak E) be the underlying undirected edge set, which 
can be written
$$
\mathfrak{E} = \{\{i,j\} \subset \mathbb Z^d \mid |i-j|_1 = 1\}. 
$$
Note that $|i-j|_1 = \sum_{k=1}^d |i_k-j_k|$, where $i = (i_1, \cdots, i_d)$ and similarly for $j$. 
The metric $|\cdot|_1$ can be called the $l_1$ norm or the taxi-cab metric. 
Hence, $\mathfrak{E}$ is the set of 
nearest neighbor edges of the lattice $\mathbb Z^d$.

- Thus, $(\mathbb Z^d, \mathfrak{E})$ is a graph. 

- Let $\Lambda \subset \mathbb Z^d$. 

- Let
$$
\mathfrak{E}_{\Lambda} = \{\{i,j\} \subset \mathbb \Lambda \mid |i-j|_1 = 1\}
$$
be the nearest neighbor undirected edge set of the sublattice 
$\Lambda$. 

- Let 
$$
E'_{\Lambda} = \{(i,j) \in \Lambda^2 \mid |i-j|_1 = 1 \} 
$$
denote the 
positively and negatively oriented nearest neighbor edges and let 
$$
E_{\Lambda} = \{(i,j) \in \Lambda^2 \mid |i-j|_1 = 1, i < j \} 
$$
denote the positively oriented directed edge set of the lattice $\Lambda$, where 
$i < j$ denotes the lexicographic ordering on $\mathbb Z^d$. 

- Let $B_n = \{0, \cdots, n-1\}^d \subset \mathbb Z^d$

- Let 
\begin{align*}
E_n^0 = \{ (i,j) & \in B_n \times B_n \mid |i-j|_1 = 1, \text{ and for some } \\
                 & 1 \le k \le d \text{ and for some } i_1, \cdots, i_k: \\
                 & i = (i_1, \cdots, i_k, 0, \cdots, 0) \text{ and } j = i + e_k \}
\end{align*}

- Let $E_n$ be defined as in @Chatterjee2016 Section 2. Then $E_n = E_{B_n}$. 

- Let
\begin{align*}
\mathfrak{E}_n^0 = \{ \{i,j\} & \subset B_n \mid |i-j|_1 = 1, \text{ and for some } \\
                 & 1 \le k \le d \text{ and for some } i_1, \cdots, i_k: \\
                 & i = (i_1, \cdots, i_k, 0, \cdots, 0) \text{ and } j = i + e_k \}
\end{align*}
be the undirected version of $E_n^0$. 

- The pair $(B_n, \mathfrak{E}_n^0)$ is a tree, or in other words, the underlying
undirected graph of $(B_n, E_n^0)$ is a tree. 

- A graph $(V,E)$ is a pair consisting of a vertex set $V$ and
an edge set $E \subset \{\{v_1,v_2\} \subset V \mid v_1 \ne v_2 \}$ 

- A finite sequence $v_1, \cdots, v_k$ of distinct vertices 
is called a path from $v_1$ to $v_k$ assuming 
$\{v_i, v_{i+1} \} \in E$ for all $i = 1, \cdots, k-1$. 
Similarly, 
to each path we can associate one and only one
list of edges
$v_1v_2, v_2v_3, \cdots, v_{k-1}v_k$, where
we have used the convention of writing an edge 
without the set bars. 

- We say that a graph is connected if for all $x,y \in V$, 
there is a path from $x$ to $y$

- Theorem (Sagan 1.10.2). Let $T$ be a graph with 
$|V| = n$ and $|E| = m$. The following are equivalent conditions
for $T$ to be a tree: 
  - $T$ is connected and acyclic
  - $T$ is acyclic and $n = m+1$
  - $T$ is connected and $n = m+1$
  - For every pair of vertices $u,v$, there is a unique path 
  from $u$ to $v$.

- Assume that $(B_n, \mathfrak{E}_n^0)$ is connected. One has
$|E_n^0| = n^d - 1$ as in section 17 of @Chatterjee2016. 
Thus, condition 3 of the theorem is satisfied, so it follows that 
$(B_n, \mathfrak{E}_n^0)$, or the underlying graph of $(B_n, E_n^0)$, is a tree.

- One can see that $(B_n, \mathfrak{E}_n^0)$ is connected in
the following way. Fix some vertex $x \in B_n$. We want 
to find a path to $y \in B_n$. First, 
find a path to $0$ in the following way. 
For each step, find the largest index $k$ which is non-zero, 
and subtract $e_k$ to obtain the next vertex. If the next vertex is
$y$, then stop. Else, continue to the 0 vertex. 
If $y$ is not reached before $0$, 
then proceed in the same way starting from 
$y$ and concatenate the paths. 


### January 13, 2026

I have written down on paper a ridiculous 
amount about Lemma 9.3 of @Chatterjee2016, so 
it's about time I start typing some of it.

#### One Argument from Lemma 9.3 of Chatterjee 2016

First let me cite one result about independent probabilistic objects,
which in shorthand says that
$X$ and $Y$ are independent random variables if 
$\mathbb E (f(X)g(Y)) = \mathbb E (f(X)) \mathbb E (g(Y))$
for a sufficiently robust family 
of functions $f, g$. I couldn't find this result in 
Klenke's book (although it's almost certainly implicit somewhere). I also
couldn't find it in Williams's Prob. With Martingales, though
looking at the Monotone class theorem looked 
related. I did find the result in Prof. Eberle's Bachelor's 
Probability theory notes: it is Satz 3.38. 

Recall the guage fixing procedure, 
which can be viewed abstractly as a map 
\begin{equation}
G_U': U(B_n) \to U_0(B_n),
\end{equation}
where I added the prime to
distinguish from $G_U(x)$, which
is an assigment to the vertex $x \in B_n$ 
of a unitary matrix.
I previously proved on paper that 
this map is onto, but let me skip a 
discussion of that. Given a configuration
$U \in U(B_n)$ and the gauge transform $G_U$ (determined by $U$), 
let $V(x,y) = G_U(x) U(x,y) G_U(y)^*$.

Lemma 9.3 says that the matrices
$\{V(x,y) \mid (x,y) \in E_n^1, U \in U(B_n)\}$ are independent
and Haar-distributed. 
Let me skip for now the discussion of 
these matrices being Haar-distributed. 

Let $i,j \in E_n^1$ be distinct edges.
Let $f, g \ge 0$ be measurable with respect to 
$(U(N), \mathcal{B}, \text{Haar})$.
A first step to Lemma 9.3 is 
proving that 
\begin{equation}
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ f(V_i) ] \mathbb E [ g(V_j) ]
\end{equation}
where the expectation is with regards
to product Haar measure on $U(B_n)$ because 
$V_i$ and $V_j$ are implicitly variables of 
the entire unitary configuration, not only the 
$E_n^1$ edge matrices. Explicity, 
we want to show that 
\begin{equation}
\int_{U(B_n)} f(V_i) g(V_j) \prod_{e \in E_n} d\sigma(U_e) = \int_{U(B_n)} f(V_i) \prod_{e \in E_n} d\sigma (U_e) \times \int_{U(B_n)} g(V_j) \prod_{e \in E_n} d\sigma (U_e),
\end{equation}

where as in Chatterjee, we wrote $\text{Haar} = d\sigma$, for compactness of notation. 
By the definition of conditional expectation, 
we have that 


$$
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ]] 
$${#eq-1}

Indeed, $Z(\omega) = \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ] (\omega)$ 
is the almost surely unique random variable such that 
\begin{equation}
\mathbb E[ 1_B( f(V_i) g(V_j) )] = \mathbb E[ 1_B( Z(\omega) )] 
\end{equation}
for all $B \in \sigma(U_0(B_n))$, the sigma algebra generated by
$U_0(B_n)$, viewed as a collection of random variables $(U_e)_{e \in E_n^0}$
on the product space $U(B_n)$.
Of course, $U(B_n) \in \sigma(U_0(B_n))$, so we get @eq-1.

Next, consider the following lemma from Prof. Eberle's notes:

::: {.callout-note title="Lemma (Eberle)"}
Let $(\Omega, \mathcal{A}, P)$ be a probability space and let $\mathcal{F} \subset \mathcal{A}$ be a sub-sigma algebra.
Let $(S,\mathcal{S})$ and $(T, \mathcal{T})$ be measurable spaces. If $Y: \Omega \to S$ is $\mathcal{F}$ measurable and $X: \Omega \to T$ is independent of $\mathcal{F}$, and $\psi: S \times T \to [0,\infty)$ is a product measurable map, then 
$$
\mathbb E[\psi(X,Y) \mid \mathcal{F}](\omega) = \mathbb E[\psi(X,Y(\omega))]
$$
for almost all $\omega \in \Omega$.
:::
Note that on the right hand side in the formula in the lemma, $Y(\omega)$ is to be treated as a constant when you integrate to compute the expectation, whereas $X$ is still considered as a variable: 
$$
\mathbb E[\psi(X,Y(\omega))] = \int_\Omega \psi(X(\omega'), Y(\omega)) dP(\omega').
$$
In essense, you have fixed the variable $Y$ because you already know what it is, based on $\mathcal{F}$. 
In my setting, this is just
a rigorous way of writing that
the matrices attached to $E_n^0$ edges inside
the intergral can be treated as fixed. Let me 
demonstrate. 

Let $S := \prod_{E_n^0} U(N) = U_0(B_n)$ and $T := \prod_{E_n^1} U(N)$. 
Let also $\Omega = \prod_{E_n} U(N) = U(B_n)$. 
Let also 
$$
Y := Y(\omega) := Y(U_e)_{e \in E_n} = (U_e)_{e \in E_n^0}
$$
and 
$$
X := X(\omega) := X(U_e)_{e \in E_n} = (U_e)_{e \in E_n^1}.
$$
Note that $V_i$ can really be viewed as a function of 
the matrix variables on all edges $E_n$. We can write:
$$
V_i(U_e)_{e \in E_n} = U_0 \cdots U_0 U(i) U_0 \cdots U_0
$$
where $U_0$ is a placeholder for matrices attached to $E_n^0$ edges, and $U(i)$ is a matrix attached to edge $i \in E_n^1$. We can write in this way
because $G_U(x)$ is just a product of $U_0(B_n)$ matrices for any $x \in B_n$, and $G_U$ is deterministically a function of the variables $U_0(B_n)$. 

For ease of notation, let 
$$U_1 := (U_e)_{e \in E_n^1}$$
and 
$$U_0 := (U_e)_{e \in E_n^0} \in U_0(B_n).$$
Thus, we consider the definition 
\begin{align*}
\psi(X,Y) &= \psi(U_0,U_1)\\
&= f(U_0 U(i) U_0) g(U_0 U(j) U_0).
\end{align*}

Then by the Lemma, we have

\begin{align*}
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](\omega) &= 
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](U_e)_{e \in E_n} \\
&= \mathbb E[ f(V_i(U_e)_{e \in E_n^0}) g(V_j(U_e)_{e \in E_n^0}) ] \\
&= \int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \prod_{e \in E_n} d\sigma(U'(e))
\end{align*}

Then by left and right invariance of
the Haar measure, we compute
\begin{align*}
\int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \prod_{e \in E_n} d\sigma(U'(e)) &= \int_{U(B_n)} f(U'(i)) g(U'(j)) \prod_{e \in E_n} d\sigma(U'(e))
\end{align*}

Hence, 
$$
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](\omega) = \mathbb E_{U'} [f(U'(i)) g(U'(j))].
$$

Then, 
\begin{align*}
\mathbb E [ f(V_i) g(V_j) ] &= \mathbb E [ \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ]] \\
&= \int_{U(B_n)} \mathbb E_{U'} [f(U'(i)) g(U'(j))] \prod_{e \in E_n} d\sigma(U_e)\\
&= \mathbb E_{U'} [f(U'(i)) g(U'(j))] \int_{U(B_n)} 1 \prod_{e \in E_n} d\sigma(U_e),
\end{align*}
where we pulled a "constant" out of the integral. Then by Haar invariance and Fubini
a couple times to split up $f$ and $g$, we conclude
the result 
\begin{equation}
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ f(V_i) ] \mathbb E [ g(V_j) ].
\end{equation}

I'm tired of this article for today, but one improvement would be to 
write out explicitly what the product $U_0 \cdots U_0$ actually is. 
In particular, I should first write down the tree
with root at the origin in the lattice. Then 
I should rewrite $i \in E_n^1$ as an 
edge of the form $(i_l, i_r)$, and then 
$G_U(i_l)$ will just be the product of 
$U_0$ matrices indexed by 
the edges from the origin $0$ to $i_l$ along the
tree. So if these tree edges 
are $t_0, \cdots, t_{i_l}$, then
the product would be 
$$
U_0 \cdots U_0 = U(t_0) \cdots U(t_{i_l}).
$$

### January 3, 2026

I'm studying section 12 in 
@Chatterjee2016, which is titled Some Standard Results About Gaussian 
Measures. I'd like to understand why
$Q$ being a positive definite $n$ by $n$ 
real matrix implies that 
$$x^TQx + v^Tx + C =: P(x) \ge c \lVert x \rVert^2$$
for some positive constant $c$, 
for all $\Vert x \rVert$ sufficiently large, where 
$x = (x_1, \cdots, x_n) \in \mathbb R^n$. 
Maybe I should first mention what this is saying. 
When a real matrix $Q$ is positive definite, 
it means that the polynomial 
$$ \begin{matrix}
Q_{11} x_1^2 &+& \cdots &+& Q_{1n} x_1 x_n\\
\vdots && \ddots && \vdots \\
Q_{n1} x_1x_n &+& \cdots &+& Q_{nn} x_n^2
\end{matrix}$$
is strictly positive for all $x \ne 0$. So 
the matrix $Q$ is the coefficient matrix of the 
degree 2 monomials. Oh yeah, and $P(x)$ above is
just a general degree 2 polynomial in 
the real variables $x_1, \cdots, x_n$. 
I'll mention that the variables commute, so 
we can just assume that $Q$ is symmetric, 
because we can break up the coefficients 
into two equal halves. The main point of all this 
is that $P(x) \ge c \lVert x \rVert^2$
is a necessary and sufficient criterion for
the integrability of a Gaussian density term
for a Gaussian measure (see section 12), 
so knowing that $Q$ being positive definite 
is equivalent allows you to later bring in 
stuff about smallest eigenvalues of $Q$, linear algebra
stuff, and other information that is not obvious 
from first glance. 

This implication of $Q$ positive definite implies
$P(x) \ge c \lVert x \rVert^2$ is too hard for my 
little brain, so as a first step I need 
to consider an easier case: assume $v = 0$ and 
$C = 0$, so the polynomial $P$ is just 
made up of the terms in $x^TQx$. 
So I want to show that 
$$P(x) = x^T Qx \ge c \lVert x \rVert^2$$
for some 
$c > 0$ and for all $\lVert x \rVert > r > 0$, 
where $r$ is some radius that is big enough. 

Let's assume that 
$$R_* := \inf_{\lVert x \rVert = 1} x^TQx > 0.$$
Hopefully I can prove later on that 
$Q$ being positive definite implies this infimum 
statement truly does hold, but I believe $Q$ being 
symmetric is also needed. 

If this infimum statement does truly hold, then 
since the infimum is a lower bound, 
then for any $\lVert x \rVert = 1$, we 
get the inequality
$$x^TQx \ge R_*$$
and since $\lVert x \rVert^2 = 1$, 
we also have 
$$x^TQx \ge R_* \lVert x \rVert^2.$$

Then take some arbitrary $x \in \mathbb R^n \setminus \overline{B}(0,1)$, that is, 
$\lVert x \rVert > 1.$
Scaling down $x$, we see that $\frac{x}{\lVert x \rVert}$ is 
on the sphere of radius 1. 
So it holds that 
$$\frac{x}{\lVert x \rVert}^T Q \frac{x}{\lVert x \rVert} \ge R_* \lVert \frac{x}{\lVert x \rVert} \rVert^2.$$
Then multiplying through by $\lVert x \rVert^2$, 
we see that 
$$x^T Q x \ge R_* \lVert x \rVert^2.$$
We have thus shown that 
with $c := R_*$ and 
$r = 1$, then for all
$\lVert x \rVert \ge r$, 
we have $P(x) = x^T Qx \ge c \lVert x \rVert^2,$
the desired conclusion.

Here are some other observations that
don't fit anywhere yet: 

#### Random Oberservations

Observation 1: $c\lVert x \rVert^2 = cx_1^2 + \cdots + cx_n^2.$

Observation 2: Consider the expression $x^TQx$ as being built of 
$$x^TQx = \text{diagonal}(Q,x) + \text{off-diagonal}(Q,x),$$
where 
$$\text{diagonal}(Q,x) = Q_{11}x_1^2 + \cdots + Q_{nn} x_n^2$$
and 
$$\text{off-diagonal}(Q,x) = \sum_{i \ne j} Q_{ij}x_ix_j.$$

Observation 3: I notice that the diagonal entries of $Q$ must be positive. 
Here's why. If $e_i = (0, \cdots, 1, \cdots, 0) \in \mathbb R^n$ is 
the vector with $0$ at every index besides the $i$th index, 
then 
$Q_{ii} = e_i^{T} Q e_i > 0$, 
by positive definiteness. 

#### Further Discussion

I wish I had time to graph some of the simpler 
cases. For instance, the case of $\mathbb R^n = \mathbb R^2$,
where we can name the variables $x$ and $y$, 
and then we can see the conic sections. 
Then the off diagonal part 
looks like
\begin{align*}
\text{off-diagonal}(Q,(x,y)) = Q_{12}xy + Q_{21}xy,
\end{align*}
which, when plotted as the graph of the 
function $z(x,y) = (Q_{12}+Q_{21})xy$ in $\mathbb R^3$, 
looks like a hyperbolic parabaloid. 
The diagonal part, being 
a quadratic form with positive coefficients, is always positive, 
and then 
$x^TQx> 0$ takes the geometric meaning
that the negaative part of the hyperbolic parabaloid
is dominated by the 
diagonal part.

#### Still Needed

Next I need to show that $Q$ being 
positive definite (and symmetric)
implies that $$R_* := \inf_{\lVert x \rVert = 1} x^TQx > 0.$$

Then I need to tackle the case where the linear part $v^Tx$ is 
non zero and the constant $C$ is also non-zero.

