---
title: "Yang-Mills Lattice Gauge Theory - William's Explorations"

bibliography: references.bib

format:
  html:
    code-fold: true
jupyter: python3
---

### January 13, 2026

I have written down on paper a ridiculous 
amount about Lemma 9.3 of @Chatterjee2016, so 
it's about time I start typing some of it.

#### One Argument from Lemma 9.3 of Chatterjee 2016

First let me cite one result about independent probabilistic objects,
which in shorthand says that
$X$ and $Y$ are independent random variables if 
$\mathbb E (f(X)g(Y)) = \mathbb E (f(X)) \mathbb E (g(Y))$
for a sufficiently robust family 
of functions $f, g$. I couldn't find this result in 
Klenke's book (although it's almost certainly implicit somewhere). I also
couldn't find it in Williams's Prob. With Martingales, though
looking at the Monotone class theorem looked 
related. I did find the result in Prof. Eberle's Bachelor's 
Probability theory notes: it is Satz 3.38. 

Recall the guage fixing procedure, 
which can be viewed abstractly as a map 
\begin{equation}
G_U': U(B_n) \to U_0(B_n),
\end{equation}
where I added the prime to
distinguish from $G_U(x)$, which
is an assigment to the vertex $x \in B_n$ 
of a unitary matrix.
I previously proved on paper that 
this map is onto, but let me skip a 
discussion of that. Given a configuration
$U \in U(B_n)$ and the gauge transform $G_U$ (determined by $U$), 
let $V(x,y) = G_U(x) U(x,y) G_U(y)^*$.

Lemma 9.3 says that the matrices
$\{V(x,y) \mid (x,y) \in E_n^1, U \in U(B_n)\}$ are independent
and Haar-distributed. 
Let me skip for now the discussion of 
these matrices being Haar-distributed. 

Let $i,j \in E_n^1$ be distinct edges.
Let $f, g \ge 0$ be measurable with respect to 
$(U(N), \mathcal{B}, \text{Haar})$.
A first step to Lemma 9.3 is 
proving that 
\begin{equation}
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ f(V_i) ] \mathbb E [ g(V_j) ]
\end{equation}
where the expectation is with regards
to product Haar measure on $U(B_n)$ because 
$V_i$ and $V_j$ are implicitly variables of 
the entire unitary configuration, not only the 
$E_n^1$ edge matrices. Explicity, 
we want to show that 
\begin{equation}
\int_{U(B_n)} f(V_i) g(V_j) \prod_{e \in E_n} d\sigma(U_e) = \int_{U(B_n)} f(V_i) \prod_{e \in E_n} d\sigma (U_e) \times \int_{U(B_n)} g(V_j) \prod_{e \in E_n} d\sigma (U_e),
\end{equation}

where as in Chatterjee, we wrote $\text{Haar} = d\sigma$, for compactness of notation. 
By the definition of conditional expectation, 
we have that 


$$
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ]] 
$${#eq-1}

Indeed, $Z(\omega) = \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ] (\omega)$ 
is the almost surely unique random variable such that 
\begin{equation}
\mathbb E[ 1_B( f(V_i) g(V_j) )] = \mathbb E[ 1_B( Z(\omega) )] 
\end{equation}
for all $B \in \sigma(U_0(B_n))$, the sigma algebra generated by
$U_0(B_n)$, viewed as a collection of random variables $(U_e)_{e \in E_n^0}$
on the product space $U(B_n)$.
Of course, $U(B_n) \in \sigma(U_0(B_n))$, so we get @eq-1.

Next, consider the following lemma from Prof. Eberle's notes:

::: {.callout-note title="Lemma (Eberle)"}
Let $(\Omega, \mathcal{A}, P)$ be a probability space and let $\mathcal{F} \subset \mathcal{A}$ be a sub-sigma algebra.
Let $(S,\mathcal{S})$ and $(T, \mathcal{T})$ be measurable spaces. If $Y: \Omega \to S$ is $\mathcal{F}$ measurable and $X: \Omega \to T$ is independent of $\mathcal{F}$, and $\psi: S \times T \to [0,\infty)$ is a product measurable map, then 
$$
\mathbb E[\psi(X,Y) \mid \mathcal{F}](\omega) = \mathbb E[\psi(X,Y(\omega))]
$$
for almost all $\omega \in \Omega$.
:::
Note that on the right hand side in the formula in the lemma, $Y(\omega)$ is to be treated as a constant when you integrate to compute the expectation, whereas $X$ is still considered as a variable: 
$$
\mathbb E[\psi(X,Y(\omega))] = \int_\Omega \psi(X(\omega'), Y(\omega)) dP(\omega').
$$
In essense, you have fixed the variable $Y$ because you already know what it is, based on $\mathcal{F}$. 
In my setting, this is just
a rigorous way of writing that
the matrices attached to $E_n^0$ edges inside
the intergral can be treated as fixed. Let me 
demonstrate. 

Let $S := \prod_{E_n^0} U(N) = U_0(B_n)$ and $T := \prod_{E_n^1} U(N)$. 
Let also $\Omega = \prod_{E_n} U(N) = U(B_n)$. 
Let also 
$$
Y := Y(\omega) := Y(U_e)_{e \in E_n} = (U_e)_{e \in E_n^0}
$$
and 
$$
X := X(\omega) := X(U_e)_{e \in E_n} = (U_e)_{e \in E_n^1}.
$$
Note that $V_i$ can really be viewed as a function of 
the matrix variables on all edges $E_n$. We can write:
$$
V_i(U_e)_{e \in E_n} = U_0 \cdots U_0 U(i) U_0 \cdots U_0
$$
where $U_0$ is a placeholder for matrices attached to $E_n^0$ edges, and $U(i)$ is a matrix attached to edge $i \in E_n^1$. We can write in this way
because $G_U(x)$ is just a product of $U_0(B_n)$ matrices for any $x \in B_n$, and $G_U$ is deterministically a function of the variables $U_0(B_n)$. 

For ease of notation, let 
$$U_1 := (U_e)_{e \in E_n^1}$$
and 
$$U_0 := (U_e)_{e \in E_n^0} \in U_0(B_n).$$
Thus, we consider the definition 
\begin{align*}
\psi(X,Y) &= \psi(U_0,U_1)\\
&= f(U_0 U(i) U_0) g(U_0 U(j) U_0).
\end{align*}

Then by the Lemma, we have

\begin{align*}
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](\omega) &= 
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](U_e)_{e \in E_n} \\
&= \mathbb E[ f(V_i(U_e)_{e \in E_n^0}) g(V_j(U_e)_{e \in E_n^0}) ] \\
&= \int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \prod_{e \in E_n} d\sigma(U'(e))
\end{align*}

Then by left and right invariance of
the Haar measure, we compute
\begin{align*}
\int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \prod_{e \in E_n} d\sigma(U'(e)) &= \int_{U(B_n)} f(U'(i)) g(U'(j)) \prod_{e \in E_n} d\sigma(U'(e))
\end{align*}

Hence, 
$$
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](\omega) = \mathbb E_{U'} [f(U'(i)) g(U'(j))].
$$

Then, 
\begin{align*}
\mathbb E [ f(V_i) g(V_j) ] &= \mathbb E [ \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ]] \\
&= \int_{U(B_n)} \mathbb E_{U'} [f(U'(i)) g(U'(j))] \prod_{e \in E_n} d\sigma(U_e)\\
&= \mathbb E_{U'} [f(U'(i)) g(U'(j))] \int_{U(B_n)} 1 \prod_{e \in E_n} d\sigma(U_e),
\end{align*}
where we pulled a "constant" out of the integral. Then again by Haar invariance, we conclude
the result 
\begin{equation}
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ f(V_i) ] \mathbb E [ g(V_j) ].
\end{equation}

I'm tired of this article for today, but one improvement would be to 
write out explicitly what the product $U_0 \cdots U_0$ actually is. 
In particular, I should first write down the tree
with root at the origin in the lattice. Then 
I should rewrite $i \in E_n^1$ as an 
edge of the form $(i_l, i_r)$, and then 
$G_U(i_l)$ will just be the product of 
$U_0$ matrices indexed by 
the edges from the origin $0$ to $i_l$ along the
tree. So if these tree edges 
are $t_0, \cdots, t_{i_l}$, then
the product would be 
$$
U_0 \cdots U_0 = U(t_0) \cdots U(t_{i_l}).
$$

### January 3, 2026

I'm studying section 12 in 
@Chatterjee2016, which is titled Some Standard Results About Gaussian 
Measures. I'd like to understand why
$Q$ being a positive definite $n$ by $n$ 
real matrix implies that 
$$x^TQx + v^Tx + C =: P(x) \ge c \lVert x \rVert^2$$
for some positive constant $c$, 
for all $\Vert x \rVert$ sufficiently large, where 
$x = (x_1, \cdots, x_n) \in \mathbb R^n$. 
Maybe I should first mention what this is saying. 
When a real matrix $Q$ is positive definite, 
it means that the polynomial 
$$ \begin{matrix}
Q_{11} x_1^2 &+& \cdots &+& Q_{1n} x_1 x_n\\
\vdots && \ddots && \vdots \\
Q_{n1} x_1x_n &+& \cdots &+& Q_{nn} x_n^2
\end{matrix}$$
is strictly positive for all $x \ne 0$. So 
the matrix $Q$ is the coefficient matrix of the 
degree 2 monomials. Oh yeah, and $P(x)$ above is
just a general degree 2 polynomial in 
the real variables $x_1, \cdots, x_n$. 
I'll mention that the variables commute, so 
we can just assume that $Q$ is symmetric, 
because we can break up the coefficients 
into two equal halves. The main point of all this 
is that $P(x) \ge c \lVert x \rVert^2$
is a necessary and sufficient criterion for
the integrability of a Gaussian density term
for a Gaussian measure (see section 12), 
so knowing that $Q$ being positive definite 
is equivalent allows you to later bring in 
stuff about smallest eigenvalues of $Q$, linear algebra
stuff, and other information that is not obvious 
from first glance. 

This implication of $Q$ positive definite implies
$P(x) \ge c \lVert x \rVert^2$ is too hard for my 
little brain, so as a first step I need 
to consider an easier case: assume $v = 0$ and 
$C = 0$, so the polynomial $P$ is just 
made up of the terms in $x^TQx$. 
So I want to show that 
$$P(x) = x^T Qx \ge c \lVert x \rVert^2$$
for some 
$c > 0$ and for all $\lVert x \rVert > r > 0$, 
where $r$ is some radius that is big enough. 

Let's assume that 
$$R_* := \inf_{\lVert x \rVert = 1} x^TQx > 0.$$
Hopefully I can prove later on that 
$Q$ being positive definite implies this infimum 
statement truly does hold, but I believe $Q$ being 
symmetric is also needed. 

If this infimum statement does truly hold, then 
since the infimum is a lower bound, 
then for any $\lVert x \rVert = 1$, we 
get the inequality
$$x^TQx \ge R_*$$
and since $\lVert x \rVert^2 = 1$, 
we also have 
$$x^TQx \ge R_* \lVert x \rVert^2.$$

Then take some arbitrary $x \in \mathbb R^n \setminus \overline{B}(0,1)$, that is, 
$\lVert x \rVert > 1.$
Scaling down $x$, we see that $\frac{x}{\lVert x \rVert}$ is 
on the sphere of radius 1. 
So it holds that 
$$\frac{x}{\lVert x \rVert}^T Q \frac{x}{\lVert x \rVert} \ge R_* \lVert \frac{x}{\lVert x \rVert} \rVert^2.$$
Then multiplying through by $\lVert x \rVert^2$, 
we see that 
$$x^T Q x \ge R_* \lVert x \rVert^2.$$
We have thus shown that 
with $c := R_*$ and 
$r = 1$, then for all
$\lVert x \rVert \ge r$, 
we have $P(x) = x^T Qx \ge c \lVert x \rVert^2,$
the desired conclusion.

Here are some other observations that
don't fit anywhere yet: 

#### Random Oberservations

Observation 1: $c\lVert x \rVert^2 = cx_1^2 + \cdots + cx_n^2.$

Observation 2: Consider the expression $x^TQx$ as being built of 
$$x^TQx = \text{diagonal}(Q,x) + \text{off-diagonal}(Q,x),$$
where 
$$\text{diagonal}(Q,x) = Q_{11}x_1^2 + \cdots + Q_{nn} x_n^2$$
and 
$$\text{off-diagonal}(Q,x) = \sum_{i \ne j} Q_{ij}x_ix_j.$$

Observation 3: I notice that the diagonal entries of $Q$ must be positive. 
Here's why. If $e_i = (0, \cdots, 1, \cdots, 0) \in \mathbb R^n$ is 
the vector with $0$ at every index besides the $i$th index, 
then 
$Q_{ii} = e_i^{T} Q e_i > 0$, 
by positive definiteness. 

#### Further Discussion

I wish I had time to graph some of the simpler 
cases. For instance, the case of $\mathbb R^n = \mathbb R^2$,
where we can name the variables $x$ and $y$, 
and then we can see the conic sections. 
Then the off diagonal part 
looks like
\begin{align*}
\text{off-diagonal}(Q,(x,y)) = Q_{12}xy + Q_{21}xy,
\end{align*}
which, when plotted as the graph of the 
function $z(x,y) = (Q_{12}+Q_{21})xy$ in $\mathbb R^3$, 
looks like a hyperbolic parabaloid. 
The diagonal part, being 
a quadratic form with positive coefficients, is always positive, 
and then 
$x^TQx> 0$ takes the geometric meaning
that the negaative part of the hyperbolic parabaloid
is dominated by the 
diagonal part.

#### Still Needed

Next I need to show that $Q$ being 
positive definite (and symmetric)
implies that $$R_* := \inf_{\lVert x \rVert = 1} x^TQx > 0.$$

Then I need to tackle the case where the linear part $v^Tx$ is 
non zero and the constant $C$ is also non-zero.

