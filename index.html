<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Yang-Mills Lattice Gauge Theory - William’s Explorations – William's Lattice YM Explorations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">William’s Lattice YM Explorations</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-older-posts" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Older Posts</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-older-posts">    
        <li>
    <a class="dropdown-item" href="./December2025.html">
 <span class="dropdown-text">December 2025</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./phone-notes.html"> 
<span class="menu-text">Twitter-like feed</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-other" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Other</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-other">    
        <li>
    <a class="dropdown-item" href="./KleinGordon.html">
 <span class="dropdown-text">Wightman Axioms for free scalar quantum field</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./list-cqft.html">
 <span class="dropdown-text">List of results from CQFT 2</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#january-31-2026" id="toc-january-31-2026" class="nav-link active" data-scroll-target="#january-31-2026">January 31, 2026</a></li>
  <li><a href="#january-30-2026" id="toc-january-30-2026" class="nav-link" data-scroll-target="#january-30-2026">January 30, 2026</a></li>
  <li><a href="#january-29-2026" id="toc-january-29-2026" class="nav-link" data-scroll-target="#january-29-2026">January 29, 2026</a></li>
  <li><a href="#january-22-2026" id="toc-january-22-2026" class="nav-link" data-scroll-target="#january-22-2026">January 22, 2026</a></li>
  <li><a href="#january-21-2026" id="toc-january-21-2026" class="nav-link" data-scroll-target="#january-21-2026">January 21, 2026</a></li>
  <li><a href="#january-19-2026" id="toc-january-19-2026" class="nav-link" data-scroll-target="#january-19-2026">January 19, 2026</a></li>
  <li><a href="#january-16-2026" id="toc-january-16-2026" class="nav-link" data-scroll-target="#january-16-2026">January 16, 2026</a></li>
  <li><a href="#january-15-2026" id="toc-january-15-2026" class="nav-link" data-scroll-target="#january-15-2026">January 15, 2026</a></li>
  <li><a href="#january-14-2026" id="toc-january-14-2026" class="nav-link" data-scroll-target="#january-14-2026">January 14, 2026</a></li>
  <li><a href="#january-13-2026" id="toc-january-13-2026" class="nav-link" data-scroll-target="#january-13-2026">January 13, 2026</a></li>
  <li><a href="#january-3-2026" id="toc-january-3-2026" class="nav-link" data-scroll-target="#january-3-2026">January 3, 2026</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Yang-Mills Lattice Gauge Theory - William’s Explorations</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="january-31-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-31-2026">January 31, 2026</h3>
<section id="summary-of-the-lorentz-and-poincaré-groups" class="level4">
<h4 class="anchored" data-anchor-id="summary-of-the-lorentz-and-poincaré-groups">Summary of the Lorentz and Poincaré groups</h4>
<p>In this section I would like to breifly summarize Section 2.3 “The Lorentz and Poincaré Groups” from <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>.</p>
<p>The Poincaré group and the Lorentz group <span class="math inline">\(O(1,3)\)</span> are mathematical objects that have physical meaning in special relativity. For instance, Lorentz transformations describe special relativistic coordinate changes of Minkowski spacetime. But from a mathematical point of view, these two groups have certain structure and properties. For example, viewed as a set, <span class="math display">\[
\mathcal{P} = \{(a,L) \mid a \in \mathbb R^4, L \in O(1,3)\}.
\]</span> Viewed as an abstract group, one writes <span class="math display">\[(a,L)(a',L') = (a + La', LL')\]</span> for the group law, with the identity element <span class="math inline">\((0, 1_{\mathbb R^4})\)</span>. In view of defining strongly continuous projective representations of the proper Poincare group <span class="math inline">\(\mathcal{P}_+^\uparrow\)</span> on some vector space, and by inspecting the definition of strong continuity, we consider the subspace topology on <span class="math inline">\(O(1,3)\)</span> inherited from <span class="math inline">\(\mathbb R^{4 \times 4}\)</span>. This topological structure admits a notion of connectivity, which is often mentioned. Connectivity is a desirable property because of the following result:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Bargmann's Theorem">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Bargmann’s Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(G\)</span> be a connected, simply connected and compact Lie group. Then every finite dimensional stronly continuous projective unitary representation of <span class="math inline">\(G\)</span> can be lifted to a strongly continuous unitary representation of <span class="math inline">\(G\)</span>.</p>
</div>
</div>
<p>Connectivity is shown in section 2.3 through a discussion of parametrizations. Additionally, based on the theorem, the Lorentz and Poincaré groups ought to have a smooth structure to turn them into Lie groups.</p>
<p>Okay I will not conclude this section because I’m too tired.</p>
</section>
<section id="lemma-2.4-discussion" class="level4">
<h4 class="anchored" data-anchor-id="lemma-2.4-discussion">Lemma 2.4 Discussion</h4>
<p>The strategy to construct an analytic atlas for <span class="math inline">\(O(1,3)\)</span> is as follows. First one asks, if <span class="math inline">\(O(1,3)\)</span> is truly a Lie group, then what would be <em>its</em> Lie algebra? (Recall that the notion of a Lie algebra associated to a Lie group is distinct from the notion of a standalone Lie algebra.) The candidate is the matrix group <span class="math inline">\(\mathfrak{so}(1,3)\)</span>. One then establishes that <span class="math inline">\(\mathfrak{so}(1,3)\)</span> is a Lie algebra. One then finishes the proof by using the exponential and logarithm maps together with the Lie algebra <span class="math inline">\(\mathfrak{so}(1,3)\)</span> to establish an analytic atlas for <span class="math inline">\(O(1,3)\)</span>. This explains one reason why the Lie algebra is of interest: it is used to establish charts for the (to-be) Lie group.</p>
</section>
<section id="scattered-thoughts" class="level4">
<h4 class="anchored" data-anchor-id="scattered-thoughts">Scattered Thoughts</h4>
<ul>
<li><p>One equips <span class="math inline">\(O(1,3)\)</span> with the subspace topology inhereted from <span class="math inline">\(\mathbb R^4\)</span>. Matrix multiplication <span class="math inline">\(\cdot: O(1,3)^2 \to O(1,3)\)</span> is continuous with respect to this topology and so is matrix inverse: <span class="math inline">\(L \mapsto L^{-1}\)</span>.</p></li>
<li><p>Example 1.5 is important because it defines the generators <span class="math inline">\(X_1, X_2,\)</span> and <span class="math inline">\(X_3\)</span> of <span class="math inline">\(SO(3)\)</span>. The three matrices are also a basis for <span class="math inline">\(\mathfrak{so}(3)\)</span>, and the exponential map <span class="math inline">\(\exp: \mathfrak{so}(3) \to SO(3)\)</span> is surjective.</p></li>
<li><p>What is the role of the analytic structure of <span class="math inline">\(O(1,3)\)</span> and its Lie algebra?</p></li>
<li><p>I see where connectedness comes in now. It is a condition in the hypothesis of Proposition 1.2, which says that you can lift a projective representation to a unitary representation.</p></li>
<li><p>How do unitary representations arise in the setting of quantum mechanics? Rotating the domain of wave functions using the group <span class="math inline">\(SO(3)\)</span> gives you a unitary representation.</p></li>
<li><p>Discuss <em>irreducible</em> representations. A reducible representation of <span class="math inline">\(G\)</span> on a vector space <span class="math inline">\(V\)</span> would be one for which there is a non-trivial subspace <span class="math inline">\(S \subset V\)</span> for which <span class="math inline">\(\rho(g)S \subset S\)</span> for all <span class="math inline">\(g \in G\)</span>. In plain english, the vector space <span class="math inline">\(V\)</span> can not be broken up in a way that is compatible with all the maps implemented via the group <span class="math inline">\(G\)</span>. For example, by Lemma 3.2, the representation <span class="math display">\[
\mathcal{P}_+^{\uparrow} \ni (a,L) \mapsto U(a,L) \in \mathcal{U}(L^2(S_m^+))
\]</span> is irreducible. Just from the definition of irreducibility, this means that there is no non-trivial invariant subspace of <span class="math inline">\(L^2(S_m^+)\)</span> under <span class="math inline">\(U(a,L)\)</span> for all <span class="math inline">\((a,L)\)</span>.</p></li>
</ul>
</section>
<section id="remarks-about-analysis-and-geometry-on-manifolds" class="level4">
<h4 class="anchored" data-anchor-id="remarks-about-analysis-and-geometry-on-manifolds">Remarks about Analysis and Geometry on Manifolds</h4>
<p>Stokes’s Theorem is the big theorem in the intro lecture course called Analysis and Geometry on Manifolds at Uni. Bonn. Recall the fundamental theorem of calculus: <span class="math display">\[
\int_{[a,b]} f' = f|_a^b.
\]</span> Stokes’s theorem generalizes this: <span class="math display">\[
\int_{M} d\omega = \int_{\partial M} \omega.
\]</span> Recall also Gauss’s theorem for a vector field <span class="math inline">\(F: U \to \mathbb R^n\)</span>, where the outward pointing normal is denoted by <span class="math inline">\(\nu\)</span>: <span class="math display">\[
\int_{U} \text{Div} F = -\int_{\partial U} \langle F, \nu \rangle.
\]</span> Gauss’s theorem says that summing up the divergence of a physical field inside a sphere is the same as summing up the dot product of the field arrows with the arrows that are normal to the sphere on the outer surface of the sphere. For example, the electric flux of an electric field through a spherical shell can be viewed mathematically in the expressions <span class="math display">\[
\int_{\partial S} \langle E, \nu \rangle
\]</span> or <span class="math display">\[
\int_{\theta, \phi} E \cdot (d\theta \times d\phi) dA(\phi, \theta).
\]</span> Then supposing there is a point charge in the interior of the sphere, from which infinitesimal electric field divergences can be computed, one gets from Gauss’s theorem that the electric flux (the surface integral) is proportional to the total enclosed charge, <span class="math inline">\(Q_0\)</span>.</p>
<p>The question remains how to view Gauss’s theorem as a version of Stoke’s theorem. By analogy, one would want there to be a differential form <span class="math inline">\(\omega\)</span> that can be identified with <span class="math inline">\(\text{Div} F dx\)</span>, and satisfying the property that the exterior derivative <span class="math inline">\(d\omega\)</span> corresponds to <span class="math inline">\(F \cdot \nu dS\)</span>, (<span class="math inline">\(dS\)</span> denoting surface measure), or equivalently for <span class="math inline">\(d\omega\)</span> to correspond to <span class="math inline">\(F \cdot (d\theta \times d\phi) dA = F \cdot d\mathbf{A}\)</span>, where <span class="math inline">\(d\mathbf{A}\)</span> denotes a directed surface area element.</p>
<p>I was not able to figure this out in time before tennis practice, but the following resources are relevant:</p>
<ul>
<li><a href="https://sites.ualberta.ca/~vbouchar/MATH215/section_divergence.html" class="uri">https://sites.ualberta.ca/~vbouchar/MATH215/section_divergence.html</a>. As an aside, I am inspired how the author, Vincent Bouchard, lists out all the results of the calculus course.</li>
<li>Exercise 8 in Section 6 of Chapter I: Calculus on Euclidean space in the book <span class="citation" data-cites="OneillElem">O’Neill (<a href="#ref-OneillElem" role="doc-biblioref">1966</a>)</span></li>
<li>section called Exterior Derivatives and Vector Calculus in <span class="math inline">\(\mathbb R^3\)</span> in Chapter 14 of <span class="citation" data-cites="LeeSM">Lee (<a href="#ref-LeeSM" role="doc-biblioref">2013</a>)</span></li>
<li>section called Divergence Theorem in Chapter 16 of <span class="citation" data-cites="LeeSM">Lee (<a href="#ref-LeeSM" role="doc-biblioref">2013</a>)</span></li>
</ul>
</section>
</section>
<section id="january-30-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-30-2026">January 30, 2026</h3>
<section id="dyson-brownian-motion" class="level4">
<h4 class="anchored" data-anchor-id="dyson-brownian-motion">Dyson Brownian Motion</h4>
<p>In this subsection I want to learn about Dyson Brownian motion. Let me learn about it from a high-level/superficial point of view.</p>
<p>I’m going to follow the article here: <a href="https://terrytao.wordpress.com/2010/01/18/254a-notes-3b-brownian-motion-and-dyson-brownian-motion/" class="uri">https://terrytao.wordpress.com/2010/01/18/254a-notes-3b-brownian-motion-and-dyson-brownian-motion/</a>.</p>
<p>Supposedly, Dyson Brownian motion is Brownian motion defined on the spectrum of <span class="math inline">\(n \times n\)</span> Hermitian matrices. To make sense of this previous sentence, what is <em>usual</em> Brownian motion defined on? The answer is that BM is defined on <span class="math inline">\(\mathbb R\)</span> in the sense that BM is a family of random variables <span class="math inline">\((X_t)_{t \in \mathcal{I}}\)</span> where each random variable <span class="math inline">\(X_t\)</span> for fixed <span class="math inline">\(t \in \mathcal{I}\)</span> takes values in <span class="math inline">\(\mathbb R\)</span>. Next, Brownian motion can be defined on vector spaces of matrices. What does this mean? This means that Brownian motion can be seen as a family of random variables <span class="math inline">\((X_t)_{t \in \mathcal{I}}\)</span> where each random variable <span class="math inline">\(X_t\)</span> for fixed <span class="math inline">\(t\)</span> is no longer a random real number, but a random matrix. That is, if <span class="math inline">\(\text{Mat}\)</span> denotes a vector space of matrices, then <span class="math inline">\(X_t(\omega) \in \text{Mat}\)</span> for any <span class="math inline">\(t \in \mathcal{I}\)</span> and any <span class="math inline">\(\omega\)</span> in the underlying probability state space. Now suppose that <span class="math inline">\(\text{Mat} = H(N)\)</span>, the space of <span class="math inline">\(N \times N\)</span> complex Hermitian matrices. Hermitian matrices are diagonalizable, meaning that <span class="math inline">\(H \in H(N)\)</span> implies the existence of a unitary matrix <span class="math inline">\(U\)</span> and a diagonal matrix <span class="math inline">\(D\)</span> such that <span class="math inline">\(H = U^* D U\)</span>, where the diagonal of <span class="math inline">\(D\)</span> consists of the eigenvalues of <span class="math inline">\(H\)</span>, which are all real. These eigenvalues form the spectrum of <span class="math inline">\(H\)</span>, denoted <span class="math inline">\(\sigma(H) = \{\lambda_1 \le \cdots \le \lambda_N\}\)</span>, where the ordering is made possible by virtue of <span class="math inline">\(\lambda_i \in \mathbb R\)</span> for all <span class="math inline">\(i\)</span>. To be precise, Dyson Brownian motion is defined on a quotient space of equivalence classes of Hermitian matrices, where the equivalence relation if given by unitary matrix conjugation. That is, we say <span class="math inline">\(H_1 \sim H_2\)</span> if and only if there exists a unitary matrix <span class="math inline">\(U\)</span> such that <span class="math inline">\(H_1 = UH_2U^*\)</span>. This is the same as the equivalence relation <span class="math inline">\(H_1 \sim H_2\)</span> if and only if <span class="math inline">\(\sigma(H_1) = \sigma(H_2)\)</span>. Thus, we should expect that for a Dyson Brownian motion <span class="math inline">\((X_t)_{t \in \mathcal{I}}\)</span>, that <span class="math inline">\(X_t \in H(N) / \sim\)</span>, or identifying equivalences classes with possible spectra, then <span class="math inline">\(X_t \in \{x \in \mathbb R^N \mid x_1 \le \cdots \le x_N\}.\)</span></p>
<p>Now recall that for Brownian motion <span class="math inline">\((B_t)_{t \in \mathcal{I}}\)</span> on <span class="math inline">\(\mathbb R\)</span> starting at <span class="math inline">\(0\)</span>, each random variable <span class="math inline">\(B_t\)</span> for fixed <span class="math inline">\(t\)</span> has a Gaussian distribution, and that the finite dimensional distributions are Gaussian vectors. And a multi-dimensional Brownian motion can be viewed as multiple one-dimensional Brownian motions tupled together. It remains to be shown what kind of finite dimensional distributions a Dyson Brownian Motion ought to have. For instance, suppose a Brownian motion is defined on Hermitian matrices <span class="math inline">\(H(N)\)</span>. How are certain eigenvalues weighted according to Brownian motion on Hermitian matrices? For instance, does one spectrum have a higher probability density than another spectrum? This needs to be answered.</p>
</section>
<section id="learning-about-representation-theory-of-poincare-group" class="level4">
<h4 class="anchored" data-anchor-id="learning-about-representation-theory-of-poincare-group">Learning about Representation theory of Poincare Group</h4>
<p>In this section I want to learn about Section 3.2 of <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>.</p>
<p>The Poincaré group is</p>
</section>
</section>
<section id="january-29-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-29-2026">January 29, 2026</h3>
<p>In this post, I want to learn about section 2.4 in <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>, called Operators on Fock space.</p>
<section id="integral-kernels-related-to-annihilation-operators" class="level4">
<h4 class="anchored" data-anchor-id="integral-kernels-related-to-annihilation-operators">Integral Kernels related to Annihilation Operators</h4>
<p>I believe the annihilation operators <span class="math inline">\((a(f))_{f \in S(\mathbb R^d)}\)</span> define an operator valued distribution. Typically this would imply that <span class="math inline">\(a_x\)</span> doesn’t exist for <span class="math inline">\(x \in \mathbb R^d\)</span> and that it has to be understood in the smeared out sense using test functions: <span class="math display">\[
a(f) = \int_{\mathbb R^d} dx \overline{f}(x) a_x.
\]</span> But it turns out that <span class="math inline">\(a_x\)</span> actually can be defined as an operator on some subspace of the Bosonic Fock space. This is defined in equation (2.22) of <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>. Not only this, but even taking powers of <span class="math inline">\(a_x\)</span> is allowed: <span class="math inline">\(a_xa_x \cdots a_x = a_x \circ a_x \cdots \circ a_x\)</span>, understood in terms of composition of linear operators on the certain subspace of Fock space mentioned above. This is usually prohibited for quantum fields because of domain issues for unbounded operators.</p>
<p>Next, fix a measurable function <span class="math inline">\(\Lambda: \mathbb R^{dk+dl} \to \mathbb C\)</span>, which will be interpreted as an integral kernel. What is an integral kernel? Here is a link to an explanation: <a href="https://en.wikipedia.org/wiki/Integral_transform" class="uri">https://en.wikipedia.org/wiki/Integral_transform</a>. My takeaway is that integral kernels are extremely important for understanding CQFT on a heuristic level. There are integral kernels of distributions <span class="math inline">\(\Phi: S(\mathbb R^n) \to \mathbb C\)</span>, there are integral kernels of transforms: <span class="math inline">\(\Phi: S(\mathbb R^d) \to S(\mathbb R^d)\)</span>, and there are integral kernels of quadratic forms: <span class="math inline">\(\Phi: \mathcal{H} \times \mathcal{H} \to \mathbb C\)</span>. In all these cases, one wants to turn functions, or pairs of functions, into something else, and to do so you integrate these input functions against an integral kernel. In which way is <span class="math inline">\(\Lambda\)</span> to be understood as an integral kernel? Based on the fact that <span class="math inline">\(a_x\)</span> can be understood as an operator valued function on the dense domain <span class="math inline">\(\mathcal{D}_{S}\)</span>, that powers of <span class="math inline">\(a_x\)</span> make sense, and letting <span class="math inline">\(\langle \cdot, \cdot \rangle: \mathcal{F}_s \times \mathcal{F}_s \to \mathbb C\)</span> denote the inner product on Bosonic Fock space, one can define the quadratic form (see 2.23) <span class="math display">\[
Q_\Lambda(\phi, \psi) = \int_{\mathbb R^{dk+dl}} (dx)_{i=1}^k(dy)_{j=1}^l \Lambda((x_i)_{i=1}^k, (y_j)_{j=1}^l) \langle \prod_{i=1}^k a_{x_i} \phi, \prod_{j=1}^l a_{y_j} \psi \rangle \in \mathbb C
\]</span> for all <span class="math inline">\((\phi, \psi) \in \mathcal{D}_{S} \times \mathcal{D}_{S}\)</span>.</p>
<p>Why is <span class="math inline">\(Q_\Lambda\)</span> useful? The notes <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span> say that identifications (such as <span class="math inline">\(a^*(f) =: \int dx f(x) a^*(x)\)</span> can be made precise) using statements about densely defined quadratic forms. So, which heuristic identification does <span class="math inline">\(Q_\Lambda\)</span> make precise? Well, it’s not an identification that is made precise. Rather it’s the expression <span class="math display">\[
\int_{\mathbb R^{dk+dl}} dx_1 \cdots dx_k dy_1 \cdots dy_l \Lambda (x_1, \cdots, x_k, y_1, \cdots, y_l) a_{x_1}^* \cdots a_{x_k}^* a_{y_1} \cdots a_{y_l}
\]</span> (from the left hand side of 2.23) which is to be understood as the quadratic form <span class="math inline">\(Q_\Lambda\)</span>. The point is that even though the <span class="math inline">\(a^*\)</span> are not well-defined pointwise (according to the notes), the integral can still be understood in a rigorous sense using <span class="math inline">\(Q_{\Lambda}\)</span>.</p>
<p>Perhaps a more comprehensible example from Section 5.4.2 of <span class="citation" data-cites="Dimock">Dimock (<a href="#ref-Dimock" role="doc-biblioref">2011</a>)</span> is helpful to hammer in the point. This section agrees that <span class="math inline">\(a_x\)</span> is well-defined on a dense subspace, but that the adjoint <span class="math inline">\(a^*\)</span> is not well-defined as an operator on the subspace <span class="math inline">\(\mathcal{D}_{\mathcal{S}}\)</span>. Nevertheless, recalling that to each “nice” quadratic form is a linear operator given in the second slot, we define <span class="math inline">\(a^*\)</span> via the quadratic form <span class="math display">\[
\langle \phi, a^*(x) \psi \rangle := \langle a(x) \phi, \psi \rangle
\]</span> for <span class="math inline">\(\phi, \psi \in \mathcal{D}_{\mathcal{S}}\)</span>. The point is that a quadratic form was used to give a rigorous meaning to <span class="math inline">\(a^*\)</span>.</p>
</section>
</section>
<section id="january-22-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-22-2026">January 22, 2026</h3>
<p>In this post I want to learn about the spectral theorem in the context of quantum mechanics.</p>
<!-- (Let me begin with a specific point of confusion 
in @BrenneckeCQFT, which I hope I can overcome 
using some propositions about the spectral calculus
from the notes of my Dirichlet Forms on Graphs course.)
-->
<!--
[//]: <> (This is also a comment.)
-->
<p>The following quote is a passage from <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Passage from Brennecke notes">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Passage from Brennecke notes
</div>
</div>
<div class="callout-body-container callout-body">
<p>“Based on the spectral theorem, let us point out how, in the context of quantum mechanics, observables like the position or the momentum of a particle are connected with self-adjoint operators. Suppose that <span class="math inline">\(A: D_A \to \mathcal{H}\)</span> represents some observable <span class="math inline">\(\mathcal{O}\)</span> and that the system is in state <span class="math inline">\(\psi \in \mathcal{H}\)</span>. Then, based on the normalization <span class="math inline">\(\lvert \psi \rvert = 1\)</span> and on the spectral decomposition (1.11) of <span class="math inline">\(A\)</span>, one identifies <span class="math inline">\(\mathcal{O}\)</span> with a real-valued random variable (ranging almost surely in the spectrum <span class="math inline">\(\sigma(A) \subset \mathbb R\)</span> of <span class="math inline">\(A\)</span>) and the probability <span class="math inline">\(\mathbb P\)</span> that <span class="math inline">\(\mathcal{O}\)</span> takes a specific value in some measurable set <span class="math inline">\(\Omega \in \mathcal{B}(\mathbb R)\)</span> is defined as <span id="eq-obser"><span class="math display">\[
\mathbb P(\mathcal{O} \in \Omega) = \int_{\Omega} \langle \psi, \chi_{d\lambda}(A) \psi \rangle
= \langle \psi, \chi_{\Omega}(A) \psi \rangle.
\tag{1}\]</span></span></p>
<p>Notice that the law <span class="math inline">\(\Omega \mapsto \mathcal{O}_*(\mathbb P)(\Omega)
= \mathbb P(\mathcal{O} \in \Omega)\)</span> defines indeed a Borel probability measure on <span class="math inline">\(\mathbb R\)</span>” (p 12).</p>
<p>Also, for easy reference, (1.11) says, “In terms of the projection-valued measure, <span class="math inline">\(A: D_A \to \mathcal{H}\)</span> has the spectral decomposition <span class="math display">\[
A = \int_{\sigma(A)} \lambda \chi_{d\lambda}(A),
\]</span> where <span class="math inline">\(\sigma(A) \subset \mathbb R\)</span> denotes the spectrum of <span class="math inline">\(A\)</span>.”</p>
</div>
</div>
<section id="explanation-of-the-probability-part" class="level4">
<h4 class="anchored" data-anchor-id="explanation-of-the-probability-part">Explanation of the probability part</h4>
<p>First, the quantum system is in state <span class="math inline">\(\psi \in \mathcal{H}\)</span>. We want to know the chance that some observable <span class="math inline">\(\mathcal{O}\)</span> takes values in the subset <span class="math inline">\(\Omega \subset \sigma(A)\)</span>, based on the fact that the system is in state <span class="math inline">\(\psi\)</span>. For instance, if <span class="math inline">\(A\)</span> has actual discrete eigenvalues, say <span class="math inline">\((\lambda_i)_{i \in [n]}\)</span>, then we might want to know the chance that <span class="math inline">\(\mathcal{O} = \lambda_i\)</span> for some <span class="math inline">\(i\)</span>.</p>
<p>Well, the spectral theorem says roughly that the Hilbert space <span class="math inline">\(\mathcal{H}\)</span> can be decomposed based on the spectrum of <span class="math inline">\(A\)</span>: <span class="math display">\[
\mathcal{H} = \oplus_{\lambda \in \sigma(A)}E_\lambda
\]</span> where <span class="math inline">\(E_\lambda\)</span> is the subspace spanned by eigenvectors associated to <span class="math inline">\(\lambda\)</span>.</p>
<p>When looking only at a subset <span class="math inline">\(\Omega\)</span>, then <span class="math inline">\(H_{\Omega} := \oplus_{\lambda \in \Omega} E_\lambda\)</span> forms some subspace of <span class="math inline">\(\mathcal{H}\)</span>. My thinking is then that this subspace <span class="math inline">\(H_\Omega\)</span> is the set of quantum states that are associated to the observable <span class="math inline">\(\mathcal{O}\)</span> ranging in <span class="math inline">\(\Omega\)</span>.</p>
<p>Next, note that <span class="math inline">\(\chi_\Omega(A)\)</span> is the orthogonal projection onto the subspace <span class="math inline">\(H_\Omega\)</span>. Let’s assume the case that <span class="math inline">\(\psi\)</span> is not already in the subspace <span class="math inline">\(H_\Omega\)</span>. Then we get the following picture. <img src="./pics/IMG_0585.jpeg" class="img-fluid" alt="Caption"></p>
<p>Given the picture, which shows an orthogonal decomposition of the vector <span class="math inline">\(\psi\)</span>, how do we interpret <span class="math inline">\(\langle \psi , \chi_{\Omega}(A) \psi \rangle\)</span> as a probability? Note that, using <span class="math inline">\(\lVert \psi \rVert = 1\)</span>, it follows that<br>
<span class="math inline">\(\langle \psi, \chi_\Omega(A) \psi \rangle = \cos \theta \lVert \chi_\Omega(A) \psi \rVert.\)</span> From here, it makes sense to me that <span class="math inline">\(\psi\)</span> being closer to the subspace <span class="math inline">\(H_\Omega\)</span>, which corresponds to the observable <span class="math inline">\(\mathcal{O}\)</span> being more likely to take values in <span class="math inline">\(\Omega\)</span>, is reflected by an higher value of <span class="math inline">\(\langle \psi, \chi_{\Omega} \psi \rangle\)</span>, because <span class="math inline">\(\theta \to 0\)</span> in this case.</p>
</section>
<section id="further-ramblings-about-definitions" class="level4">
<h4 class="anchored" data-anchor-id="further-ramblings-about-definitions">Further Ramblings about definitions</h4>
<p>If <span class="math inline">\(A\)</span> is finite dimensional, then <span class="math inline">\(\sigma(A)\)</span> just consists of a total of <span class="math inline">\(\text{rank}(A)\)</span> points, and then the spectral decomposition (1.11) reads <span class="math display">\[
A  = \int_{\sigma(A)} \lambda \chi_{d\lambda}(A) = \sum_{\lambda \in \sigma(A)} \lambda U^* (\chi_{\{\lambda\}}) U = \sum_{\lambda \in \sigma(A)} \lambda \langle \phi_\lambda, \cdot \rangle \phi_\lambda
\]</span> where <span class="math inline">\(\phi_\lambda\)</span> is an eigenvector associated to the eigenvalue <span class="math inline">\(\lambda\)</span>.</p>
<p>How do you use the formula <span class="math inline">\(A  = \int_{\sigma(A)} \lambda \chi_{d\lambda}(A)\)</span> to compute <span class="math inline">\(A v\)</span> for some <span class="math inline">\(v \in D_A\)</span>? When <span class="math inline">\(A\)</span> is finite dimensional, it is <span class="math inline">\(Av = \sum_{\lambda \in \sigma(A)} \lambda \langle \phi_\lambda, v \rangle \phi_\lambda.\)</span> For the infinite dimensional case, does <a href="#eq-obser" class="quarto-xref">Equation&nbsp;1</a> provide a clue? If so, then here’s my guess: <span class="math display">\[
A v = \int_{\sigma(A)} \lambda \langle v, \chi_{d\lambda}(A)v \rangle.
\]</span> How do I check whether or not this guess is correct? Well, the left hand side <span class="math inline">\(Av\)</span> is a vector in the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, whereas the right hand side is just a sum of complex numbers, which is not a Hilbert space element. Thus, the guess is incorrect. Let me make another guess based on the notion that <span class="math inline">\(\chi_{d\lambda}(A)\)</span> is a spectral projection valued measure. In analogy to the finite dimensional case, <span class="math inline">\(\chi_{d\lambda}(A)\)</span> should project <span class="math inline">\(v\)</span> onto the subspace of <span class="math inline">\(\mathcal{H}\)</span> consisting of eigenvectors associated to eigenvalues <span class="math inline">\(\lambda \in d\lambda\)</span>, where <span class="math inline">\(d\lambda\)</span> I think of as a very small interval on the real line. Anyways, based on this analogy, my next guess is: <span class="math display">\[
Av = \int_{\sigma(A)} \lambda \left( \chi_{d\lambda}(A)v \right).
\]</span> Actually, this must be correct, and it does agree with the finite dimensional case (identifying <span class="math inline">\(\chi_{d\lambda}(A)v\)</span> and <span class="math inline">\(\langle \phi_\lambda, \cdot \rangle \phi_\lambda\)</span> from above), but can I make sense of the integral on the right hand side based on my knowledge of integration theory? The issue is, given that <span class="math inline">\(\chi_{\Omega}(A)v \in \mathcal{H}\)</span> for any subset <span class="math inline">\(\Omega \subset \sigma(A)\)</span>, then I’m looking at an infinite sum of Hilbert space elements. Thus, not only is this a vector-valued integral for vectors in a finite-dimensional space, where the integral can just be treated component-wise, but it is vector-valued for vectors in an infinite-dimensional space. Time to pull out <span class="citation" data-cites="Amann">Herbert Amann (<a href="#ref-Amann" role="doc-biblioref">2009</a>)</span>. There it is defined a so called Bochner-Lebesgue integral of a Banach-space-valued function <span class="math inline">\(f\)</span> over a measure space <span class="math inline">\(X\)</span> in Chapter X section 2. There is a lot of theory there, but I think what would help me in this post is to look at integrals of simple functions as approximators of the final integral <span class="math inline">\(\int_{\sigma(A)} \lambda \left( \chi_{d\lambda}(A)v \right)\)</span>. Indeed, the idea behind the definition of the Bochner integral is to define the integral for simple functions, then set up a notion of a Cauchy sequence of these vector-valued simple functions (distinct from the notion that is available, for say, metric spaces like <span class="math inline">\(L^2\)</span> with equivalence classes of functions that are not vector-valued) based on a seminorm defined by the integral of a simple function, then to say that the Bochner integral of a general function <span class="math inline">\(f\)</span>, where <span class="math inline">\(f\)</span> is approached by the simple functions <span class="math inline">\(\phi_j\)</span> in the sense of <span class="math inline">\(\mu\)</span>-a.e. pointwise convergence, has Bochner integral defined by the limit <span class="math inline">\(\int \phi_j\)</span>, as long as the <span class="math inline">\(\phi_j\)</span> are a Cauchy sequence. So it is implied that it is a-priori possible for <span class="math inline">\(\phi_j\)</span> to approach <span class="math inline">\(f\)</span> pointwise, but for <span class="math inline">\(\phi_j\)</span> to not form a Cauchy sequence, and thus to face ill-definedness of <span class="math inline">\(\int f\)</span>. Otherwise, the Cauchy condition would be superfluous.</p>
<p>So to understand <span class="math inline">\(\int_{\sigma(A)} \lambda \left( \chi_{d\lambda}(A)v \right)\)</span>, what would be the function <span class="math inline">\(f\)</span>, and what would be the simple functions <span class="math inline">\(\phi_j\)</span>? It is not clear to me how to write the integrand <span class="math inline">\(\lambda \left( \chi_{d\lambda}(A)v \right)\)</span> as an <span class="math inline">\(\mathcal{H}\)</span>-valued function of the variable <span class="math inline">\(\lambda\)</span> in the spectrum because <span class="math inline">\(\chi_{d\lambda}(A)\)</span> is defined on subsets containing <span class="math inline">\(\lambda\)</span>, not <span class="math inline">\(\lambda\)</span> pointwise. But assuming singleton sets are measurable, then let me tentatively write <span class="math inline">\(f(\lambda) =  \left( \chi_{\{\lambda\}}(A)v \right)\)</span>.</p>
</section>
</section>
<section id="january-21-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-21-2026">January 21, 2026</h3>
<div class="callout callout-style-default callout-note callout-titled" title="Axial graph is a tree">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Axial graph is a tree
</div>
</div>
<div class="callout-body-container callout-body">
<p>The underlying undirected graph of the pair <span class="math inline">\((B_n, E_n^0)\)</span> is a tree. In particular, given any vertex <span class="math inline">\(x \in B_n,\)</span> there is a unique sequence of <span class="math inline">\(E_n^0\)</span> edges <span class="math inline">\(e(x,1), \cdots, e(x,|x|_1)\)</span> leading from the origin <span class="math inline">\(0 \in B_n\)</span> to <span class="math inline">\(x\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of Axial Gauge Fixing">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition of Axial Gauge Fixing
</div>
</div>
<div class="callout-body-container callout-body">
<p>We define <span class="math inline">\(G_n: U(B_n) \times B_n\)</span> by <span class="math display">\[
G_n(U, x) = G_n((U_e)_{e \in E_n},x) = \begin{cases}
I &amp; \text{if }x = 0 \\
U_{e(x,1)}U_{e(x,2)} \cdots U_{e(x,|x|_1)} &amp; \text{if }x \ne 0
\end{cases}
\]</span> for all <span class="math inline">\(U = (U_e)_{e \in E_n} \in U(B_n)\)</span>. Observe that, <span class="math inline">\(G_n(U, \cdot) \in G(B_n)\)</span> defines a gauge transform, that is, an assignment of unitary matrices to vertices.</p>
<p>Then, given <span class="math inline">\((U_e)_{e \in E_n} = U \in U(B_n)\)</span>, and <span class="math inline">\(G_U(x)\)</span> defined as in Section 9 of <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span> for all <span class="math inline">\(x \in B_n\)</span>, we have the equality <span class="math inline">\(G_U(x) = G_n(U, x)\)</span> for all <span class="math inline">\(x \in B_n\)</span>. In other words, <span class="math inline">\(G_U\)</span> and <span class="math inline">\(G_n\)</span> produce the same gauge transform for a given configuration <span class="math inline">\(U\)</span>.</p>
<p>Fix a unitary configuration <span class="math inline">\((U_e)_{e \in E_n} = U \in U(B_n)\)</span> and pick a gauge transform <span class="math inline">\(G \in G(B_n)\)</span>. Then another configuration <span class="math inline">\(V:= GU\)</span> is defined as <span class="math display">\[
V(x,y) = G(x)U(x,y)G(y)^{-1}
\]</span> for all <span class="math inline">\((x,y) \in E_n\)</span>. In particular, <span class="math inline">\(V\)</span> is a function of the configuration <span class="math inline">\(U\)</span>, but this is not expressed in the notation <span class="math inline">\(V(x,y)\)</span>.</p>
<p>In particular, <span class="math inline">\(V := G_UU\)</span> and <span class="math inline">\(V' := G_n(U, \cdot)U\)</span> define the same unitary configuration: <span class="math inline">\(V_e = V'_e\)</span> for all <span class="math inline">\(e \in E_n\)</span>. Also, for any <span class="math inline">\((x,y) \in E_n\)</span>, we note that <span class="math display">\[
V(x,y) =
\begin{cases}
U_{e(x,1)} \cdots U_{e(x,|x|_1)} U(x,y) U^*_{e(y,|y|_1)} \cdots U^*_{e(y,1)} &amp; \text{if } x \ne 0 \\
U(x,y) U^*_{e(y,1)} &amp; \text{if } x = 0.
\end{cases}
\]</span></p>
</div>
</div>
<p>Take <span class="math inline">\((x,y), (v,w) \in E_n^1\)</span> distinct and assume that <span class="math inline">\(x,y,w,v \ne 0\)</span>. I want to go through a proof that the <span class="math inline">\(U(N)\)</span>-valued random variables <span class="math display">\[
V(x,y), V(v,w): \left(\prod_{e \in E_n} U(N), \bigotimes_{e \in E_n} \mathcal{B}, \prod_{e \in E_n} d\sigma(U_e) \right) \to (U(N), \mathcal{B}, d\sigma)
\]</span> on the probability space <span class="math display">\[
(\Omega, \mathcal{F}, P) := (U(B_n), \mathcal{B}^{\otimes E_n}, \prod_{e \in E_n} d\sigma_e) = \left(\prod_{e \in E_n} U(N), \bigotimes_{e \in E_n} \mathcal{B}, \prod_{e \in E_n} d\sigma(U_e) \right)
\]</span> are independent. Let <span class="math inline">\(\Pi_{E_n^1}: U(B_n) \to \prod_{e \in E_n^1} U(N) =: U(E_n^1)\)</span> denote the projection onto the <span class="math inline">\(E_n^1\)</span> coordinates, meaning <span class="math display">\[
\Pi_{E_n^1}((U_e)_{e \in E_n}) = (U_e)_{e \in E_n^1}.
\]</span></p>
<p>Let <span class="math inline">\(\Pi_{E_n^0}: U(B_n) \to \prod_{e \in E_n^0} U(N) =: U(E_n^0)\)</span> denote the projection onto the <span class="math inline">\(E_n^0\)</span> coordinates, meaning <span class="math display">\[
\Pi_{E_n^0}((U_e)_{e \in E_n}) = (U_e)_{e \in E_n^0}.
\]</span></p>
<p>Then <span class="math inline">\(\Pi_{E_n^0}\)</span> and <span class="math inline">\(\Pi_{E_n^1}\)</span> are random variables on the probability space <span class="math inline">\(\Omega = U(B_n)\)</span>, and <span class="math inline">\(\Pi_{E_n^0}\)</span> is <span class="math inline">\(\mathcal{B}^{\otimes E_n^0}\)</span> measurable, while <span class="math inline">\(\Pi_{E_n^1}\)</span> is <span class="math inline">\(\mathcal{B}^{\otimes E_n^1}\)</span> measurable. Furthermore, <span class="math inline">\(\Pi_{E_n^0}\)</span> and <span class="math inline">\(\Pi_{E_n^1}\)</span> are independent.</p>
<p>Recalling the definiton of <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> above, and given a real-valued <span class="math inline">\(L^1\)</span> random variable <span class="math inline">\(f: \Omega = U(B_n) \to \mathbb R\)</span>, then the following relations are all just definitions:</p>
<p><span class="math display">\[\begin{align*}
\mathbb E_P[f] &amp; = \int_{\Omega} f(\omega) dP (\omega) \\
&amp;= \int_{U(B_n)} f((U_e)_{e \in E_n}) \prod_{e \in E_n} d \sigma (U_e)
\end{align*}\]</span></p>
<p>By Fubini, we also have <span class="math display">\[\begin{align*}
\mathbb E_P[f] &amp;= \int_{U(E_n^0)} \left( \int_{U(E_n^1)} f((U_e)_{e \in E_n}) \prod_{e \in E_n^1} d \sigma (U_e) \right) \prod_{e \in E_n^0} d \sigma (U_e) \\
&amp;= \int_{U(E_n^1)} \left( \int_{U(E_n^0)} f((U_e)_{e \in E_n}) \prod_{e \in E_n^0} d \sigma (U_e) \right)\prod_{e \in E_n^1} d \sigma (U_e).
\end{align*}\]</span></p>
<p>Using the definitions of <span class="math inline">\(\Pi_{E_n^0}\)</span> and <span class="math inline">\(\Pi_{E_n^1}\)</span>, we also have <span class="math display">\[\begin{align*}
\mathbb E_P[f] &amp;= \int_{U(B_n)} f(\Pi_{E_n^0}(\omega), \Pi_{E_n^1}(\omega)) dP(\omega).
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(f\)</span> is product measurable and <span class="math inline">\(\Pi_{E_n^0}\)</span> and <span class="math inline">\(\Pi_{E_n^1}\)</span> are independent, then <span class="math display">\[\begin{align*}
\mathbb E_P[f \mid U(E_n^0)](\omega) &amp;= \mathbb E_P[f(\Pi_{E_n^0}(\omega),\Pi_{E_n^1})] \\
&amp;= \int_{U(B_n)} f(\Pi_{E_n^0}(\omega),\Pi_{E_n^1}(\omega')) dP(\omega') \\
&amp;= \int_{U(B_n)} f((U_e)_{e \in E_n^0}, (U'_e)_{e \in E_n^1}) \prod_{e \in E_n} d\sigma(U'_e) \\
&amp;= \int_{U(E_n^0)} \left(   \int_{U(E_n^1)}  f((U_e)_{e \in E_n^0}, (U'_e)_{e \in E_n^1})   \prod_{e \in E_n^1} d\sigma(U'_e)     \right) \prod_{e \in E_n^0} d\sigma(U'_e) \\
&amp;= \int_{U(E_n^1)}  f((U_e)_{e \in E_n^0}, (U'_e)_{e \in E_n^1})   \prod_{e \in E_n^1} d\sigma(U'_e) \times
\int_{U(E_n^0)}  1   \prod_{e \in E_n^0} d\sigma(U'_e)\\
&amp;= \int_{U(E_n^1)}  f((U_e)_{e \in E_n^0}, (U'_e)_{e \in E_n^1})   \prod_{e \in E_n^1} d\sigma(U'_e)
\end{align*}\]</span> for almost all <span class="math inline">\(\omega = (U_e)_{e \in E_n} \in \Omega = U(B_n)\)</span>. Equality number 5 above is key. It shows that conditioning on <span class="math inline">\(E_n^0\)</span> edges allows us to break up integrals into products and reduce the degrees of freedom. Without conditioning, we cannot treat <span class="math inline">\(f(U_e)_{e \in E_n}\)</span> as a constant with respect to the <span class="math inline">\(U(E_n^0)\)</span> variables.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Lemma">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Lemma
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(V(x,y)\)</span> and <span class="math inline">\(V(v,w)\)</span> are independent.</p>
</div>
</div>
<section id="proof." class="level5">
<h5 class="anchored" data-anchor-id="proof.">Proof.</h5>
<p>Let <span class="math inline">\(f,g: (U(N), \mathcal{B}) \to \mathbb R_{\ge 0}\)</span> be <span class="math inline">\(\mathcal{B}\)</span>-measurable. We want to show that <span class="math display">\[\mathbb E_P[f(V(x,y)) \times g(V(v,w))] = \mathbb E_P[f(V(x,y))] \mathbb E_P[g(V(v,w))].\]</span> We begin with <span class="math display">\[\begin{align*}
E_P[f(V(x,y)) \times g(V(v,w))] =&amp; E_P[E_P[f(V(x,y)) \times g(V(v,w)) \mid U(E_n^0)]]\\
=&amp; \int_{U(B_n)} E_P[f(V(x,y)) \times g(V(v,w)) \mid U(E_n^0)](\omega) dP(\omega) \\
=&amp; \int_{U(B_n)} E_P[f(G_n(U,x)U(x,y)G_n(U,y)^*) \\
&amp;                   \times g(G_n(U,v)U(v,w)G_n(U,w)^*) \mid U(E_n^0)](U) \prod_{e \in E_n}d\sigma(U_e) \\
=&amp; \int_{U(B_n)} E_P[f(U_{e(x,1)} \cdots U_{e(x,|x|_1)} U(x,y) U_{e(y,|y|_1)}^* \cdots U_{e(y,1)}^* ) \\
&amp;                   \times g(U_{e(v,1)} \cdots U_{e(v,|v|_1)} U(v,w) U_{e(w,|w|_1)}^* \cdots U_{e(w,1)}^* ) \mid U(E_n^0)](U) \prod_{e \in E_n}d\sigma(U_e) \\
\end{align*}\]</span> (then by independence) <span class="math display">\[\begin{align*}
=&amp; \int_{U(B_n)} \bigg ( \int_{ U(E_n^1) } f(U_{e(x,1)} \cdots U_{e(x,|x|_1)} U'(x,y) U_{e(y,|y|_1)}^* \cdots U_{e(y,1)}^* ) \\
&amp; \times g(U_{e(v,1)} \cdots U_{e(v,|v|_1)} U'(v,w) U_{e(w,|w|_1)}^* \cdots U_{e(w,1)}^* ) \prod_{e \in E_n^1} d \sigma (U'_e) \bigg ) \prod_{e \in E_n} d\sigma(U_e)\\
\end{align*}\]</span> (then by Haar invariance) <span class="math display">\[\begin{align*}
=&amp; \int_{U(B_n)} \bigg ( \int_{ U(E_n^1) } f( U'(x,y) ) \times g( U'(v,w) ) \prod_{e \in E_n^1} d \sigma (U'_e) \bigg ) \prod_{e \in E_n} d\sigma(U_e)\\
\end{align*}\]</span> (then by pulling a constant out of the integral) <span class="math display">\[\begin{align*}
=&amp; \bigg ( \int_{ U(E_n^1) } f( U'(x,y) ) \times g( U'(v,w) ) \prod_{e \in E_n^1} d \sigma (U'_e) \bigg ) \times \int_{U(B_n)} 1 \prod_{e \in E_n} d\sigma(U_e)\\
\end{align*}\]</span> (then by Fubini) <span class="math display">\[\begin{align*}
=&amp; \int_{ U(N)^2 } f( U'(x,y) ) \times g( U'(v,w) ) d \sigma (U'_{(x,y)}) d \sigma (U'_{(v,w)}) \\
=&amp; \int_{U(N)} f( U'(x,y) ) d\sigma(U'_{(x,y)}) \times \int_{U(N)} g( U'(v,w) ) d \sigma (U'_{(v,w)})
\end{align*}\]</span> (then since Haar measure is a probability measure and Fubini) <span class="math display">\[\begin{align*}
=&amp; \int_{U(B_n)} f( U'(x,y) ) \prod_{e \in E_n} d\sigma(U'_e) \times \int_{U(B_n)} g( U'(v,w) ) \prod_{e \in E_n} d\sigma(U'_e)
\end{align*}\]</span> (then by Haar invariance) <span class="math display">\[\begin{align*}
=&amp; \int_{U(B_n)} f( U_{e(x,1)}' \cdots U_{e(x,|x|_1)}' U'(x,y) U_{e(y,|y|_1)}'^* \cdots U_{e(y,1)}'^* ) \prod_{e \in E_n} d\sigma(U'_e) \\
&amp; \times \int_{U(B_n)} g( U_{e(v,1)}' \cdots U_{e(v,|v|_1)}' U'(v,w) U_{e(w,|w|_1)}'^* \cdots U_{e(w,1)}'^* ) \prod_{e \in E_n} d\sigma(U'_e) \\
\end{align*}\]</span> (and by definition of <span class="math inline">\(V(x,y)\)</span> and <span class="math inline">\(V(v,w)\)</span> ) <span class="math display">\[\begin{align*}
=&amp; \int_{U(B_n)} f( V(x,y) ) \prod_{e \in E_n} d\sigma(U'_e) \times \int_{U(B_n)} g( V(v,w) ) \prod_{e \in E_n} d\sigma(U'_e) \\
=&amp; \mathbb E_P[f(V(x,y))] \mathbb E_P[g(V(v,w))].
\end{align*}\]</span></p>
<p>QED</p>
</section>
</section>
<section id="january-19-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-19-2026">January 19, 2026</h3>
<section id="diagonalizability" class="level4">
<h4 class="anchored" data-anchor-id="diagonalizability">Diagonalizability</h4>
<p>In this post I want to learn about Fourier transforms as coordinate changes. It is also often said that the Fourier transform diagonalizes operators, so that is why I have titled this section accordingly.</p>
<p>Let <span class="math inline">\(A\)</span> be a two by two real matrix. Suppose that there exist an orthonormal matrix <span class="math inline">\(U \in O(2)\)</span> and a diagonal matrix <span class="math inline">\(D = \begin{bmatrix} \lambda_1 &amp; 0 \\
0 &amp; \lambda_2 \end{bmatrix} \in \mathbb R^{2 \times 2},\)</span> such that <span class="math inline">\(A = UDU^T.\)</span> <strong>Then the <span class="math inline">\(i\)</span>th column of <span class="math inline">\(U\)</span> is an eigenvector of <span class="math inline">\(A\)</span> with corresponding eigenvalue <span class="math inline">\(\lambda_i\)</span>.</strong> Recall that a non-zero column vector <span class="math inline">\(x = [x_1, x_2]^T \in \mathbb R^{2}\)</span> is called an eigenvector of <span class="math inline">\(A\)</span> with corresponding eigenvalue <span class="math inline">\(\lambda \in \mathbb R\)</span> if <span class="math display">\[Ax = \lambda x.\]</span> Here is how to see that <span class="math inline">\(U_{\cdot, i}\)</span> satisfies <span class="math inline">\(A U_{\cdot, i} = \lambda_i U_{\cdot, i}\)</span>. Let <span class="math inline">\(i = 1\)</span>. We compute <span class="math display">\[\begin{align*}
(UDU^T)(U_{\cdot,1}) &amp;= (UD) \begin{bmatrix}
                              1 \\ 0 \end{bmatrix} &amp;&amp; \text{orthonormality}\\
                     &amp;= U \begin{bmatrix}
                              \lambda_1 \\ 0 \end{bmatrix} \\
                     &amp;= \lambda_1 U_{\cdot, 1}.
\end{align*}\]</span> A similar computation follows for <span class="math inline">\(i = 2\)</span>. In conclusion, when <span class="math inline">\(A = UDU^T\)</span> for an orthonormal matrix <span class="math inline">\(U\)</span> (used orthonormality in first step), then the columns of <span class="math inline">\(U\)</span> are eigenvectors of <span class="math inline">\(A\)</span>.</p>
<p>Let <span class="math inline">\(T: V \to V\)</span> be a linear operator on an abstract two dimensional vector space <span class="math inline">\(V\)</span> over the field <span class="math inline">\(\mathbb R\)</span>. Let <span class="math inline">\(v_1, v_2\)</span> and <span class="math inline">\(w_1, w_2\)</span> be two bases of <span class="math inline">\(V\)</span>. Suppose <span class="math display">\[
\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = A,
\]</span> which means that <span class="math inline">\(T(v_1) = A_{1,1} w_1 + A_{2,1} w_2\)</span> and <span class="math inline">\(T(v_2) = A_{1,2} w_1 + A_{2,2} w_2\)</span>. In other words, the first column of <span class="math inline">\(A\)</span> holds the components of the abstract vector <span class="math inline">\(T(v_1)\)</span> in the basis <span class="math inline">\(w_1,w_2\)</span>, and the second column of <span class="math inline">\(A\)</span> holds the components of the abstract vector <span class="math inline">\(T(v_2)\)</span> in the basis <span class="math inline">\(w_1, w_2\)</span>. This could be written in the language of coordinate functions for manifolds, but let me not go there. One more way to visualize <span class="math inline">\(\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = A\)</span> is via <span class="math display">\[
A = \begin{bmatrix}
(Tv_1)_{w_1,w_2} &amp; (Tv_2)_{w_1,w_2}
\end{bmatrix}.
\]</span></p>
<p>We say that the operator <span class="math inline">\(T\)</span> is diagonalizable if there exists a basis <span class="math inline">\(e_1, e_2\)</span> of <span class="math inline">\(V\)</span> such that <span class="math inline">\(e_1, e_2\)</span> are eigenvectors of <span class="math inline">\(T\)</span>: <span class="math inline">\(Te_1 = \lambda_1 e_1\)</span> and <span class="math inline">\(T e_2 = \lambda_2 e_2\)</span>. Then we see that <span class="math display">\[
\text{Mat}_{e_1,e_2}^{e_1,e_2}(T) = \begin{bmatrix}
(Te_1)_{e_1,e_2} &amp; (Te_2)_{e_1, e_2} \end{bmatrix}
= \begin{bmatrix} \lambda_1 &amp; 0 \\ 0 &amp; \lambda_2 \end{bmatrix}.
\]</span></p>
<p>Suppose we know that <span class="math inline">\(T\)</span> is diagonalizable. Fix two bases <span class="math inline">\(v_1, v_2\)</span> and <span class="math inline">\(w_1, w_2\)</span> of <span class="math inline">\(V\)</span>. Then the input space for the matrix <span class="math inline">\(A = \text{Mat}_{v_1, v_2}^{w_1, w_2}(T)\)</span> is the space of all possible coordinates (i.e.&nbsp;<span class="math inline">\(\mathbb R^2\)</span>) in the basis <span class="math inline">\(v_1, v_2\)</span>.<br>
Meanwhile, the output space for the matrix <span class="math inline">\(A\)</span> is the space of all possible coordinates in the basis <span class="math inline">\(w_1, w_2\)</span>.</p>
<p>Then an equation of the form <span class="math display">\[
A = UDV,
\]</span> or more explicitly <span class="math display">\[
\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = U \cdot \text{Mat}_{e_1,e_2}^{e_1,e_2}(T) \cdot V
\]</span> would suggest that <span class="math inline">\(V\)</span> is a change of basis matrix from the basis <span class="math inline">\(v_1, v_2\)</span> to the basis <span class="math inline">\(e_1, e_2\)</span> and <span class="math inline">\(U\)</span> is a change of basis matrix from the basis <span class="math inline">\(e_1, e_2\)</span> to the basis <span class="math inline">\(w_1, w_2\)</span>.</p>
<p>Let me review the idea of change of basis matrices. Suppose <span class="math inline">\(U\)</span> is a change of basis matrix from the basis <span class="math inline">\((e_1,e_2)\)</span> to the basis <span class="math inline">\((w_1,w_2)\)</span> of the abstract vector space <span class="math inline">\(V\)</span>. Then we should be able to deduce how <span class="math inline">\(U\)</span> looks based on the one property it should satisfy, which is the following. <span class="math inline">\(U\)</span> should take the components of some abstract vector <span class="math inline">\(v \in V\)</span> in the first basis <span class="math inline">\(e_1, e_2\)</span>, and rewrite such coordinates with respect to the second basis <span class="math inline">\(w_1, w_2\)</span>. In particular, this should work for the special cases of <span class="math inline">\(v = e_1\)</span> and <span class="math inline">\(v = e_2\)</span>.</p>
<p>Suppose <span class="math inline">\(e_1 = U_{11}w_1 + U_{21}w_2\)</span> and that <span class="math inline">\(e_2 = U_{12}w_1 + U_{22}w_2\)</span> for some real numbers <span class="math inline">\(U_{11}, U_{21}, U_{12}, U_{22},\)</span> which are suggestively written. Then the desired property of a change of basis matrix says that <span class="math display">\[
U (e_1)_{e_1,e_2} = U \begin{bmatrix} 1 \\ 0 \end{bmatrix} = (e_1)_{w_1,w_2} = \begin{bmatrix} U_{11} \\ U_{21} \end{bmatrix}
\]</span> and <span class="math display">\[
U (e_2)_{e_1,e_2} = U \begin{bmatrix} 0 \\ 1 \end{bmatrix} = (e_2)_{w_1,w_2} = \begin{bmatrix} U_{12} \\ U_{22} \end{bmatrix}.
\]</span> This actually determines that <span class="math display">\[ U = \begin{bmatrix} (e_1)_{w_1,w_2} &amp; (e_2)_{w_1, w_2} \end{bmatrix}
= \begin{bmatrix} U_{11} &amp; U_{12} \\ U_{21} &amp; U_{22} \end{bmatrix}.
\]</span></p>
<p>Let that conclude my discussion on change of basis matrices, and let me now rewrite an earlier equation as <span class="math display">\[
\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = \begin{bmatrix} (e_1)_{w_1,w_2} &amp; (e_2)_{w_1, w_2} \end{bmatrix} \cdot \text{Mat}_{e_1,e_2}^{e_1,e_2}(T) \cdot \begin{bmatrix} (v_1)_{e_1,e_2} &amp; (v_2)_{e_1, e_2} \end{bmatrix}
\]</span> From here, we see that, if <span class="math inline">\(T: V \to V\)</span> is diagonalizable, meaning that there exists a basis of <span class="math inline">\(V\)</span> consisting of eigenvectors of <span class="math inline">\(T\)</span>, then any matrix of <span class="math inline">\(T\)</span> in arbitrary bases <span class="math inline">\(v_1, v_2\)</span> and <span class="math inline">\(w_1, w_2\)</span> will also be diagonalizable in the sense of matrices. Why? Because change of basis matrices always exist, and then we just multiply the diagonal matrix on either side accordingly to get equality with <span class="math inline">\(A\)</span>.</p>
<p>Now let me approach things from a different perspective to learn about multiplication operators. I want to learn about the folklore that the Fourier transform turns a differentiation operator into a multiplication operator, and to see how this connects to the discussion above of diagonalization.</p>
<p>Consider the following watered down statement of the spectral theorem, based on the rigorous version found at the beginning of <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>. Let <span class="math inline">\(T_A: V \to V\)</span> be self-adjoint. Then there is a measure space <span class="math inline">\(\Omega\)</span>, a unitary map <span class="math inline">\(U: V \to L^2(\Omega)\)</span> and a real-valued measurable function <span class="math inline">\(f: \Omega \to
\mathbb R\)</span> such that <span class="math display">\[
(U T_A U^*) \phi = f \phi =: \mathcal{M}_f (\phi)
\]</span> for all <span class="math inline">\(\phi \in L^2(\Omega)\)</span>. In other words, conjugation of <span class="math inline">\(T_A\)</span> by <span class="math inline">\(U\)</span> results in a multiplication operator, i.e., <span class="math inline">\(\mathcal{M}_f\)</span>.</p>
<p>As a side note, I have used the notation <span class="math inline">\(T_A\)</span> because I want the matrix of <span class="math inline">\(T_A\)</span> in the canonical basis to be the matrix <span class="math inline">\(A\)</span>, when <span class="math inline">\(V = \mathbb R^n\)</span>.</p>
<p>I can already say that the equation <span class="math inline">\((U T_A U^*) \phi = \mathcal{M}_f(\phi)\)</span> reminds me of the equation <span class="math inline">\(UAU^T = D\)</span>, which comes from <span class="math inline">\(A = U^TDU\)</span>, a variant of what I saw earlier, which would suggest that a diagonal matrix <span class="math inline">\(D\)</span> would correspond to the multiplication operator <span class="math inline">\(\mathcal{M}_f\)</span> in some way. It also suggests that the unitary operator <span class="math inline">\(U: V \to L^2(\Omega)\)</span> corresponds to a change of basis: the input basis could be the usual basis of <span class="math inline">\(V\)</span> (if there is one), and the output basis would be the canonical basis of the <span class="math inline">\(L^2\)</span> space given by the theorem, which (I hope) would be something like a basis of characters in the sense of Fourier analysis. But all of this is what I want to find out.</p>
<p>To move this discussion into the setting of diagonalization of finite dimensional operators, as I began with in this post, let me try to understand the following section from <span class="citation" data-cites="BrenneckeCQFT">Brennecke (<a href="#ref-BrenneckeCQFT" role="doc-biblioref">2026</a>)</span>:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Quote">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Quote
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Theorem generalizes the well-known fact from linear algebra that every Hermitian matrix <span class="math inline">\(H = H^* \in \mathbb C^{n \times n}\)</span> can be diagonalized and admits an orthonornal eigenbasis <span class="math inline">\((\phi_i)_{i=1}^n\)</span> so that <span class="math inline">\(H(\phi_i) = \lambda_i \phi_i\)</span> for suitable (real) eigenvalues <span class="math inline">\(\lambda_i \in \mathbb R\)</span>. In this case, the spectral projection valued measure representation of <span class="math inline">\(H\)</span> is simply given by <span class="math display">\[
H = \sum \lambda_i |\phi_i \rangle \langle \phi_i |.
\]</span> The map <span class="math inline">\(U\)</span> can be defined by linearly extending <span class="math inline">\(\mathbb C^n \ni \phi_i \mapsto \chi_{\{ \lambda_i \}} \in L^2(\Omega, \mathcal{B}(\Omega), \mu)\)</span>, where <span class="math inline">\(\Omega = \{\lambda_i: i = 1, \cdots, n \}\)</span> and where <span class="math inline">\(\mu\)</span> denotes the counting measure on <span class="math inline">\(\Omega\)</span>.</p>
</div>
</div>
<p>In the case of the quote, the map <span class="math display">\[
f: \Omega = \{\lambda_i : i \in [n]\} \to \mathbb R
\]</span> in the spectral theorem is given by <span class="math display">\[
f(x) = x, \text{ i.e., } f = id.
\]</span> Note also that when <span class="math inline">\(\Omega = \{\lambda_i : i \in [n]\}\)</span>, it follows that <span class="math inline">\(L^2(\Omega) = \mathbb C^{\Omega} = \mathbb C^n\)</span>, assuming the functions in <span class="math inline">\(L^2\)</span> are complex valued.</p>
<p>Let <span class="math inline">\(T_H: V \to V\)</span> be a linear operator from a complex vector space <span class="math inline">\(V\)</span> of dimension <span class="math inline">\(n\)</span>, and let <span class="math inline">\(H\)</span> be the matrix of <span class="math inline">\(T_H\)</span> in the sense that <span class="math display">\[
\text{Mat}_{\bf{e}_1, \cdots, \bf{e}_n}^{\bf{e}_1, \cdots, \bf{e}_n}(T_H) = H,
\]</span> where the boldface list <span class="math inline">\(\bf{e}_1, \cdots, \bf{e}_n\)</span> is the standard basis of <span class="math inline">\(\mathbb C^n\)</span>. And assume <span class="math inline">\(H\)</span> is Hermitian, as in the quote, with eigenbasis <span class="math inline">\((\phi_i)_{i=1}^n\)</span> and correponding eigenvalues <span class="math inline">\((\lambda_i)_{i=1}^n\)</span>. The statement of the spectral theorem, taking into account <span class="math inline">\(f\)</span> and <span class="math inline">\(U\)</span> from above, reads as follows: <span id="eq-diag"><span class="math display">\[
(U T_H U^*) \phi = \mathcal{M}_{id} \phi \text{ pointwise on }\Omega
\tag{2}\]</span></span> for all <span class="math inline">\(\phi \in L^2(\Omega) = \mathbb C^n\)</span>.</p>
<p>Let me try to interpret <a href="#eq-diag" class="quarto-xref">Equation&nbsp;2</a> in terms of diagonal matrices. So <span class="math inline">\(\phi\)</span> in an assignment of a complex number to each eigenvalue <span class="math inline">\(\lambda_i\)</span>, for <span class="math inline">\(i \in [n]\)</span>. Thus, <span class="math display">\[
[\phi(\lambda_i), \cdots, \phi(\lambda_n)]^T \in \mathbb C^n.
\]</span> Next, <span class="math inline">\(\mathcal{M}_{id} \phi\)</span> is a function from <span class="math inline">\(\Omega \to \mathbb C\)</span>. So there are <span class="math inline">\(n = |\Omega|\)</span> possible inputs, and the corresponding outputs, in vector form, are <span class="math display">\[
[(\mathcal{M}_{id}\phi)(\lambda_1), \cdots, (\mathcal{M}_{id}\phi)(\lambda_n) ]^T = [\lambda_1 \phi(\lambda_1), \cdots, \lambda_n \phi(\lambda_n) ]^T \in \mathbb C.
\]</span> Thus, <span class="math display">\[
[(\mathcal{M}_{id}\phi)(\lambda_1), \cdots, (\mathcal{M}_{id}\phi)(\lambda_n) ]^T = \begin{bmatrix} \lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{bmatrix} [\phi(\lambda_i), \cdots, \phi(\lambda_n)]^T.
\]</span> This shows how the multiplication operator <span class="math inline">\(\mathcal{M}_{id}\)</span> corresponds to a diagonal matrix with the eigenvalues on the diagonal. In other words, <a href="#eq-diag" class="quarto-xref">Equation&nbsp;2</a> corresponds to the equation <span class="math inline">\(UHU^* = D\)</span> in matrix langauge, where <span class="math inline">\(H\)</span> may not be diaganal with respect to the standard basis <span class="math inline">\(\bf{e}_1, \cdots, \bf{e}_n\)</span>. Continuing the change of basis analogy, it would be that <span class="math inline">\(U^*\)</span> is a change of basis from the basis <span class="math inline">\((\chi_{\{\lambda_i\}})_{i=1}^n\)</span> of <span class="math inline">\(L^2(\Omega)\)</span> to the basis <span class="math inline">\(\bf{e}_1, \cdots, \bf{e}_n\)</span> of <span class="math inline">\(\mathbb C^n\)</span>, and <span class="math inline">\(U\)</span> is a change of basis from the standard basis of <span class="math inline">\(\mathbb C^n\)</span> to the eigenvalue point functions.</p>
<p>If diagonalizing the operator <span class="math inline">\(T_H\)</span> means conjugating by a unitary map <span class="math inline">\(U\)</span>, then I think via the above paragraphs, I have understood diagonalization. But I think it also instructive to check that I actually chose the correct <span class="math inline">\(f\)</span> in the spectral theorem (<span class="math inline">\(f = id\)</span>) by verifying the eigenvalue equations in the following way.</p>
<p>Since <span class="math inline">\(UT_HU^* = \mathcal{M}_{id}\)</span>, it stands to reason that <span class="math inline">\(T_H = U^* \mathcal{M}_{id} U\)</span>. Let’s check this on the eigenbasis, where we know the left hand side will read: <span class="math display">\[
T_H \phi_i = \lambda_i \phi_i.
\]</span> On the right hand side, fix <span class="math inline">\(i=1\)</span>. We compute <span class="math display">\[\begin{align*}
(U^* \mathcal{M}_{id} U )(\phi_1) &amp;= U^* (\mathcal{M}_{id}(U(\phi_1))) \\
&amp;= U^*(id(\cdot) \chi_{\{\lambda_1\}}(\cdot) )\\
&amp;= \lambda_1 \phi_1.
\end{align*}\]</span> The last equality is least clear, so let me delve further into it. Note that <span class="math inline">\(U(\lambda_1 \phi_1) = \lambda_1 U(\phi_1) = \lambda_1 \chi_{\{\lambda_1 \}}(\cdot) \in L^2(\Omega)\)</span> by linearity. But by definition of the indicator function <span class="math inline">\(\chi_{\{\lambda_1 \}}(\cdot)\)</span>, it turns out that <span class="math inline">\(\lambda_i \mapsto \lambda_i \chi_{\{\lambda_1 \}}(\lambda_i)\)</span>, or equivalently, the function <span class="math inline">\(id(\cdot) \chi_{\{\lambda_1\}}(\cdot) \in L^2(\Omega)\)</span>, is actually equal to the function <span class="math inline">\(\lambda_1 \chi_{\{\lambda_1 \}}(\cdot)\)</span>. That is, <span class="math display">\[
id(\cdot) \chi_{\{\lambda_1\}}(\cdot) = \lambda_1 \chi_{\{\lambda_1 \}}(\cdot).
\]</span> Thus, it makes sense to write <span class="math inline">\(U^*(id(\cdot) \chi_{\{\lambda_1\}}(\cdot)) = \lambda_1 \phi_1\)</span>. The upshot is that this then extends to when <span class="math inline">\(i \ne 1\)</span>: <span class="math inline">\(U^*(id(\cdot) \chi_{\{\lambda_i\}}(\cdot)) = \lambda_i \phi_i\)</span>. This confirms that <span class="math display">\[
T_H \phi_i = (U^* \mathcal{M}_{id} U) \phi_i
\]</span> for all <span class="math inline">\(i \in [n]\)</span>. Then we get equality on all of <span class="math inline">\(V\)</span> by unique linear extension.</p>
</section>
<section id="fourier-transform-as-a-diagonalizer" class="level4">
<h4 class="anchored" data-anchor-id="fourier-transform-as-a-diagonalizer">Fourier Transform as a diagonalizer</h4>
<p>Let me sketch out what I’m thinking. There is some function space (or a dense subset of one), call it <span class="math inline">\(C^1(\mathbb R/\mathbb Z)\)</span>, on which the differentiation operator <span class="math inline">\(-i \frac{d}{dt}\)</span> is self-adjoint. Also, if <span class="math inline">\(e_n(x) = e^{-2\pi i n x}\)</span> are characters, and the Fourier transform of <span class="math inline">\(f \in C^1(\mathbb R/\mathbb Z)\)</span> is given by <span class="math display">\[
\mathcal{F}(f)(n) = \langle f, e_n \rangle_{L^2}
\]</span> such that <span class="math inline">\(\mathcal{F}f : \mathbb Z \to \mathbb C\)</span>, or equivalently, <span class="math inline">\(\mathcal{F}f \in \mathbb C^{\mathbb Z}\)</span> is an absolutely summable sequence, then <span class="math display">\[
(\mathcal{F} \circ -i\frac{d}{dt}) f (n) = -i \cdot i n (\mathcal{F}f)(n) = \mathcal{M}_{id}(n) (\mathcal{F}f)(n)
\]</span> for all integers <span class="math inline">\(n\)</span>. If we let <span class="math inline">\(A = -i \frac{d}{dt}\)</span>, then this is the statement that <span class="math display">\[
\mathcal{F} \circ A = \mathcal{M}_{id} \circ \mathcal{F},
\]</span> or that <span class="math display">\[
\mathcal{F} \circ A \circ \mathcal{F}^* = \mathcal{M}_{id}.
\]</span> Thus, we see that the fourier transform diagonalizes the momentum operator <span class="math inline">\(A\)</span>. We can also see that the Fourier transform corresponds to the change of basis matrix <span class="math inline">\(U\)</span> from the beginning of the post, at least positionally. In this case, I think the Fourier transform maps the basis <span class="math inline">\((e_n)_{n \in \mathbb N}\)</span> of characters to the standard basis of <span class="math inline">\(\mathbb C^{\mathbb N}\)</span> (elements with 1 in the ith position, 0 everywhere else). Indeed, <span class="math display">\[
(\mathcal{F}e_i)(n) = \langle e_i, e_n \rangle = \delta_{i}(n).
\]</span> The fact that the list of characters <span class="math inline">\((e_n(\cdot))\)</span> is a basis of, for example, <span class="math inline">\(L^2(\mathbb R/\mathbb Z)\)</span>, is the Fourier theorem in Tao’s Analysis 2 book <span class="citation" data-cites="TaoAna2">Tao (<a href="#ref-TaoAna2" role="doc-biblioref">2006</a>)</span>.</p>
</section>
</section>
<section id="january-16-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-16-2026">January 16, 2026</h3>
<p>Let <span class="math inline">\(x,y,v,w \in B_n\)</span> such that <span class="math inline">\((x,y) \in E_n^1\)</span> and <span class="math inline">\((v,w) \in E_n^1\)</span>, and so that these are distinct edges. As in section 9 of <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span>, for a configuration <span class="math inline">\(U \in U(B_n)\)</span> drawn from the product Haar measure, define <span class="math inline">\(V(x,y) = G_U(x)U(x,y)G_U(y)^*\)</span> and <span class="math inline">\(V(v,w) = G_U(v)U(v,w)G_U(w)^*\)</span>.</p>
<p>Let <span class="math inline">\(a = |E_n^0|\)</span> and let <span class="math inline">\(r = |E_n^1|\)</span>. Let <span class="math inline">\(\phi: \{1,\cdots, a\} \to E_n^0\)</span> be a bijection and let <span class="math inline">\(\psi: \{1, \cdots, r\} \to E_n^1\)</span> be a bijection as well.</p>
<p>Thus, <span class="math inline">\(V(x,y)\)</span> is a random variable from the probability space <span class="math inline">\(\left( \prod_{E_n} U(N), \mathcal{B}(\prod_{E_n} U(N)), du_{\phi(1)} \cdots du_{\phi(a)} du_{\psi(1)} \cdots du_{\psi(r)} \right)\)</span> to the probability space <span class="math inline">\((U(N), \mathcal{B}(U(N)), \sigma)\)</span>, where <span class="math inline">\(\sigma\)</span> denotes Haar measure.</p>
<p>For the vertex <span class="math inline">\(x\)</span>, let <span class="math display">\[
i_1(x), \cdots, i_{|x|_1}(x)
\]</span> be the unique list of integers from the set <span class="math inline">\(\{1, \cdots, a\}\)</span> such that <span class="math display">\[
\phi(i_1(x)), \cdots, \phi(i_{|x|_1}(x))
\]</span> is the path of bonds in <span class="math inline">\(E_n^0\)</span> leading from vertex <span class="math inline">\(0 \in B_n\)</span> to vertex <span class="math inline">\(x \in B_n\)</span>.</p>
<p>Similary, define the lists <span class="math display">\[
i_1(y), \cdots, i_{|y|_1}(y)
\]</span> and <span class="math display">\[
i_1(v), \cdots, i_{|v|_1}(v)
\]</span> and <span class="math display">\[
i_1(w), \cdots, i_{|w|_1}(w).
\]</span> Recall the definition from the January 15, 2026 post: <span class="math inline">\(G_n: U(B_n) \times B_n \to U(N)\)</span> is given by <span class="math display">\[
G_n(U,z) = \begin{cases}
I &amp; \text{ if }z = 0 \\
u_{\phi(i_1(z))}\cdots u_{\phi(i_{|z|_1}(z))} &amp; \text{ else},
\end{cases}
\]</span> where <span class="math inline">\(\phi(i_1(z)), \cdots, \phi(i_{|z|_1}(z))\)</span> is the unique path of bonds in <span class="math inline">\(E_n^0\)</span> between <span class="math inline">\(0 \in B_n\)</span> and <span class="math inline">\(z \in B_n\)</span>. By the Lemma from the January 15th post as well, we have the following: <span class="math display">\[\begin{align*}
V(x,y) &amp;= G_U(x)U(x,y)G_U(y)^* \\
&amp;= G_n(U,x) U(x,y) G_n(U,y)^* \\
&amp;= U_{\phi(i_1(x))} \cdots U_{\phi(i_{|x|_1}(x))} U(x,y) U_{\phi(i_{|y|_1}(y))}^* \cdots U_{\phi(i_{1}(y))}^*,
\end{align*}\]</span> and <span class="math display">\[\begin{align*}
V(v, w) &amp;= U_{\phi(i_1(v))} \cdots U_{\phi(i_{|v|_1}(v))} U(v,w) U_{\phi(i_{|w|_1}(w))}^* \cdots U_{\phi(i_{1}(w))}^*.
\end{align*}\]</span></p>
<p>I’ll have to continue another day.</p>
</section>
<section id="january-15-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-15-2026">January 15, 2026</h3>
<p>Fix the box size <span class="math inline">\(n \in \mathbb N\)</span> and the dimension <span class="math inline">\(d \in \mathbb N\)</span>. Let <span class="math inline">\(a = |E_n^0|\)</span> and let <span class="math inline">\(r = |E_n^1|\)</span>.</p>
<p>Let <span class="math inline">\(\phi_n: \{1, \cdots, a\} \to E_n^0\)</span> be a bijection and let <span class="math inline">\(\psi_n: \{1, \cdots, r\} \to E_n^1\)</span> be a bijection as well, but let me suppress the <span class="math inline">\(n\)</span> from the notation, leaving us with <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span>.</p>
<p>A configuration is an assignment <span class="math inline">\(E_n \to U(N)\)</span>, or a sequence <span class="math inline">\((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)} )\)</span>, where <span class="math inline">\(u_{\phi(l)} \in U(N)\)</span> for all <span class="math inline">\(l \in \{1, \cdots, a\}\)</span> and <span class="math inline">\(u_{\psi(l)} \in U(N)\)</span> for all <span class="math inline">\(l \in \{1, \cdots, r\}\)</span>.</p>
<p>Next we consider notation for axial gauge fixing. Recall that the set of all configurations on <span class="math inline">\((B_n, E_n)\)</span> is denoted <span class="math inline">\(U(B_n)\)</span>. Let <span class="math inline">\(G_n: U(B_n) \times B_n \to U(N)\)</span> be defined by <span class="math display">\[G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), x) =
\begin{cases}
I &amp; \text{if } x = 0 \\
u_{\phi(i_1(x))} \cdots u_{\phi(i_k(x))} &amp; \text{if } x \ne 0.
\end{cases}
\]</span> where <span class="math inline">\(k = |x|_1\)</span> and <span class="math inline">\(i_1(x), \cdots, i_k(x)\)</span> is the unique sequence of integers taken from the set <span class="math inline">\(\{1, \cdots, a\}\)</span> such that <span class="math inline">\(\phi(i_1(x)), \cdots, \phi(i_k(x))\)</span> is the path of bonds in <span class="math inline">\(E_n^0\)</span> leading from vertex <span class="math inline">\(0 \in B_n\)</span> to vertex <span class="math inline">\(x \in B_n\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Lemma">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Lemma
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(G_n\)</span> agrees with the definition of gauge fixing given in section 9 of <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span>.</p>
</div>
</div>
<p>Proof. We have fixed the box size <span class="math inline">\(n\)</span>. Next, fix a configuration <span class="math inline">\(U \in U(B_n)\)</span>, or we may write <span class="math inline">\(U = (u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)} )\)</span> for some <span class="math inline">\(u_{(\cdot)} \in U(N)\)</span>.</p>
<p>We proceed by induction using the lexicographic ordering <span class="math inline">\(\prec\)</span> on the box lattice sites to show that <span class="math display">\[G_U(x) = G_n(U,x)\]</span> for all <span class="math inline">\(x \in B_n\)</span>, where the left hand side is Chatterjee’s notation for axial gauge fixing.</p>
<p>The base case is <span class="math inline">\(x = 0\)</span>. We indeed have <span class="math inline">\(G_U(0) = I = G_n(U,0)\)</span>.</p>
<p>Now assume the induction hypothesis that <span class="math inline">\(G_U(y) = G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), y)\)</span> for all <span class="math inline">\(y \prec x\)</span>, for some <span class="math inline">\(x = (x_1, \cdots, x_d) \in B_n\)</span>.</p>
<p>Let <span class="math inline">\(j \in \{1, \cdots, d\}\)</span> be the largest index such that <span class="math inline">\(x_j \ne 0\)</span>. Let <span class="math inline">\(y := x - e_j\)</span>. Then <span class="math inline">\(x \in B_n\)</span> and <span class="math inline">\(y \prec x\)</span>. Hence, <span class="math display">\[\begin{align*}
G_U(x) &amp;= G_U(y)U(y,x) \\
&amp;= G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), y) U(y,x) \\
&amp;= u_{\phi(i_1(y))} \cdots u_{\phi(i_k(y))} U(y,x)
\end{align*}\]</span> where <span class="math inline">\(k = |y|_1\)</span> and <span class="math inline">\(i_1(y), \cdots, i_k(y)\)</span> is the unique sequence of integers taken from the set <span class="math inline">\(\{1, \cdots, a\}\)</span> such that <span class="math inline">\(\phi(i_1(y)), \cdots, \phi(i_k(y))\)</span> is the path of bonds in <span class="math inline">\(E_n^0\)</span> leading from vertex <span class="math inline">\(0 \in B_n\)</span> to vertex <span class="math inline">\(y \in B_n\)</span>. Observe that <span class="math inline">\((y,x) \in E_n^0\)</span>. Hence, the sequence <span class="math inline">\(\phi(i_1(y)), \cdots, \phi(i_k(y)), (y,x)\)</span> is a path from <span class="math inline">\(0 \in B_n\)</span> to <span class="math inline">\(x \in B_n\)</span> in the graph <span class="math inline">\((B_n, E_n^0)\)</span>. But <span class="math inline">\((B_n, E_n^0)\)</span> is a tree, so it is a unique path. Define <span class="math inline">\(i_1(x) := i_1(y)\)</span> and so forth until <span class="math inline">\(i_k(x) := i_k(y)\)</span>. Then let <span class="math inline">\(i_{k+1}(x)\)</span> be the unique integer in the set <span class="math inline">\(\{1, \cdots, a\} \setminus \{i_1(y), \cdots, i_k(y))\}\)</span> such that <span class="math inline">\(\phi(i_{k+1}(x)) = (y,x)\)</span>. Based on this, we conclude that <span class="math display">\[\begin{align*}
u_{\phi(i_1(y))} \cdots u_{\phi(i_k(y))} U(y,x) &amp;= u_{\phi(i_1(x))} \cdots u_{\phi(i_k(x))} u_{\phi(i_{k+1}(x))} \\
&amp;= G_n((u_{\phi(1)}, \cdots, u_{\phi(a)}, u_{\psi(1)}, \cdots, u_{\psi(r)}), x)
\end{align*}\]</span> which closes the induction. <span class="math inline">\(\square\)</span></p>
<p>Tomorrow, I want to use the notation <span class="math inline">\(G_n\)</span> and the notion of paths to rewrite my journal entry from January 13, 2026.</p>
</section>
<section id="january-14-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-14-2026">January 14, 2026</h3>
<section id="literature-reflection-aka-rabbit-hole-reflection" class="level4">
<h4 class="anchored" data-anchor-id="literature-reflection-aka-rabbit-hole-reflection">Literature Reflection (aka rabbit hole reflection)</h4>
<p>I want to reflect in this post about one rabbbit hole I went down.</p>
<p>The story begins when I was looking for notation to describe the tree that is implicit in the <span class="math inline">\(E_n^0\)</span> edges of the box <span class="math inline">\(B_n\)</span>, as described in <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span>. So I visited first my old repository of pdfs from my Davidson times. In that repository I looked at Bruce Sagan’s “The Art of Counting,” which did not have what I needed in terms of describing a lattice tree, although I could use the definition of tree mentioned there. Then I visited Bonas’s “A Walk through Combinatorics,” which didn’t have anything on lattices <span class="math inline">\(\mathbb Z^d\)</span>, but had the definitions of graphs and such. Then I remembered the book Prof.&nbsp;Brennecke recommended, namely, “Statistical Mechanics of Lattice Systems” by Friedli and Velenik. That book was written in 2019, and I noticed a general difference between the notation in Chatterjee’s 2016 paper and the Lattice Systems book. This got me a bit worried because it suggested that that notation I’m learning via the 2016 paper is outdated.</p>
<p>Then the rabbit hole officially began because I was no longer looking for notation for the tree. I couldn’t help myself and I looked up Velenik’s website because I was so impressed by the lattice systems book and I wondered if Velenik was already famous. Then, fueled by a lust for fame, I looked up Dominil-Copin, who also works at the same place as Velenik. I downloaded the article: “Marginal Triviality of the scaling limits …” by Copin and Aizenman. The one upshot of this is that I discovered what Prof.&nbsp;Brennecke meant when he said a Field’s medal was awarded to proving the triviality of some quantum field model. Unable to stop my fame lust, I also looked up Chatterjee’s website, and saw the Yang-Mills program he has concocted together with S. Cao. At this point I was very burnt out and desperate to break the loop.</p>
<p>Anyways, I think I’ll continue to look at some of the current articles for ideas on notation, but hopefully I won’t feel stuck in a loop like I did today.</p>
</section>
<section id="notation" class="level4">
<h4 class="anchored" data-anchor-id="notation">Notation</h4>
<p>I think it is better to make a list of the possible notations for lattice gauge theory. I’ll allow myself to write down multiple notations and see later which ones stick.</p>
<ul>
<li><p>Fix <span class="math inline">\(\mathbb Z^d\)</span> as the d-dimensional integer lattice.</p></li>
<li><p>Let <span class="math inline">\(e_1, \cdots, e_d\)</span> be the standard basis vectors in <span class="math inline">\(\mathbb R^d\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\mathfrak{E}\)</span> (mathfrak E) be the underlying undirected edge set, which can be written <span class="math display">\[
\mathfrak{E} = \{\{i,j\} \subset \mathbb Z^d \mid |i-j|_1 = 1\}.
\]</span> Note that <span class="math inline">\(|i-j|_1 = \sum_{k=1}^d |i_k-j_k|\)</span>, where <span class="math inline">\(i = (i_1, \cdots, i_d)\)</span> and similarly for <span class="math inline">\(j\)</span>. The metric <span class="math inline">\(|\cdot|_1\)</span> can be called the <span class="math inline">\(l_1\)</span> norm or the taxi-cab metric. Hence, <span class="math inline">\(\mathfrak{E}\)</span> is the set of nearest neighbor edges of the lattice <span class="math inline">\(\mathbb Z^d\)</span>.</p></li>
<li><p>Thus, <span class="math inline">\((\mathbb Z^d, \mathfrak{E})\)</span> is a graph.</p></li>
<li><p>Let <span class="math inline">\(\Lambda \subset \mathbb Z^d\)</span>.</p></li>
<li><p>Let <span class="math display">\[
\mathfrak{E}_{\Lambda} = \{\{i,j\} \subset \mathbb \Lambda \mid |i-j|_1 = 1\}
\]</span> be the nearest neighbor undirected edge set of the sublattice <span class="math inline">\(\Lambda\)</span>.</p></li>
<li><p>Let <span class="math display">\[
E'_{\Lambda} = \{(i,j) \in \Lambda^2 \mid |i-j|_1 = 1 \}
\]</span> denote the positively and negatively oriented nearest neighbor edges and let <span class="math display">\[
E_{\Lambda} = \{(i,j) \in \Lambda^2 \mid |i-j|_1 = 1, i &lt; j \}
\]</span> denote the positively oriented directed edge set of the lattice <span class="math inline">\(\Lambda\)</span>, where <span class="math inline">\(i &lt; j\)</span> denotes the lexicographic ordering on <span class="math inline">\(\mathbb Z^d\)</span>.</p></li>
<li><p>Let <span class="math inline">\(B_n = \{0, \cdots, n-1\}^d \subset \mathbb Z^d\)</span></p></li>
<li><p>Let <span class="math display">\[\begin{align*}
E_n^0 = \{ (i,j) &amp; \in B_n \times B_n \mid |i-j|_1 = 1, \text{ and for some } \\
               &amp; 1 \le k \le d \text{ and for some } i_1, \cdots, i_k: \\
               &amp; i = (i_1, \cdots, i_k, 0, \cdots, 0) \text{ and } j = i + e_k \}
\end{align*}\]</span></p></li>
<li><p>Let <span class="math inline">\(E_n\)</span> be defined as in <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span> Section 2. Then <span class="math inline">\(E_n = E_{B_n}\)</span>.</p></li>
<li><p>Let <span class="math display">\[\begin{align*}
\mathfrak{E}_n^0 = \{ \{i,j\} &amp; \subset B_n \mid |i-j|_1 = 1, \text{ and for some } \\
               &amp; 1 \le k \le d \text{ and for some } i_1, \cdots, i_k: \\
               &amp; i = (i_1, \cdots, i_k, 0, \cdots, 0) \text{ and } j = i + e_k \}
\end{align*}\]</span> be the undirected version of <span class="math inline">\(E_n^0\)</span>.</p></li>
<li><p>The pair <span class="math inline">\((B_n, \mathfrak{E}_n^0)\)</span> is a tree, or in other words, the underlying undirected graph of <span class="math inline">\((B_n, E_n^0)\)</span> is a tree.</p></li>
<li><p>A graph <span class="math inline">\((V,E)\)</span> is a pair consisting of a vertex set <span class="math inline">\(V\)</span> and an edge set <span class="math inline">\(E \subset \{\{v_1,v_2\} \subset V \mid v_1 \ne v_2 \}\)</span></p></li>
<li><p>A finite sequence <span class="math inline">\(v_1, \cdots, v_k\)</span> of distinct vertices is called a path from <span class="math inline">\(v_1\)</span> to <span class="math inline">\(v_k\)</span> assuming <span class="math inline">\(\{v_i, v_{i+1} \} \in E\)</span> for all <span class="math inline">\(i = 1, \cdots, k-1\)</span>. Similarly, to each path we can associate one and only one list of edges <span class="math inline">\(v_1v_2, v_2v_3, \cdots, v_{k-1}v_k\)</span>, where we have used the convention of writing an edge without the set bars.</p></li>
<li><p>We say that a graph is connected if for all <span class="math inline">\(x,y \in V\)</span>, there is a path from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span></p></li>
<li><p>Theorem (Sagan 1.10.2). Let <span class="math inline">\(T\)</span> be a graph with <span class="math inline">\(|V| = n\)</span> and <span class="math inline">\(|E| = m\)</span>. The following are equivalent conditions for <span class="math inline">\(T\)</span> to be a tree:</p>
<ul>
<li><span class="math inline">\(T\)</span> is connected and acyclic</li>
<li><span class="math inline">\(T\)</span> is acyclic and <span class="math inline">\(n = m+1\)</span></li>
<li><span class="math inline">\(T\)</span> is connected and <span class="math inline">\(n = m+1\)</span></li>
<li>For every pair of vertices <span class="math inline">\(u,v\)</span>, there is a unique path from <span class="math inline">\(u\)</span> to <span class="math inline">\(v\)</span>.</li>
</ul></li>
<li><p>Assume that <span class="math inline">\((B_n, \mathfrak{E}_n^0)\)</span> is connected. One has <span class="math inline">\(|E_n^0| = n^d - 1\)</span> as in section 17 of <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span>. Thus, condition 3 of the theorem is satisfied, so it follows that <span class="math inline">\((B_n, \mathfrak{E}_n^0)\)</span>, or the underlying graph of <span class="math inline">\((B_n, E_n^0)\)</span>, is a tree.</p></li>
<li><p>One can see that <span class="math inline">\((B_n, \mathfrak{E}_n^0)\)</span> is connected in the following way. Fix some vertex <span class="math inline">\(x \in B_n\)</span>. We want to find a path to <span class="math inline">\(y \in B_n\)</span>. First, find a path to <span class="math inline">\(0\)</span> in the following way. For each step, find the largest index <span class="math inline">\(k\)</span> which is non-zero, and subtract <span class="math inline">\(e_k\)</span> to obtain the next vertex. If the next vertex is <span class="math inline">\(y\)</span>, then stop. Else, continue to the 0 vertex. If <span class="math inline">\(y\)</span> is not reached before <span class="math inline">\(0\)</span>, then proceed in the same way starting from <span class="math inline">\(y\)</span> and concatenate the paths.</p></li>
</ul>
</section>
</section>
<section id="january-13-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-13-2026">January 13, 2026</h3>
<p>I have written down on paper a ridiculous amount about Lemma 9.3 of <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span>, so it’s about time I start typing some of it.</p>
<section id="one-argument-from-lemma-9.3-of-chatterjee-2016" class="level4">
<h4 class="anchored" data-anchor-id="one-argument-from-lemma-9.3-of-chatterjee-2016">One Argument from Lemma 9.3 of Chatterjee 2016</h4>
<p>First let me cite one result about independent probabilistic objects, which in shorthand says that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables if <span class="math inline">\(\mathbb E (f(X)g(Y)) = \mathbb E (f(X)) \mathbb E (g(Y))\)</span> for a sufficiently robust family of functions <span class="math inline">\(f, g\)</span>. I couldn’t find this result in Klenke’s book (although it’s almost certainly implicit somewhere). I also couldn’t find it in Williams’s Prob. With Martingales, though looking at the Monotone class theorem looked related. I did find the result in Prof.&nbsp;Eberle’s Bachelor’s Probability theory notes: it is Satz 3.38.</p>
<p>Recall the guage fixing procedure, which can be viewed abstractly as a map <span class="math display">\[\begin{equation}
G_U': U(B_n) \to U_0(B_n),
\end{equation}\]</span> where I added the prime to distinguish from <span class="math inline">\(G_U(x)\)</span>, which is an assigment to the vertex <span class="math inline">\(x \in B_n\)</span> of a unitary matrix. I previously proved on paper that this map is onto, but let me skip a discussion of that. Given a configuration <span class="math inline">\(U \in U(B_n)\)</span> and the gauge transform <span class="math inline">\(G_U\)</span> (determined by <span class="math inline">\(U\)</span>), let <span class="math inline">\(V(x,y) = G_U(x) U(x,y) G_U(y)^*\)</span>.</p>
<p>Lemma 9.3 says that the matrices <span class="math inline">\(\{V(x,y) \mid (x,y) \in E_n^1, U \in U(B_n)\}\)</span> are independent and Haar-distributed. Let me skip for now the discussion of these matrices being Haar-distributed.</p>
<p>Let <span class="math inline">\(i,j \in E_n^1\)</span> be distinct edges. Let <span class="math inline">\(f, g \ge 0\)</span> be measurable with respect to <span class="math inline">\((U(N), \mathcal{B}, \text{Haar})\)</span>. A first step to Lemma 9.3 is proving that <span class="math display">\[\begin{equation}
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ f(V_i) ] \mathbb E [ g(V_j) ]
\end{equation}\]</span> where the expectation is with regards to product Haar measure on <span class="math inline">\(U(B_n)\)</span> because <span class="math inline">\(V_i\)</span> and <span class="math inline">\(V_j\)</span> are implicitly variables of the entire unitary configuration, not only the <span class="math inline">\(E_n^1\)</span> edge matrices. Explicity, we want to show that <span class="math display">\[\begin{equation}
\int_{U(B_n)} f(V_i) g(V_j) \prod_{e \in E_n} d\sigma(U_e) = \int_{U(B_n)} f(V_i) \prod_{e \in E_n} d\sigma (U_e) \times \int_{U(B_n)} g(V_j) \prod_{e \in E_n} d\sigma (U_e),
\end{equation}\]</span></p>
<p>where as in Chatterjee, we wrote <span class="math inline">\(\text{Haar} = d\sigma\)</span>, for compactness of notation. By the definition of conditional expectation, we have that</p>
<p><span id="eq-1"><span class="math display">\[
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ]]
\tag{3}\]</span></span></p>
<p>Indeed, <span class="math inline">\(Z(\omega) = \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ] (\omega)\)</span> is the almost surely unique random variable such that <span class="math display">\[\begin{equation}
\mathbb E[ 1_B( f(V_i) g(V_j) )] = \mathbb E[ 1_B( Z(\omega) )]
\end{equation}\]</span> for all <span class="math inline">\(B \in \sigma(U_0(B_n))\)</span>, the sigma algebra generated by <span class="math inline">\(U_0(B_n)\)</span>, viewed as a collection of random variables <span class="math inline">\((U_e)_{e \in E_n^0}\)</span> on the product space <span class="math inline">\(U(B_n)\)</span>. Of course, <span class="math inline">\(U(B_n) \in \sigma(U_0(B_n))\)</span>, so we get <a href="#eq-1" class="quarto-xref">Equation&nbsp;3</a>.</p>
<p>Next, consider the following lemma from Prof.&nbsp;Eberle’s notes:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Lemma (Eberle)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Lemma (Eberle)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\((\Omega, \mathcal{A}, P)\)</span> be a probability space and let <span class="math inline">\(\mathcal{F} \subset \mathcal{A}\)</span> be a sub-sigma algebra. Let <span class="math inline">\((S,\mathcal{S})\)</span> and <span class="math inline">\((T, \mathcal{T})\)</span> be measurable spaces. If <span class="math inline">\(Y: \Omega \to S\)</span> is <span class="math inline">\(\mathcal{F}\)</span> measurable and <span class="math inline">\(X: \Omega \to T\)</span> is independent of <span class="math inline">\(\mathcal{F}\)</span>, and <span class="math inline">\(\psi: S \times T \to [0,\infty)\)</span> is a product measurable map, then <span class="math display">\[
\mathbb E[\psi(X,Y) \mid \mathcal{F}](\omega) = \mathbb E[\psi(X,Y(\omega))]
\]</span> for almost all <span class="math inline">\(\omega \in \Omega\)</span>.</p>
</div>
</div>
<p>Note that on the right hand side in the formula in the lemma, <span class="math inline">\(Y(\omega)\)</span> is to be treated as a constant when you integrate to compute the expectation, whereas <span class="math inline">\(X\)</span> is still considered as a variable: <span class="math display">\[
\mathbb E[\psi(X,Y(\omega))] = \int_\Omega \psi(X(\omega'), Y(\omega)) dP(\omega').
\]</span> In essense, you have fixed the variable <span class="math inline">\(Y\)</span> because you already know what it is, based on <span class="math inline">\(\mathcal{F}\)</span>. In my setting, this is just a rigorous way of writing that the matrices attached to <span class="math inline">\(E_n^0\)</span> edges inside the intergral can be treated as fixed. Let me demonstrate.</p>
<p>Let <span class="math inline">\(S := \prod_{E_n^0} U(N) = U_0(B_n)\)</span> and <span class="math inline">\(T := \prod_{E_n^1} U(N)\)</span>. Let also <span class="math inline">\(\Omega = \prod_{E_n} U(N) = U(B_n)\)</span>. Let also <span class="math display">\[
Y := Y(\omega) := Y(U_e)_{e \in E_n} = (U_e)_{e \in E_n^0}
\]</span> and <span class="math display">\[
X := X(\omega) := X(U_e)_{e \in E_n} = (U_e)_{e \in E_n^1}.
\]</span> Note that <span class="math inline">\(V_i\)</span> can really be viewed as a function of the matrix variables on all edges <span class="math inline">\(E_n\)</span>. We can write: <span class="math display">\[
V_i(U_e)_{e \in E_n} = U_0 \cdots U_0 U(i) U_0 \cdots U_0
\]</span> where <span class="math inline">\(U_0\)</span> is a placeholder for matrices attached to <span class="math inline">\(E_n^0\)</span> edges, and <span class="math inline">\(U(i)\)</span> is a matrix attached to edge <span class="math inline">\(i \in E_n^1\)</span>. We can write in this way because <span class="math inline">\(G_U(x)\)</span> is just a product of <span class="math inline">\(U_0(B_n)\)</span> matrices for any <span class="math inline">\(x \in B_n\)</span>, and <span class="math inline">\(G_U\)</span> is deterministically a function of the variables <span class="math inline">\(U_0(B_n)\)</span>.</p>
<p>For ease of notation, let <span class="math display">\[U_1 := (U_e)_{e \in E_n^1}\]</span> and <span class="math display">\[U_0 := (U_e)_{e \in E_n^0} \in U_0(B_n).\]</span> Thus, we consider the definition <span class="math display">\[\begin{align*}
\psi(X,Y) &amp;= \psi(U_0,U_1)\\
&amp;= f(U_0 U(i) U_0) g(U_0 U(j) U_0).
\end{align*}\]</span></p>
<p>Then by the Lemma, we have</p>
<p><span class="math display">\[\begin{align*}
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](\omega) &amp;=
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](U_e)_{e \in E_n} \\
&amp;= \mathbb E[ f(V_i(U_e)_{e \in E_n^0}) g(V_j(U_e)_{e \in E_n^0}) ] \\
&amp;= \int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \prod_{e \in E_n} d\sigma(U'(e))
\end{align*}\]</span></p>
<p>Then by left and right invariance of the Haar measure, we compute <span class="math display">\[\begin{align*}
\int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \prod_{e \in E_n} d\sigma(U'(e)) &amp;= \int_{U(B_n)} f(U'(i)) g(U'(j)) \prod_{e \in E_n} d\sigma(U'(e))
\end{align*}\]</span></p>
<p>Hence, <span class="math display">\[
\mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ](\omega) = \mathbb E_{U'} [f(U'(i)) g(U'(j))].
\]</span></p>
<p>Then, <span class="math display">\[\begin{align*}
\mathbb E [ f(V_i) g(V_j) ] &amp;= \mathbb E [ \mathbb E[ f(V_i) g(V_j) \mid U_0(B_n) ]] \\
&amp;= \int_{U(B_n)} \mathbb E_{U'} [f(U'(i)) g(U'(j))] \prod_{e \in E_n} d\sigma(U_e)\\
&amp;= \mathbb E_{U'} [f(U'(i)) g(U'(j))] \int_{U(B_n)} 1 \prod_{e \in E_n} d\sigma(U_e),
\end{align*}\]</span> where we pulled a “constant” out of the integral. Then by Haar invariance and Fubini a couple times to split up <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, we conclude the result <span class="math display">\[\begin{equation}
\mathbb E [ f(V_i) g(V_j) ] = \mathbb E [ f(V_i) ] \mathbb E [ g(V_j) ].
\end{equation}\]</span></p>
<p>I’m tired of this article for today, but one improvement would be to write out explicitly what the product <span class="math inline">\(U_0 \cdots U_0\)</span> actually is. In particular, I should first write down the tree with root at the origin in the lattice. Then I should rewrite <span class="math inline">\(i \in E_n^1\)</span> as an edge of the form <span class="math inline">\((i_l, i_r)\)</span>, and then <span class="math inline">\(G_U(i_l)\)</span> will just be the product of <span class="math inline">\(U_0\)</span> matrices indexed by the edges from the origin <span class="math inline">\(0\)</span> to <span class="math inline">\(i_l\)</span> along the tree. So if these tree edges are <span class="math inline">\(t_0, \cdots, t_{i_l}\)</span>, then the product would be <span class="math display">\[
U_0 \cdots U_0 = U(t_0) \cdots U(t_{i_l}).
\]</span></p>
</section>
</section>
<section id="january-3-2026" class="level3">
<h3 class="anchored" data-anchor-id="january-3-2026">January 3, 2026</h3>
<p>I’m studying section 12 in <span class="citation" data-cites="Chatterjee2016">Chatterjee (<a href="#ref-Chatterjee2016" role="doc-biblioref">2016</a>)</span>, which is titled Some Standard Results About Gaussian Measures. I’d like to understand why <span class="math inline">\(Q\)</span> being a positive definite <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span> real matrix implies that <span class="math display">\[x^TQx + v^Tx + C =: P(x) \ge c \lVert x \rVert^2\]</span> for some positive constant <span class="math inline">\(c\)</span>, for all <span class="math inline">\(\Vert x \rVert\)</span> sufficiently large, where <span class="math inline">\(x = (x_1, \cdots, x_n) \in \mathbb R^n\)</span>. Maybe I should first mention what this is saying. When a real matrix <span class="math inline">\(Q\)</span> is positive definite, it means that the polynomial <span class="math display">\[ \begin{matrix}
Q_{11} x_1^2 &amp;+&amp; \cdots &amp;+&amp; Q_{1n} x_1 x_n\\
\vdots &amp;&amp; \ddots &amp;&amp; \vdots \\
Q_{n1} x_1x_n &amp;+&amp; \cdots &amp;+&amp; Q_{nn} x_n^2
\end{matrix}\]</span> is strictly positive for all <span class="math inline">\(x \ne 0\)</span>. So the matrix <span class="math inline">\(Q\)</span> is the coefficient matrix of the degree 2 monomials. Oh yeah, and <span class="math inline">\(P(x)\)</span> above is just a general degree 2 polynomial in the real variables <span class="math inline">\(x_1, \cdots, x_n\)</span>. I’ll mention that the variables commute, so we can just assume that <span class="math inline">\(Q\)</span> is symmetric, because we can break up the coefficients into two equal halves. The main point of all this is that <span class="math inline">\(P(x) \ge c \lVert x \rVert^2\)</span> is a necessary and sufficient criterion for the integrability of a Gaussian density term for a Gaussian measure (see section 12), so knowing that <span class="math inline">\(Q\)</span> being positive definite is equivalent allows you to later bring in stuff about smallest eigenvalues of <span class="math inline">\(Q\)</span>, linear algebra stuff, and other information that is not obvious from first glance.</p>
<p>This implication of <span class="math inline">\(Q\)</span> positive definite implies <span class="math inline">\(P(x) \ge c \lVert x \rVert^2\)</span> is too hard for my little brain, so as a first step I need to consider an easier case: assume <span class="math inline">\(v = 0\)</span> and <span class="math inline">\(C = 0\)</span>, so the polynomial <span class="math inline">\(P\)</span> is just made up of the terms in <span class="math inline">\(x^TQx\)</span>. So I want to show that <span class="math display">\[P(x) = x^T Qx \ge c \lVert x \rVert^2\]</span> for some <span class="math inline">\(c &gt; 0\)</span> and for all <span class="math inline">\(\lVert x \rVert &gt; r &gt; 0\)</span>, where <span class="math inline">\(r\)</span> is some radius that is big enough.</p>
<p>Let’s assume that <span class="math display">\[R_* := \inf_{\lVert x \rVert = 1} x^TQx &gt; 0.\]</span> Hopefully I can prove later on that <span class="math inline">\(Q\)</span> being positive definite implies this infimum statement truly does hold, but I believe <span class="math inline">\(Q\)</span> being symmetric is also needed.</p>
<p>If this infimum statement does truly hold, then since the infimum is a lower bound, then for any <span class="math inline">\(\lVert x \rVert = 1\)</span>, we get the inequality <span class="math display">\[x^TQx \ge R_*\]</span> and since <span class="math inline">\(\lVert x \rVert^2 = 1\)</span>, we also have <span class="math display">\[x^TQx \ge R_* \lVert x \rVert^2.\]</span></p>
<p>Then take some arbitrary <span class="math inline">\(x \in \mathbb R^n \setminus \overline{B}(0,1)\)</span>, that is, <span class="math inline">\(\lVert x \rVert &gt; 1.\)</span> Scaling down <span class="math inline">\(x\)</span>, we see that <span class="math inline">\(\frac{x}{\lVert x \rVert}\)</span> is on the sphere of radius 1. So it holds that <span class="math display">\[\frac{x}{\lVert x \rVert}^T Q \frac{x}{\lVert x \rVert} \ge R_* \lVert \frac{x}{\lVert x \rVert} \rVert^2.\]</span> Then multiplying through by <span class="math inline">\(\lVert x \rVert^2\)</span>, we see that <span class="math display">\[x^T Q x \ge R_* \lVert x \rVert^2.\]</span> We have thus shown that with <span class="math inline">\(c := R_*\)</span> and <span class="math inline">\(r = 1\)</span>, then for all <span class="math inline">\(\lVert x \rVert \ge r\)</span>, we have <span class="math inline">\(P(x) = x^T Qx \ge c \lVert x \rVert^2,\)</span> the desired conclusion.</p>
<p>Here are some other observations that don’t fit anywhere yet:</p>
<section id="random-oberservations" class="level4">
<h4 class="anchored" data-anchor-id="random-oberservations">Random Oberservations</h4>
<p>Observation 1: <span class="math inline">\(c\lVert x \rVert^2 = cx_1^2 + \cdots + cx_n^2.\)</span></p>
<p>Observation 2: Consider the expression <span class="math inline">\(x^TQx\)</span> as being built of <span class="math display">\[x^TQx = \text{diagonal}(Q,x) + \text{off-diagonal}(Q,x),\]</span> where <span class="math display">\[\text{diagonal}(Q,x) = Q_{11}x_1^2 + \cdots + Q_{nn} x_n^2\]</span> and <span class="math display">\[\text{off-diagonal}(Q,x) = \sum_{i \ne j} Q_{ij}x_ix_j.\]</span></p>
<p>Observation 3: I notice that the diagonal entries of <span class="math inline">\(Q\)</span> must be positive. Here’s why. If <span class="math inline">\(e_i = (0, \cdots, 1, \cdots, 0) \in \mathbb R^n\)</span> is the vector with <span class="math inline">\(0\)</span> at every index besides the <span class="math inline">\(i\)</span>th index, then <span class="math inline">\(Q_{ii} = e_i^{T} Q e_i &gt; 0\)</span>, by positive definiteness.</p>
</section>
<section id="further-discussion" class="level4">
<h4 class="anchored" data-anchor-id="further-discussion">Further Discussion</h4>
<p>I wish I had time to graph some of the simpler cases. For instance, the case of <span class="math inline">\(\mathbb R^n = \mathbb R^2\)</span>, where we can name the variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and then we can see the conic sections. Then the off diagonal part looks like <span class="math display">\[\begin{align*}
\text{off-diagonal}(Q,(x,y)) = Q_{12}xy + Q_{21}xy,
\end{align*}\]</span> which, when plotted as the graph of the function <span class="math inline">\(z(x,y) = (Q_{12}+Q_{21})xy\)</span> in <span class="math inline">\(\mathbb R^3\)</span>, looks like a hyperbolic parabaloid. The diagonal part, being a quadratic form with positive coefficients, is always positive, and then <span class="math inline">\(x^TQx&gt; 0\)</span> takes the geometric meaning that the negaative part of the hyperbolic parabaloid is dominated by the diagonal part.</p>
</section>
<section id="still-needed" class="level4">
<h4 class="anchored" data-anchor-id="still-needed">Still Needed</h4>
<p>Next I need to show that <span class="math inline">\(Q\)</span> being positive definite (and symmetric) implies that <span class="math display">\[R_* := \inf_{\lVert x \rVert = 1} x^TQx &gt; 0.\]</span></p>
<p>Then I need to tackle the case where the linear part <span class="math inline">\(v^Tx\)</span> is non zero and the constant <span class="math inline">\(C\)</span> is also non-zero.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-BrenneckeCQFT" class="csl-entry" role="listitem">
Brennecke, Christian. 2026. <span>“Introduction to Constructive Quantum Field Theory.”</span> Lecture Notes. <a href="https://www.iam.uni-bonn.de/users/brennecke/home">https://www.iam.uni-bonn.de/users/brennecke/home</a>.
</div>
<div id="ref-Chatterjee2016" class="csl-entry" role="listitem">
Chatterjee, Sourav. 2016. <span>“The Leading Term of the Yang-Mills Free Energy.”</span>
</div>
<div id="ref-Dimock" class="csl-entry" role="listitem">
Dimock, Jonathan. 2011. <em>Quantum Mechanics and Quantum Field Theory: A Mathematical Primer</em>. Cambridge University Press.
</div>
<div id="ref-Amann" class="csl-entry" role="listitem">
Herbert Amann, Joachim Escher. 2009. <em>Analysis III</em>. Birkhäuser.
</div>
<div id="ref-LeeSM" class="csl-entry" role="listitem">
Lee, John. 2013. <em>An Introduction to Smooth Manifolds</em>. Springer.
</div>
<div id="ref-OneillElem" class="csl-entry" role="listitem">
O’Neill, Barrett. 1966. <em>Elementary Differential Geometry</em>. Academic Press.
</div>
<div id="ref-TaoAna2" class="csl-entry" role="listitem">
Tao, Terence. 2006. <em>Analysis 2</em>. Hindustan.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/williamclarktennis\.github\.io\/YM-explorations\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>