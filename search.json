[
  {
    "objectID": "December2025.html",
    "href": "December2025.html",
    "title": "December 2025",
    "section": "",
    "text": "December 11, 2025\nToday I visited Glimm and Jaffe (1987). It is a technical work. It explains the program of constructive quantum field theory. One idea was the following: partial differential equations generalize ordinary differential equations and quantum field theories generalize partial differential equations. ODEs have one degree of freedom, usually time \\(t\\). PDEs usually have 3 or 4 degrees of freedom, say, \\(t, x, y,\\) and \\(z\\). But quantum fields have infinitely many degrees of freedom.\nODE -&gt; PDE -&gt; Quantum Fields\nRecall briefly that in a classical field, say, a vector field, one attaches a vector to each point in space. The prototypical example of a classical field is an electromagnetic field. In classical mechanics, one observes the trajectory of a particle, and its state is described by 6 degrees of freedom: 3 coordinates of position and 3 coordinates of momentum.\nSo, what are the degrees of freedom for a classical field? Note that each point in space should have a vector attached to it, say, a 3-vector. Wait, what is meant by degree of freedom for the case of a PDE depending on \\(x_1, x_2, x_3, p_1, p_2, p_3\\)? It should mean that, when values for these degrees of freedom are chosen, then all other observables are determined, so there is no more freedom left. In a classical field, then, we expect that the phase space again has something like 6 degrees of freedom. Once position and momentum are fixed, then all the vectors attached to any point in the phase space are determined.\nSo how could it be that a quantum field has infinitely many degrees of freedom? A quantum field is morally an operator attached to each point in space. I feel like there are still only finitely many degrees of freedom. Shouldn’t the operators be determined by whichever point in 4-space you are in?\n\n\nDecember 15, 2025\nToday I spent some time reading Chatterjee (2016). I feel like I have learned a little after spending time on other literature in the Communications of Mathematical Physics and Glimm and Jaffe (1987), but it felt very weak while reading Chatterjee (2016) again.\nI would like to understand the meaning of Chatterjee (2016)’s Theorem 2.1 statement and how it implies the results for the leading term of the free energy for \\(d = 3\\) and \\(d = 4\\), and also why one cannot conclude as much in the \\(d=4\\) case.\nI do see, vaguely, how Theorem 2.1 involves a renormalization (subtracting off \\(d\\) and \\(1/n\\) and so forth). I could see as well the results from section 8 and 9 on the a priori bounds, but they seemed not that impressive because the upper bound was quite huge.\n\n\nDecember 17, 2025\nToday I spent time reading Brennecke (2024) and Brennecke (2026). I’m not sure whether it is appropriate to cite lecture notes, but I mainly wanted to point out that I read Brennecke’s notes today.\nI’ve been mulling over representations. Let \\(G\\) be a group and let \\(V\\) be a vector space. A representation of \\(G\\) is a map \\(\\rho: G \\to GL(V)\\), where \\(GL(V)\\) is the set of invertible linear maps from the vector space \\(V\\) to itself. A finite dimensional representation is a representation where \\(V\\) is finite-dimensional.\nI’m confused about whether finite-dimensional representations arise in the context of the state transformation law for quantum fields. The state transformation law for a quantum field is a representation of the Poincaré group on the state space of the quantum system. In symbols: \\[\\begin{equation}\n(a,L) \\ni \\mathcal{P} \\to U(a,L) \\in \\mathcal{U}(\\mathcal{H}).\n\\end{equation}\\] Thus, \\(U(a,L)\\) is a mapping from the infinite dimensional space \\(\\mathcal{H}\\) to itself. So I don’t see how a finite dimensional representation could arise from the state transformation law.\nMy next guess as for where finite dimensional representations would arise in a quantum field theory is in the field transformation law. That is because one considers \\(n\\) by \\(n\\) matrices to describe the field transformation law. I need to recall field transformations in classical fields. Recall the notion of a vector field. And recall the transformation rules for derivations. I think the point is to describe the fields in different coordinate systems.\nPost Script: as a response to my December 11th note, I believe the infinite degrees of freedom arises from the partial derivative: \\[\\begin{equation}\n\\partial_\\psi f(\\pi) = \\frac{d}{dt} f(\\pi + t \\psi)\n\\end{equation}\\] or something of that sort, where one has the infinite dimensional space of \\(\\psi\\)’s to choose from.\n\n\nDecember 18, 2025\nIn this note I want to summarize what Prof. Brennecke told me today in my thesis meeting.\nWhat Brennecke told me was quite enlightening. In short, he pointed to each term in the Chatterjee formula and said what it meant in terms of the rest of the paper. I wish I had thought of doing that earlier.\nRecall that \\(o(n^d)\\) as \\(n \\to \\infty\\) is a term \\(g(n)\\) that satisfies \\(g(n) / n^d \\to 0\\) as \\(n \\to \\infty\\). For instance, it could be \\(g(n) = n^{d-1}\\) because then there is still an \\(n\\) left in the denominator. Basically any function of \\(n\\) that grows an order more slowly than \\(n^d\\) in the large \\(n\\) limit. For intuition related to the lattice model, recall that there are \\(n^d\\) vertices, so something that is \\(n^{d-1}\\) could be, for instance, the boundary vertices. Maybe as a future exercise, I can create a table with stuff having the various orders in the lattice model.\nRecall also that \\(O(1/n)\\) as \\(n \\to \\infty\\) referes to functions \\(g(n)\\) that are eventually of the same order as \\(1/n\\) when \\(n\\) is large.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0.1, 2, 0.01)\ny = 1/x\nfig, ax = plt.subplots()\n\nax.plot(x,y)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A visual of 1/n as n goes to infinity\n\n\n\n\n\nSo Chatterjee, in Chatterjee (2016), basically proves the formula: \\[\\begin{equation}\n\\frac{\\log Z}{n^d} = O(1)N^2\\log(g_0^2) + (d-1)\\log \\frac{\\prod_{i=1}^{N-1}j!}{(2\\pi)^{N/2}} + N^2 K_d,\n\\end{equation}\\] as \\(n \\to \\infty\\) and \\(g_0 \\to 0\\).\nThere are three terms on the right hand side. Brennecke says that the second term comes from a Radon Nikodym derivative. Let me think about that. The only sensible place a Radon Nikodym derivative could show up is when comparing the Haar measure on \\(U(N)\\) with the Lebesgue measure on \\(H(N)\\). Wait, no. Perhaps it is when comparing the Lebesgue measure on \\(H(N)\\) with the usual Lebsgue measure on \\(\\mathbb R^{2n}\\). Ok, I looked in Chatterjee (2016) section 11 and let me summarize:\nRecall that the exponential map maps Lie algebra to Lie group. In our context, the Lie algebra is the Hermitian matrices \\(H(N)\\) and the Lie group is the unitary matrices \\(U(N)\\). We already have Haar measure on \\(U(N)\\). Now, what is the only way to use the exponential map to push the Haar measure or pull the Haar measure onto the Hermitian matrices? Well, given a subset \\(A \\subset H(N)\\), we have to send that subset \\(A\\) over to the Lie group via \\(\\psi(\\cdot) = e^{i(\\cdot)}\\), the exponential map (with an i-factor), and measure it according to Haar measure: \\[\\begin{equation}\n\\nu(A) = \\sigma (\\psi(A)),  \n\\end{equation}\\] which is the formula on page 26 of Chatterjee (2016).\nOkay, I’m a bit stuck on how the second term comes from the Radon Nikodym derivative. I’ll continue that discussion later.\nThere is also the \\(N^2K_d\\) term which comes from lattice Maxwell theory, which is a Gaussian theory where you can explicitly compute things (or so I’m told). Finally, I’m not immediately sure where the \\(\\log(g_0^2)\\) term comes from. Brennecke mentioned the discrete Poincaré inequality, so it could come from there. Let me check. I don’t think that relates, rather I think the discrete Poincaré inequality allows one to say something is positive definite, which enables the Gaussian stuff to begin.\nAh wait, there are actually two Poincaré inequalities in Chatterjee (2016). The one I mentioned in the previous paragraph is Lemma 13.1. The other one that I missed is Lemma 10.2. Actually, now I understand another one of Brennecke’s comments, namely that axial guage fixing is required in order to make a smallness of the Wilson action argument, and that without it, there is no path to success. I think I would need to investigate the proof of Lemma 17.2 to be able to see how the first term arises in relation to axial guage fixing and so forth.\n\n\nDecember 19, 2025\nI want to record some observations about the Wilson action and some estimates. Consider \\[\\begin{equation}\nS_{B_n}(U) \\le C n^d \\frac{\\log \\beta }{\\beta}.\n\\end{equation}\\] Let me explain the various terms involved here. \\(Cn^d\\) arises as an estimate on the number of elements in the set \\(|B_n'|\\), which is the set of plaquettes for the box \\(B_n\\). The reciprocal of \\(\\beta\\) term should be moved to the left hand side and related to the usual integrand \\(\\exp(\\beta S_{B_n}(U))\\), which is one factor in the Radon Nikodym derivative \\(\\frac{d\\mu_{YM}}{d\\sigma_n}\\).\nThe \\(\\log \\beta\\) term is a bit harder. From the proof of Theorem 7.1 Chatterjee (2016), we have \\[\\begin{equation}\n\\sigma_{B_n} \\{ U: S_{B_n}(U) \\le |B_n'|/\\beta \\} \\ge (C_1 \\frac{1}{\\sqrt{8 \\beta}})^{C_3 n^d}\n\\end{equation}\\] where \\(C_1\\) is from Corollary 6.3, which gives an estimate on a small ball probability for unitary group, and \\(C_3n^d\\) arises as a term to help approximate the number of edges in the box \\(B_n\\) and to compensate for the term \\(N^2\\) in Corollary 6.3. Multiplying both sides by \\(e^{-|B_n'|}\\), we get \\[\\begin{equation}\ne^{-|B_n'|} \\sigma_{B_n} \\{ U: S_{B_n}(U) \\le |B_n'|/\\beta \\}\n\\ge e^{-|B_n'|} (C_1 \\frac{1}{\\sqrt{8 \\beta}})^{C_3 n^d}.\n\\end{equation}\\] Can the right hand side be written as \\(\\exp(-Cn^d \\log \\beta)\\) for some positive constant \\(C\\) (as in the statement of Theorem 7.1)? Working backwards, yes. The \\(C\\) term is malleable, so write \\(\\exp(-2Cn^d \\log \\beta) = \\exp(-Cn^d \\log \\beta - Cn^d \\log \\beta) = \\exp(-Cn^d \\log \\beta) \\exp( - Cn^d \\log \\beta)\\). One of the terms on the far right hand side can account for \\(e^{-|B_n'|}\\). That is, \\[\\begin{equation} e^{-|B_n'|} \\approx \\exp(-Cn^d \\log \\beta) \\end{equation}\\] Also, by the \\(x^{mn} = (x^m)^n\\) rule for exponents, we have \\(\\exp( - Cn^d \\log \\beta) = (\\exp( \\log \\frac{1}{\\beta}) )^{Cn^d} = (\\frac{1}{\\beta})^{Cn^d}.\\) Then \\[\\begin{equation}\n(\\frac{1}{\\beta})^{Cn^d} \\approx (C_1 \\frac{1}{\\sqrt{8 \\beta}})^{C_3 n^d}\n\\end{equation}\\] I’m tired, so I won’t finish except to write that the product Haar measure (\\(\\sigma_{B_n}\\)) of configurations where the contribution of each plaquette to the Wilson action is on average less than \\(\\beta\\), is lower bounded as \\[\\begin{equation}\n\\sigma_{B_n} \\{ U: S_{B_n}(U) \\le |B_n'|/\\beta \\} \\ge \\exp(-Cn^d \\log \\beta).\n\\end{equation}\\] I still need to figure out how to explain the term \\(\\log \\beta\\) in the first equation in today’s entry.\n\n\nDecember 22, 2025\nIn this post I want to write a detailed proof on the same content from my December 19th post. This follows Sections 6,7, and 8 from Chatterjee (2016).\n\nLemma (from Proof of Thm 7.1):\nSuppose \\(\\beta \\ge 2\\), \\(\\delta \\in (0,\\sqrt N)\\), and \\(8 \\delta^2 = \\frac{1}{\\beta}\\). Then there exists a constant \\(C\\) depending only on \\(N\\) and \\(d\\) such that \\[\\begin{equation}\n\\sigma_{B_n}(\\{ U: S_{B_n}(U) \\le |B_n'|/ \\beta \\}) \\ge e^{-Cn^d \\log \\beta}.\n\\end{equation}\\]\n\n\n\n\n\n\nNoteProof\n\n\n\nThe inequality \\[\\begin{align*}\n\\sigma_{B_n}(\\{ U: S_{B_n}(U) \\le |B_n'|/ \\beta \\}) &\\ge \\prod_{i=1}^{|E_n|} \\sigma(\\{ U: \\lVert I - U \\rVert \\le \\delta \\}) && \\text{where }8 \\delta^2 = \\frac{1}{\\beta}\n\\end{align*}\\] means that under product Haar measure, the probability that the average contribution of plaquettes to the Wilson action being less than \\(\\frac{1}{\\beta}\\), is approximated from below by the probability of edges carrying unitary matrices that are \\(\\delta = \\frac{1}{\\sqrt{8\\beta}}\\)-close to the identity matrix. Next, from the lower bound on the small ball probabilities for unitary matrices, we have \\[\\begin{align*}\n\\prod_{i=1}^{|E_n|} \\sigma(\\{ U: \\lVert I - U \\rVert \\le \\delta \\}) &\\ge \\prod_{i=1}^{|E_n|} C_1 \\delta^{N^2} \\\\\n&= \\prod_{i=1}^{|E_n|} (\\sqrt[N^2]{C_1}^2 \\delta^2)^{N^2/2} \\\\\n&= \\prod_{i=1}^{|E_n|} (\\sqrt[N^2]{C_1}^2 \\frac{1}{8 \\beta})^{N^2/2} \\\\\n&= \\prod_{i=1}^{|E_n|\\frac{N^2}{2}} \\frac{\\sqrt[N^2]{C_1}^2}{8} \\frac{1}{\\beta}.\n\\end{align*}\\] We may assume without loss of generality that \\(C_1 &lt; 1\\) because \\(C_1\\) is a factor for the lower bound in Corollary 6.3. Thus, \\(\\frac{\\sqrt[N^2]{C_1}^2}{8} &lt; 1\\), so taking more powers decreases the value. Also, \\(\\beta \\ge 2\\) by assumption, so taking more powers of \\(\\frac{1}{\\beta}\\) also decreases the value. By Lemma 17.1, we have that \\(|E_n| \\le dn^d\\), so replacing \\(|E_n|\\) by \\(dn^d\\) leads to \\[\\begin{align*}\n\\prod_{i=1}^{|E_n|\\frac{N^2}{2}} \\frac{\\sqrt[N^2]{C_1}^2}{8} \\frac{1}{\\beta} & \\ge \\prod_{i=1}^{\\frac{N^2}{2}dn^d} \\frac{\\sqrt[N^2]{C_1}^2}{8} \\prod_{i=1}^{\\frac{N^2}{2} dn^d} \\frac{1}{\\beta}.\n\\end{align*}\\] Next, observe that we can take enough powers of \\(\\frac{1}{2}\\) to lower bound \\(\\frac{\\sqrt[N^2]{C_1}^2}{8}\\), and the number of powers that are taken depends only on \\(N\\) and \\(d\\), as \\(C_1\\) only depended on \\(N\\). Thus, for some positive constant \\(C\\) depending only on \\(N\\) and \\(d\\), we know that \\[\\begin{align*}\n\\prod_{i=1}^{\\frac{N^2}{2}dn^d} \\frac{\\sqrt[N^2]{C_1}^2}{8} \\prod_{i=1}^{\\frac{N^2}{2} dn^d} \\frac{1}{\\beta} &\\ge \\prod_{i=1}^{\\frac{N^2}{2}dn^d} \\prod_{j=1}^C \\frac{1}{2} \\prod_{i=1}^{\\frac{N^2}{2} dn^d} \\frac{1}{\\beta} \\\\\n& \\ge \\prod_{i=1}^{C \\frac{N^2}{2} dn^d} \\frac{1}{\\beta},\n\\end{align*}\\] where \\(\\beta \\ge 2\\) was used in the last step. Finally, we get \\[\\begin{align*}\n\\prod_{i=1}^{C \\frac{N^2}{2} dn^d} \\frac{1}{\\beta} &= (e^{-\\log \\beta})^{Cn^d} && C \\mapsto C \\frac{N^2}{2} d \\\\\n&= e^{-Cn^d \\log \\beta}.\n\\end{align*}\\] Recalling the beginning of the proof, we conclude \\[\\begin{align*}\n\\sigma_{B_n}(\\{ U: S_{B_n}(U) \\le |B_n'|/ \\beta \\}) &\\ge e^{-Cn^d \\log \\beta}.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nReferences\n\nBrennecke, Christian. 2024. “Mathematical Quantum Mechanics with Applications.” Lecture Notes. https://www.iam.uni-bonn.de/users/brennecke/home.\n\n\n———. 2026. “Introduction to Constructive Quantum Field Theory.” Lecture Notes. https://www.iam.uni-bonn.de/users/brennecke/home.\n\n\nChatterjee, Sourav. 2016. “The Leading Term of the Yang-Mills Free Energy.”\n\n\nGlimm, James, and Arthur Jaffe. 1987. Quantum Physics: A Functional Integral Point of View."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yang-Mills Lattice Gauge Theory - William’s Explorations",
    "section": "",
    "text": "January 22, 2026\nIn this post I want to learn about the spectral theorem in the context of quantum mechanics.\nThe following quote is a passage from Brennecke (2026).\n\n\n\n\n\n\nNotePassage from Brennecke notes\n\n\n\n“Based on the spectral theorem, let us point out how, in the context of quantum mechanics, observables like the position or the momentum of a particle are connected with self-adjoint operators. Suppose that \\(A: D_A \\to \\mathcal{H}\\) represents some observable \\(\\mathcal{O}\\) and that the system is in state \\(\\psi \\in \\mathcal{H}\\). Then, based on the normalization \\(\\lvert \\psi \\rvert = 1\\) and on the spectral decomposition (1.11) of \\(A\\), one identifies \\(\\mathcal{O}\\) with a real-valued random variable (ranging almost surely in the spectrum \\(\\sigma(A) \\subset \\mathbb R\\) of \\(A\\)) and the probability \\(\\mathbb P\\) that \\(\\mathcal{O}\\) takes a specific value in some measurable set \\(\\Omega \\in \\mathcal{B}(\\mathbb R)\\) is defined as \\[\n\\mathbb P(\\mathcal{O} \\in \\Omega) = \\int_{\\Omega} \\langle \\psi, \\chi_{d\\lambda}(A) \\psi \\rangle\n= \\langle \\psi, \\chi_{\\Omega}(A) \\psi \\rangle.\n\\tag{1}\\]\nNotice that the law \\(\\Omega \\mapsto \\mathcal{O}_*(\\mathbb P)(\\Omega)\n= \\mathbb P(\\mathcal{O} \\in \\Omega)\\) defines indeed a Borel probability measure on \\(\\mathbb R\\)” (p 12).\nAlso, for easy reference, (1.11) says, “In terms of the projection-valued measure, \\(A: D_A \\to \\mathcal{H}\\) has the spectral decomposition \\[\nA = \\int_{\\sigma(A)} \\lambda \\chi_{d\\lambda}(A),\n\\] where \\(\\sigma(A) \\subset \\mathbb R\\) denotes the spectrum of \\(A\\).”\n\n\n\nExplanation of the probability part\nFirst, the quantum system is in state \\(\\psi \\in \\mathcal{H}\\). We want to know the chance that some observable \\(\\mathcal{O}\\) takes values in the subset \\(\\Omega \\subset \\sigma(A)\\), based on the fact that the system is in state \\(\\psi\\). For instance, if \\(A\\) has actual discrete eigenvalues, say \\((\\lambda_i)_{i \\in [n]}\\), then we might want to know the chance that \\(\\mathcal{O} = \\lambda_i\\) for some \\(i\\).\nWell, the spectral theorem says roughly that the Hilbert space \\(\\mathcal{H}\\) can be decomposed based on the spectrum of \\(A\\): \\[\n\\mathcal{H} = \\oplus_{\\lambda \\in \\sigma(A)}E_\\lambda\n\\] where \\(E_\\lambda\\) is the subspace spanned by eigenvectors associated to \\(\\lambda\\).\nWhen looking only at a subset \\(\\Omega\\), then \\(H_{\\Omega} := \\oplus_{\\lambda \\in \\Omega} E_\\lambda\\) forms some subspace of \\(\\mathcal{H}\\). My thinking is then that this subspace \\(H_\\Omega\\) is the set of quantum states that are associated to the observable \\(\\mathcal{O}\\) ranging in \\(\\Omega\\).\nNext, note that \\(\\chi_\\Omega(A)\\) is the orthogonal projection onto the subspace \\(H_\\Omega\\). Let’s assume the case that \\(\\psi\\) is not already in the subspace \\(H_\\Omega\\). Then we get the following picture. \nGiven the picture, which shows an orthogonal decomposition of the vector \\(\\psi\\), how do we interpret \\(\\langle \\psi , \\chi_{\\Omega}(A) \\psi \\rangle\\) as a probability? Note that, using \\(\\lVert \\psi \\rVert = 1\\), it follows that\n\\(\\langle \\psi, \\chi_\\Omega(A) \\psi \\rangle = \\cos \\theta \\lVert \\chi_\\Omega(A) \\psi \\rVert.\\) From here, it makes sense to me that \\(\\psi\\) being closer to the subspace \\(H_\\Omega\\), which corresponds to the observable \\(\\mathcal{O}\\) being more likely to take values in \\(\\Omega\\), is reflected by an higher value of \\(\\langle \\psi, \\chi_{\\Omega} \\psi \\rangle\\), because \\(\\theta \\to 0\\) in this case.\n\n\nFurther Ramblings about definitions\nIf \\(A\\) is finite dimensional, then \\(\\sigma(A)\\) just consists of a total of \\(\\text{rank}(A)\\) points, and then the spectral decomposition (1.11) reads \\[\nA  = \\int_{\\sigma(A)} \\lambda \\chi_{d\\lambda}(A) = \\sum_{\\lambda \\in \\sigma(A)} \\lambda U^* (\\chi_{\\{\\lambda\\}}) U = \\sum_{\\lambda \\in \\sigma(A)} \\lambda \\langle \\phi_\\lambda, \\cdot \\rangle \\phi_\\lambda\n\\] where \\(\\phi_\\lambda\\) is an eigenvector associated to the eigenvalue \\(\\lambda\\).\nHow do you use the formula \\(A  = \\int_{\\sigma(A)} \\lambda \\chi_{d\\lambda}(A)\\) to compute \\(A v\\) for some \\(v \\in D_A\\)? When \\(A\\) is finite dimensional, it is \\(Av = \\sum_{\\lambda \\in \\sigma(A)} \\lambda \\langle \\phi_\\lambda, v \\rangle \\phi_\\lambda.\\) For the infinite dimensional case, does Equation 1 provide a clue? If so, then here’s my guess: \\[\nA v = \\int_{\\sigma(A)} \\lambda \\langle v, \\chi_{d\\lambda}(A)v \\rangle.\n\\] How do I check whether or not this guess is correct? Well, the left hand side \\(Av\\) is a vector in the Hilbert space \\(\\mathcal{H}\\), whereas the right hand side is just a sum of complex numbers, which is not a Hilbert space element. Thus, the guess is incorrect. Let me make another guess based on the notion that \\(\\chi_{d\\lambda}(A)\\) is a spectral projection valued measure. In analogy to the finite dimensional case, \\(\\chi_{d\\lambda}(A)\\) should project \\(v\\) onto the subspace of \\(\\mathcal{H}\\) consisting of eigenvectors associated to eigenvalues \\(\\lambda \\in d\\lambda\\), where \\(d\\lambda\\) I think of as a very small interval on the real line. Anyways, based on this analogy, my next guess is: \\[\nAv = \\int_{\\sigma(A)} \\lambda \\left( \\chi_{d\\lambda}(A)v \\right).\n\\] Actually, this must be correct, and it does agree with the finite dimensional case (identifying \\(\\chi_{d\\lambda}(A)v\\) and \\(\\langle \\phi_\\lambda, \\cdot \\rangle \\phi_\\lambda\\) from above), but can I make sense of the integral on the right hand side based on my knowledge of integration theory? The issue is, given that \\(\\chi_{\\Omega}(A)v \\in \\mathcal{H}\\) for any subset \\(\\Omega \\subset \\sigma(A)\\), then I’m looking at an infinite sum of Hilbert space elements. Thus, not only is this a vector-valued integral for vectors in a finite-dimensional space, where the integral can just be treated component-wise, but it is vector-valued for vectors in an infinite-dimensional space. Time to pull out Herbert Amann (2009). There it is defined a so called Bochner-Lebesgue integral of a Banach-space-valued function \\(f\\) over a measure space \\(X\\) in Chapter X section 2. There is a lot of theory there, but I think what would help me in this post is to look at integrals of simple functions as approximators of the final integral \\(\\int_{\\sigma(A)} \\lambda \\left( \\chi_{d\\lambda}(A)v \\right)\\). Indeed, the idea behind the definition of the Bochner integral is to define the integral for simple functions, then set up a notion of a Cauchy sequence of these vector-valued simple functions (distinct from the notion that is available, for say, metric spaces like \\(L^2\\) with equivalence classes of functions that are not vector-valued) based on a seminorm defined by the integral of a simple function, then to say that the Bochner integral of a general function \\(f\\), where \\(f\\) is approached by the simple functions \\(\\phi_j\\) in the sense of \\(\\mu\\)-a.e. pointwise convergence, has Bochner integral defined by the limit \\(\\int \\phi_j\\), as long as the \\(\\phi_j\\) are a Cauchy sequence. So it is implied that it is a-priori possible for \\(\\phi_j\\) to approach \\(f\\) pointwise, but for \\(\\phi_j\\) to not form a Cauchy sequence, and thus to face ill-definedness of \\(\\int f\\). Otherwise, the Cauchy condition would be superfluous.\nSo to understand \\(\\int_{\\sigma(A)} \\lambda \\left( \\chi_{d\\lambda}(A)v \\right)\\), what would be the function \\(f\\), and what would be the simple functions \\(\\phi_j\\)? It is not clear to me how to write the integrand \\(\\lambda \\left( \\chi_{d\\lambda}(A)v \\right)\\) as an \\(\\mathcal{H}\\)-valued function of the variable \\(\\lambda\\) in the spectrum because \\(\\chi_{d\\lambda}(A)\\) is defined on subsets containing \\(\\lambda\\), not \\(\\lambda\\) pointwise. But assuming singleton sets are measurable, then let me tentatively write \\(f(\\lambda) =  \\left( \\chi_{\\{\\lambda\\}}(A)v \\right)\\).\n\n\n\nJanuary 21, 2026\n\n\n\n\n\n\nNoteAxial graph is a tree\n\n\n\nThe underlying undirected graph of the pair \\((B_n, E_n^0)\\) is a tree. In particular, given any vertex \\(x \\in B_n,\\) there is a unique sequence of \\(E_n^0\\) edges \\(e(x,1), \\cdots, e(x,|x|_1)\\) leading from the origin \\(0 \\in B_n\\) to \\(x\\).\n\n\n\n\n\n\n\n\nNoteDefinition of Axial Gauge Fixing\n\n\n\nWe define \\(G_n: U(B_n) \\times B_n\\) by \\[\nG_n(U, x) = G_n((U_e)_{e \\in E_n},x) = \\begin{cases}\nI & \\text{if }x = 0 \\\\\nU_{e(x,1)}U_{e(x,2)} \\cdots U_{e(x,|x|_1)} & \\text{if }x \\ne 0\n\\end{cases}\n\\] for all \\(U = (U_e)_{e \\in E_n} \\in U(B_n)\\). Observe that, \\(G_n(U, \\cdot) \\in G(B_n)\\) defines a gauge transform, that is, an assignment of unitary matrices to vertices.\nThen, given \\((U_e)_{e \\in E_n} = U \\in U(B_n)\\), and \\(G_U(x)\\) defined as in Section 9 of Chatterjee (2016) for all \\(x \\in B_n\\), we have the equality \\(G_U(x) = G_n(U, x)\\) for all \\(x \\in B_n\\). In other words, \\(G_U\\) and \\(G_n\\) produce the same gauge transform for a given configuration \\(U\\).\nFix a unitary configuration \\((U_e)_{e \\in E_n} = U \\in U(B_n)\\) and pick a gauge transform \\(G \\in G(B_n)\\). Then another configuration \\(V:= GU\\) is defined as \\[\nV(x,y) = G(x)U(x,y)G(y)^{-1}\n\\] for all \\((x,y) \\in E_n\\). In particular, \\(V\\) is a function of the configuration \\(U\\), but this is not expressed in the notation \\(V(x,y)\\).\nIn particular, \\(V := G_UU\\) and \\(V' := G_n(U, \\cdot)U\\) define the same unitary configuration: \\(V_e = V'_e\\) for all \\(e \\in E_n\\). Also, for any \\((x,y) \\in E_n\\), we note that \\[\nV(x,y) =\n\\begin{cases}\nU_{e(x,1)} \\cdots U_{e(x,|x|_1)} U(x,y) U^*_{e(y,|y|_1)} \\cdots U^*_{e(y,1)} & \\text{if } x \\ne 0 \\\\\nU(x,y) U^*_{e(y,1)} & \\text{if } x = 0.\n\\end{cases}\n\\]\n\n\nTake \\((x,y), (v,w) \\in E_n^1\\) distinct and assume that \\(x,y,w,v \\ne 0\\). I want to go through a proof that the \\(U(N)\\)-valued random variables \\[\nV(x,y), V(v,w): \\left(\\prod_{e \\in E_n} U(N), \\bigotimes_{e \\in E_n} \\mathcal{B}, \\prod_{e \\in E_n} d\\sigma(U_e) \\right) \\to (U(N), \\mathcal{B}, d\\sigma)\n\\] on the probability space \\[\n(\\Omega, \\mathcal{F}, P) := (U(B_n), \\mathcal{B}^{\\otimes E_n}, \\prod_{e \\in E_n} d\\sigma_e) = \\left(\\prod_{e \\in E_n} U(N), \\bigotimes_{e \\in E_n} \\mathcal{B}, \\prod_{e \\in E_n} d\\sigma(U_e) \\right)\n\\] are independent. Let \\(\\Pi_{E_n^1}: U(B_n) \\to \\prod_{e \\in E_n^1} U(N) =: U(E_n^1)\\) denote the projection onto the \\(E_n^1\\) coordinates, meaning \\[\n\\Pi_{E_n^1}((U_e)_{e \\in E_n}) = (U_e)_{e \\in E_n^1}.\n\\]\nLet \\(\\Pi_{E_n^0}: U(B_n) \\to \\prod_{e \\in E_n^0} U(N) =: U(E_n^0)\\) denote the projection onto the \\(E_n^0\\) coordinates, meaning \\[\n\\Pi_{E_n^0}((U_e)_{e \\in E_n}) = (U_e)_{e \\in E_n^0}.\n\\]\nThen \\(\\Pi_{E_n^0}\\) and \\(\\Pi_{E_n^1}\\) are random variables on the probability space \\(\\Omega = U(B_n)\\), and \\(\\Pi_{E_n^0}\\) is \\(\\mathcal{B}^{\\otimes E_n^0}\\) measurable, while \\(\\Pi_{E_n^1}\\) is \\(\\mathcal{B}^{\\otimes E_n^1}\\) measurable. Furthermore, \\(\\Pi_{E_n^0}\\) and \\(\\Pi_{E_n^1}\\) are independent.\nRecalling the definiton of \\((\\Omega, \\mathcal{F}, P)\\) above, and given a real-valued \\(L^1\\) random variable \\(f: \\Omega = U(B_n) \\to \\mathbb R\\), then the following relations are all just definitions:\n\\[\\begin{align*}\n\\mathbb E_P[f] & = \\int_{\\Omega} f(\\omega) dP (\\omega) \\\\\n&= \\int_{U(B_n)} f((U_e)_{e \\in E_n}) \\prod_{e \\in E_n} d \\sigma (U_e)\n\\end{align*}\\]\nBy Fubini, we also have \\[\\begin{align*}\n\\mathbb E_P[f] &= \\int_{U(E_n^0)} \\left( \\int_{U(E_n^1)} f((U_e)_{e \\in E_n}) \\prod_{e \\in E_n^1} d \\sigma (U_e) \\right) \\prod_{e \\in E_n^0} d \\sigma (U_e) \\\\\n&= \\int_{U(E_n^1)} \\left( \\int_{U(E_n^0)} f((U_e)_{e \\in E_n}) \\prod_{e \\in E_n^0} d \\sigma (U_e) \\right)\\prod_{e \\in E_n^1} d \\sigma (U_e).\n\\end{align*}\\]\nUsing the definitions of \\(\\Pi_{E_n^0}\\) and \\(\\Pi_{E_n^1}\\), we also have \\[\\begin{align*}\n\\mathbb E_P[f] &= \\int_{U(B_n)} f(\\Pi_{E_n^0}(\\omega), \\Pi_{E_n^1}(\\omega)) dP(\\omega).\n\\end{align*}\\]\nSince \\(f\\) is product measurable and \\(\\Pi_{E_n^0}\\) and \\(\\Pi_{E_n^1}\\) are independent, then \\[\\begin{align*}\n\\mathbb E_P[f \\mid U(E_n^0)](\\omega) &= \\mathbb E_P[f(\\Pi_{E_n^0}(\\omega),\\Pi_{E_n^1})] \\\\\n&= \\int_{U(B_n)} f(\\Pi_{E_n^0}(\\omega),\\Pi_{E_n^1}(\\omega')) dP(\\omega') \\\\\n&= \\int_{U(B_n)} f((U_e)_{e \\in E_n^0}, (U'_e)_{e \\in E_n^1}) \\prod_{e \\in E_n} d\\sigma(U'_e) \\\\\n&= \\int_{U(E_n^0)} \\left(   \\int_{U(E_n^1)}  f((U_e)_{e \\in E_n^0}, (U'_e)_{e \\in E_n^1})   \\prod_{e \\in E_n^1} d\\sigma(U'_e)     \\right) \\prod_{e \\in E_n^0} d\\sigma(U'_e) \\\\\n&= \\int_{U(E_n^1)}  f((U_e)_{e \\in E_n^0}, (U'_e)_{e \\in E_n^1})   \\prod_{e \\in E_n^1} d\\sigma(U'_e) \\times\n\\int_{U(E_n^0)}  1   \\prod_{e \\in E_n^0} d\\sigma(U'_e)\\\\\n&= \\int_{U(E_n^1)}  f((U_e)_{e \\in E_n^0}, (U'_e)_{e \\in E_n^1})   \\prod_{e \\in E_n^1} d\\sigma(U'_e)\n\\end{align*}\\] for almost all \\(\\omega = (U_e)_{e \\in E_n} \\in \\Omega = U(B_n)\\). Equality number 5 above is key. It shows that conditioning on \\(E_n^0\\) edges allows us to break up integrals into products and reduce the degrees of freedom. Without conditioning, we cannot treat \\(f(U_e)_{e \\in E_n}\\) as a constant with respect to the \\(U(E_n^0)\\) variables.\n\n\n\n\n\n\nNoteLemma\n\n\n\n\\(V(x,y)\\) and \\(V(v,w)\\) are independent.\n\n\n\nProof.\nLet \\(f,g: (U(N), \\mathcal{B}) \\to \\mathbb R_{\\ge 0}\\) be \\(\\mathcal{B}\\)-measurable. We want to show that \\[\\mathbb E_P[f(V(x,y)) \\times g(V(v,w))] = \\mathbb E_P[f(V(x,y))] \\mathbb E_P[g(V(v,w))].\\] We begin with \\[\\begin{align*}\nE_P[f(V(x,y)) \\times g(V(v,w))] =& E_P[E_P[f(V(x,y)) \\times g(V(v,w)) \\mid U(E_n^0)]]\\\\\n=& \\int_{U(B_n)} E_P[f(V(x,y)) \\times g(V(v,w)) \\mid U(E_n^0)](\\omega) dP(\\omega) \\\\\n=& \\int_{U(B_n)} E_P[f(G_n(U,x)U(x,y)G_n(U,y)^*) \\\\\n&                   \\times g(G_n(U,v)U(v,w)G_n(U,w)^*) \\mid U(E_n^0)](U) \\prod_{e \\in E_n}d\\sigma(U_e) \\\\\n=& \\int_{U(B_n)} E_P[f(U_{e(x,1)} \\cdots U_{e(x,|x|_1)} U(x,y) U_{e(y,|y|_1)}^* \\cdots U_{e(y,1)}^* ) \\\\\n&                   \\times g(U_{e(v,1)} \\cdots U_{e(v,|v|_1)} U(v,w) U_{e(w,|w|_1)}^* \\cdots U_{e(w,1)}^* ) \\mid U(E_n^0)](U) \\prod_{e \\in E_n}d\\sigma(U_e) \\\\\n\\end{align*}\\] (then by independence) \\[\\begin{align*}\n=& \\int_{U(B_n)} \\bigg ( \\int_{ U(E_n^1) } f(U_{e(x,1)} \\cdots U_{e(x,|x|_1)} U'(x,y) U_{e(y,|y|_1)}^* \\cdots U_{e(y,1)}^* ) \\\\\n& \\times g(U_{e(v,1)} \\cdots U_{e(v,|v|_1)} U'(v,w) U_{e(w,|w|_1)}^* \\cdots U_{e(w,1)}^* ) \\prod_{e \\in E_n^1} d \\sigma (U'_e) \\bigg ) \\prod_{e \\in E_n} d\\sigma(U_e)\\\\\n\\end{align*}\\] (then by Haar invariance) \\[\\begin{align*}\n=& \\int_{U(B_n)} \\bigg ( \\int_{ U(E_n^1) } f( U'(x,y) ) \\times g( U'(v,w) ) \\prod_{e \\in E_n^1} d \\sigma (U'_e) \\bigg ) \\prod_{e \\in E_n} d\\sigma(U_e)\\\\\n\\end{align*}\\] (then by pulling a constant out of the integral) \\[\\begin{align*}\n=& \\bigg ( \\int_{ U(E_n^1) } f( U'(x,y) ) \\times g( U'(v,w) ) \\prod_{e \\in E_n^1} d \\sigma (U'_e) \\bigg ) \\times \\int_{U(B_n)} 1 \\prod_{e \\in E_n} d\\sigma(U_e)\\\\\n\\end{align*}\\] (then by Fubini) \\[\\begin{align*}\n=& \\int_{ U(N)^2 } f( U'(x,y) ) \\times g( U'(v,w) ) d \\sigma (U'_{(x,y)}) d \\sigma (U'_{(v,w)}) \\\\\n=& \\int_{U(N)} f( U'(x,y) ) d\\sigma(U'_{(x,y)}) \\times \\int_{U(N)} g( U'(v,w) ) d \\sigma (U'_{(v,w)})\n\\end{align*}\\] (then since Haar measure is a probability measure and Fubini) \\[\\begin{align*}\n=& \\int_{U(B_n)} f( U'(x,y) ) \\prod_{e \\in E_n} d\\sigma(U'_e) \\times \\int_{U(B_n)} g( U'(v,w) ) \\prod_{e \\in E_n} d\\sigma(U'_e)\n\\end{align*}\\] (then by Haar invariance) \\[\\begin{align*}\n=& \\int_{U(B_n)} f( U_{e(x,1)}' \\cdots U_{e(x,|x|_1)}' U'(x,y) U_{e(y,|y|_1)}'^* \\cdots U_{e(y,1)}'^* ) \\prod_{e \\in E_n} d\\sigma(U'_e) \\\\\n& \\times \\int_{U(B_n)} g( U_{e(v,1)}' \\cdots U_{e(v,|v|_1)}' U'(v,w) U_{e(w,|w|_1)}'^* \\cdots U_{e(w,1)}'^* ) \\prod_{e \\in E_n} d\\sigma(U'_e) \\\\\n\\end{align*}\\] (and by definition of \\(V(x,y)\\) and \\(V(v,w)\\) ) \\[\\begin{align*}\n=& \\int_{U(B_n)} f( V(x,y) ) \\prod_{e \\in E_n} d\\sigma(U'_e) \\times \\int_{U(B_n)} g( V(v,w) ) \\prod_{e \\in E_n} d\\sigma(U'_e) \\\\\n=& \\mathbb E_P[f(V(x,y))] \\mathbb E_P[g(V(v,w))].\n\\end{align*}\\]\nQED\n\n\n\nJanuary 19, 2026\n\nDiagonalizability\nIn this post I want to learn about Fourier transforms as coordinate changes. It is also often said that the Fourier transform diagonalizes operators, so that is why I have titled this section accordingly.\nLet \\(A\\) be a two by two real matrix. Suppose that there exist an orthonormal matrix \\(U \\in O(2)\\) and a diagonal matrix \\(D = \\begin{bmatrix} \\lambda_1 & 0 \\\\\n0 & \\lambda_2 \\end{bmatrix} \\in \\mathbb R^{2 \\times 2},\\) such that \\(A = UDU^T.\\) Then the \\(i\\)th column of \\(U\\) is an eigenvector of \\(A\\) with corresponding eigenvalue \\(\\lambda_i\\). Recall that a non-zero column vector \\(x = [x_1, x_2]^T \\in \\mathbb R^{2}\\) is called an eigenvector of \\(A\\) with corresponding eigenvalue \\(\\lambda \\in \\mathbb R\\) if \\[Ax = \\lambda x.\\] Here is how to see that \\(U_{\\cdot, i}\\) satisfies \\(A U_{\\cdot, i} = \\lambda_i U_{\\cdot, i}\\). Let \\(i = 1\\). We compute \\[\\begin{align*}\n(UDU^T)(U_{\\cdot,1}) &= (UD) \\begin{bmatrix}\n                              1 \\\\ 0 \\end{bmatrix} && \\text{orthonormality}\\\\\n                     &= U \\begin{bmatrix}\n                              \\lambda_1 \\\\ 0 \\end{bmatrix} \\\\\n                     &= \\lambda_1 U_{\\cdot, 1}.\n\\end{align*}\\] A similar computation follows for \\(i = 2\\). In conclusion, when \\(A = UDU^T\\) for an orthonormal matrix \\(U\\) (used orthonormality in first step), then the columns of \\(U\\) are eigenvectors of \\(A\\).\nLet \\(T: V \\to V\\) be a linear operator on an abstract two dimensional vector space \\(V\\) over the field \\(\\mathbb R\\). Let \\(v_1, v_2\\) and \\(w_1, w_2\\) be two bases of \\(V\\). Suppose \\[\n\\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = A,\n\\] which means that \\(T(v_1) = A_{1,1} w_1 + A_{2,1} w_2\\) and \\(T(v_2) = A_{1,2} w_1 + A_{2,2} w_2\\). In other words, the first column of \\(A\\) holds the components of the abstract vector \\(T(v_1)\\) in the basis \\(w_1,w_2\\), and the second column of \\(A\\) holds the components of the abstract vector \\(T(v_2)\\) in the basis \\(w_1, w_2\\). This could be written in the language of coordinate functions for manifolds, but let me not go there. One more way to visualize \\(\\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = A\\) is via \\[\nA = \\begin{bmatrix}\n(Tv_1)_{w_1,w_2} & (Tv_2)_{w_1,w_2}\n\\end{bmatrix}.\n\\]\nWe say that the operator \\(T\\) is diagonalizable if there exists a basis \\(e_1, e_2\\) of \\(V\\) such that \\(e_1, e_2\\) are eigenvectors of \\(T\\): \\(Te_1 = \\lambda_1 e_1\\) and \\(T e_2 = \\lambda_2 e_2\\). Then we see that \\[\n\\text{Mat}_{e_1,e_2}^{e_1,e_2}(T) = \\begin{bmatrix}\n(Te_1)_{e_1,e_2} & (Te_2)_{e_1, e_2} \\end{bmatrix}\n= \\begin{bmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}.\n\\]\nSuppose we know that \\(T\\) is diagonalizable. Fix two bases \\(v_1, v_2\\) and \\(w_1, w_2\\) of \\(V\\). Then the input space for the matrix \\(A = \\text{Mat}_{v_1, v_2}^{w_1, w_2}(T)\\) is the space of all possible coordinates (i.e. \\(\\mathbb R^2\\)) in the basis \\(v_1, v_2\\).\nMeanwhile, the output space for the matrix \\(A\\) is the space of all possible coordinates in the basis \\(w_1, w_2\\).\nThen an equation of the form \\[\nA = UDV,\n\\] or more explicitly \\[\n\\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = U \\cdot \\text{Mat}_{e_1,e_2}^{e_1,e_2}(T) \\cdot V\n\\] would suggest that \\(V\\) is a change of basis matrix from the basis \\(v_1, v_2\\) to the basis \\(e_1, e_2\\) and \\(U\\) is a change of basis matrix from the basis \\(e_1, e_2\\) to the basis \\(w_1, w_2\\).\nLet me review the idea of change of basis matrices. Suppose \\(U\\) is a change of basis matrix from the basis \\((e_1,e_2)\\) to the basis \\((w_1,w_2)\\) of the abstract vector space \\(V\\). Then we should be able to deduce how \\(U\\) looks based on the one property it should satisfy, which is the following. \\(U\\) should take the components of some abstract vector \\(v \\in V\\) in the first basis \\(e_1, e_2\\), and rewrite such coordinates with respect to the second basis \\(w_1, w_2\\). In particular, this should work for the special cases of \\(v = e_1\\) and \\(v = e_2\\).\nSuppose \\(e_1 = U_{11}w_1 + U_{21}w_2\\) and that \\(e_2 = U_{12}w_1 + U_{22}w_2\\) for some real numbers \\(U_{11}, U_{21}, U_{12}, U_{22},\\) which are suggestively written. Then the desired property of a change of basis matrix says that \\[\nU (e_1)_{e_1,e_2} = U \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = (e_1)_{w_1,w_2} = \\begin{bmatrix} U_{11} \\\\ U_{21} \\end{bmatrix}\n\\] and \\[\nU (e_2)_{e_1,e_2} = U \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = (e_2)_{w_1,w_2} = \\begin{bmatrix} U_{12} \\\\ U_{22} \\end{bmatrix}.\n\\] This actually determines that \\[ U = \\begin{bmatrix} (e_1)_{w_1,w_2} & (e_2)_{w_1, w_2} \\end{bmatrix}\n= \\begin{bmatrix} U_{11} & U_{12} \\\\ U_{21} & U_{22} \\end{bmatrix}.\n\\]\nLet that conclude my discussion on change of basis matrices, and let me now rewrite an earlier equation as \\[\n\\text{Mat}_{v_1, v_2}^{w_1, w_2}(T) = \\begin{bmatrix} (e_1)_{w_1,w_2} & (e_2)_{w_1, w_2} \\end{bmatrix} \\cdot \\text{Mat}_{e_1,e_2}^{e_1,e_2}(T) \\cdot \\begin{bmatrix} (v_1)_{e_1,e_2} & (v_2)_{e_1, e_2} \\end{bmatrix}\n\\] From here, we see that, if \\(T: V \\to V\\) is diagonalizable, meaning that there exists a basis of \\(V\\) consisting of eigenvectors of \\(T\\), then any matrix of \\(T\\) in arbitrary bases \\(v_1, v_2\\) and \\(w_1, w_2\\) will also be diagonalizable in the sense of matrices. Why? Because change of basis matrices always exist, and then we just multiply the diagonal matrix on either side accordingly to get equality with \\(A\\).\nNow let me approach things from a different perspective to learn about multiplication operators. I want to learn about the folklore that the Fourier transform turns a differentiation operator into a multiplication operator, and to see how this connects to the discussion above of diagonalization.\nConsider the following watered down statement of the spectral theorem, based on the rigorous version found at the beginning of Brennecke (2026). Let \\(T_A: V \\to V\\) be self-adjoint. Then there is a measure space \\(\\Omega\\), a unitary map \\(U: V \\to L^2(\\Omega)\\) and a real-valued measurable function \\(f: \\Omega \\to\n\\mathbb R\\) such that \\[\n(U T_A U^*) \\phi = f \\phi =: \\mathcal{M}_f (\\phi)\n\\] for all \\(\\phi \\in L^2(\\Omega)\\). In other words, conjugation of \\(T_A\\) by \\(U\\) results in a multiplication operator, i.e., \\(\\mathcal{M}_f\\).\nAs a side note, I have used the notation \\(T_A\\) because I want the matrix of \\(T_A\\) in the canonical basis to be the matrix \\(A\\), when \\(V = \\mathbb R^n\\).\nI can already say that the equation \\((U T_A U^*) \\phi = \\mathcal{M}_f(\\phi)\\) reminds me of the equation \\(UAU^T = D\\), which comes from \\(A = U^TDU\\), a variant of what I saw earlier, which would suggest that a diagonal matrix \\(D\\) would correspond to the multiplication operator \\(\\mathcal{M}_f\\) in some way. It also suggests that the unitary operator \\(U: V \\to L^2(\\Omega)\\) corresponds to a change of basis: the input basis could be the usual basis of \\(V\\) (if there is one), and the output basis would be the canonical basis of the \\(L^2\\) space given by the theorem, which (I hope) would be something like a basis of characters in the sense of Fourier analysis. But all of this is what I want to find out.\nTo move this discussion into the setting of diagonalization of finite dimensional operators, as I began with in this post, let me try to understand the following section from Brennecke (2026):\n\n\n\n\n\n\nNoteQuote\n\n\n\nThe Theorem generalizes the well-known fact from linear algebra that every Hermitian matrix \\(H = H^* \\in \\mathbb C^{n \\times n}\\) can be diagonalized and admits an orthonornal eigenbasis \\((\\phi_i)_{i=1}^n\\) so that \\(H(\\phi_i) = \\lambda_i \\phi_i\\) for suitable (real) eigenvalues \\(\\lambda_i \\in \\mathbb R\\). In this case, the spectral projection valued measure representation of \\(H\\) is simply given by \\[\nH = \\sum \\lambda_i |\\phi_i \\rangle \\langle \\phi_i |.\n\\] The map \\(U\\) can be defined by linearly extending \\(\\mathbb C^n \\ni \\phi_i \\mapsto \\chi_{\\{ \\lambda_i \\}} \\in L^2(\\Omega, \\mathcal{B}(\\Omega), \\mu)\\), where \\(\\Omega = \\{\\lambda_i: i = 1, \\cdots, n \\}\\) and where \\(\\mu\\) denotes the counting measure on \\(\\Omega\\).\n\n\nIn the case of the quote, the map \\[\nf: \\Omega = \\{\\lambda_i : i \\in [n]\\} \\to \\mathbb R\n\\] in the spectral theorem is given by \\[\nf(x) = x, \\text{ i.e., } f = id.\n\\] Note also that when \\(\\Omega = \\{\\lambda_i : i \\in [n]\\}\\), it follows that \\(L^2(\\Omega) = \\mathbb C^{\\Omega} = \\mathbb C^n\\), assuming the functions in \\(L^2\\) are complex valued.\nLet \\(T_H: V \\to V\\) be a linear operator from a complex vector space \\(V\\) of dimension \\(n\\), and let \\(H\\) be the matrix of \\(T_H\\) in the sense that \\[\n\\text{Mat}_{\\bf{e}_1, \\cdots, \\bf{e}_n}^{\\bf{e}_1, \\cdots, \\bf{e}_n}(T_H) = H,\n\\] where the boldface list \\(\\bf{e}_1, \\cdots, \\bf{e}_n\\) is the standard basis of \\(\\mathbb C^n\\). And assume \\(H\\) is Hermitian, as in the quote, with eigenbasis \\((\\phi_i)_{i=1}^n\\) and correponding eigenvalues \\((\\lambda_i)_{i=1}^n\\). The statement of the spectral theorem, taking into account \\(f\\) and \\(U\\) from above, reads as follows: \\[\n(U T_H U^*) \\phi = \\mathcal{M}_{id} \\phi \\text{ pointwise on }\\Omega\n\\tag{2}\\] for all \\(\\phi \\in L^2(\\Omega) = \\mathbb C^n\\).\nLet me try to interpret Equation 2 in terms of diagonal matrices. So \\(\\phi\\) in an assignment of a complex number to each eigenvalue \\(\\lambda_i\\), for \\(i \\in [n]\\). Thus, \\[\n[\\phi(\\lambda_i), \\cdots, \\phi(\\lambda_n)]^T \\in \\mathbb C^n.\n\\] Next, \\(\\mathcal{M}_{id} \\phi\\) is a function from \\(\\Omega \\to \\mathbb C\\). So there are \\(n = |\\Omega|\\) possible inputs, and the corresponding outputs, in vector form, are \\[\n[(\\mathcal{M}_{id}\\phi)(\\lambda_1), \\cdots, (\\mathcal{M}_{id}\\phi)(\\lambda_n) ]^T = [\\lambda_1 \\phi(\\lambda_1), \\cdots, \\lambda_n \\phi(\\lambda_n) ]^T \\in \\mathbb C.\n\\] Thus, \\[\n[(\\mathcal{M}_{id}\\phi)(\\lambda_1), \\cdots, (\\mathcal{M}_{id}\\phi)(\\lambda_n) ]^T = \\begin{bmatrix} \\lambda_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\lambda_n\n\\end{bmatrix} [\\phi(\\lambda_i), \\cdots, \\phi(\\lambda_n)]^T.\n\\] This shows how the multiplication operator \\(\\mathcal{M}_{id}\\) corresponds to a diagonal matrix with the eigenvalues on the diagonal. In other words, Equation 2 corresponds to the equation \\(UHU^* = D\\) in matrix langauge, where \\(H\\) may not be diaganal with respect to the standard basis \\(\\bf{e}_1, \\cdots, \\bf{e}_n\\). Continuing the change of basis analogy, it would be that \\(U^*\\) is a change of basis from the basis \\((\\chi_{\\{\\lambda_i\\}})_{i=1}^n\\) of \\(L^2(\\Omega)\\) to the basis \\(\\bf{e}_1, \\cdots, \\bf{e}_n\\) of \\(\\mathbb C^n\\), and \\(U\\) is a change of basis from the standard basis of \\(\\mathbb C^n\\) to the eigenvalue point functions.\nIf diagonalizing the operator \\(T_H\\) means conjugating by a unitary map \\(U\\), then I think via the above paragraphs, I have understood diagonalization. But I think it also instructive to check that I actually chose the correct \\(f\\) in the spectral theorem (\\(f = id\\)) by verifying the eigenvalue equations in the following way.\nSince \\(UT_HU^* = \\mathcal{M}_{id}\\), it stands to reason that \\(T_H = U^* \\mathcal{M}_{id} U\\). Let’s check this on the eigenbasis, where we know the left hand side will read: \\[\nT_H \\phi_i = \\lambda_i \\phi_i.\n\\] On the right hand side, fix \\(i=1\\). We compute \\[\\begin{align*}\n(U^* \\mathcal{M}_{id} U )(\\phi_1) &= U^* (\\mathcal{M}_{id}(U(\\phi_1))) \\\\\n&= U^*(id(\\cdot) \\chi_{\\{\\lambda_1\\}}(\\cdot) )\\\\\n&= \\lambda_1 \\phi_1.\n\\end{align*}\\] The last equality is least clear, so let me delve further into it. Note that \\(U(\\lambda_1 \\phi_1) = \\lambda_1 U(\\phi_1) = \\lambda_1 \\chi_{\\{\\lambda_1 \\}}(\\cdot) \\in L^2(\\Omega)\\) by linearity. But by definition of the indicator function \\(\\chi_{\\{\\lambda_1 \\}}(\\cdot)\\), it turns out that \\(\\lambda_i \\mapsto \\lambda_i \\chi_{\\{\\lambda_1 \\}}(\\lambda_i)\\), or equivalently, the function \\(id(\\cdot) \\chi_{\\{\\lambda_1\\}}(\\cdot) \\in L^2(\\Omega)\\), is actually equal to the function \\(\\lambda_1 \\chi_{\\{\\lambda_1 \\}}(\\cdot)\\). That is, \\[\nid(\\cdot) \\chi_{\\{\\lambda_1\\}}(\\cdot) = \\lambda_1 \\chi_{\\{\\lambda_1 \\}}(\\cdot).\n\\] Thus, it makes sense to write \\(U^*(id(\\cdot) \\chi_{\\{\\lambda_1\\}}(\\cdot)) = \\lambda_1 \\phi_1\\). The upshot is that this then extends to when \\(i \\ne 1\\): \\(U^*(id(\\cdot) \\chi_{\\{\\lambda_i\\}}(\\cdot)) = \\lambda_i \\phi_i\\). This confirms that \\[\nT_H \\phi_i = (U^* \\mathcal{M}_{id} U) \\phi_i\n\\] for all \\(i \\in [n]\\). Then we get equality on all of \\(V\\) by unique linear extension.\n\n\nFourier Transform as a diagonalizer\nLet me sketch out what I’m thinking. There is some function space (or a dense subset of one), call it \\(C^1(\\mathbb R/\\mathbb Z)\\), on which the differentiation operator \\(-i \\frac{d}{dt}\\) is self-adjoint. Also, if \\(e_n(x) = e^{-2\\pi i n x}\\) are characters, and the Fourier transform of \\(f \\in C^1(\\mathbb R/\\mathbb Z)\\) is given by \\[\n\\mathcal{F}(f)(n) = \\langle f, e_n \\rangle_{L^2}\n\\] such that \\(\\mathcal{F}f : \\mathbb Z \\to \\mathbb C\\), or equivalently, \\(\\mathcal{F}f \\in \\mathbb C^{\\mathbb Z}\\) is an absolutely summable sequence, then \\[\n(\\mathcal{F} \\circ -i\\frac{d}{dt}) f (n) = -i \\cdot i n (\\mathcal{F}f)(n) = \\mathcal{M}_{id}(n) (\\mathcal{F}f)(n)\n\\] for all integers \\(n\\). If we let \\(A = -i \\frac{d}{dt}\\), then this is the statement that \\[\n\\mathcal{F} \\circ A = \\mathcal{M}_{id} \\circ \\mathcal{F},\n\\] or that \\[\n\\mathcal{F} \\circ A \\circ \\mathcal{F}^* = \\mathcal{M}_{id}.\n\\] Thus, we see that the fourier transform diagonalizes the momentum operator \\(A\\). We can also see that the Fourier transform corresponds to the change of basis matrix \\(U\\) from the beginning of the post, at least positionally. In this case, I think the Fourier transform maps the basis \\((e_n)_{n \\in \\mathbb N}\\) of characters to the standard basis of \\(\\mathbb C^{\\mathbb N}\\) (elements with 1 in the ith position, 0 everywhere else). Indeed, \\[\n(\\mathcal{F}e_i)(n) = \\langle e_i, e_n \\rangle = \\delta_{i}(n).\n\\] The fact that the list of characters \\((e_n(\\cdot))\\) is a basis of, for example, \\(L^2(\\mathbb R/\\mathbb Z)\\), is the Fourier theorem in Tao’s Analysis 2 book Tao (2006).\n\n\n\nJanuary 16, 2026\nLet \\(x,y,v,w \\in B_n\\) such that \\((x,y) \\in E_n^1\\) and \\((v,w) \\in E_n^1\\), and so that these are distinct edges. As in section 9 of Chatterjee (2016), for a configuration \\(U \\in U(B_n)\\) drawn from the product Haar measure, define \\(V(x,y) = G_U(x)U(x,y)G_U(y)^*\\) and \\(V(v,w) = G_U(v)U(v,w)G_U(w)^*\\).\nLet \\(a = |E_n^0|\\) and let \\(r = |E_n^1|\\). Let \\(\\phi: \\{1,\\cdots, a\\} \\to E_n^0\\) be a bijection and let \\(\\psi: \\{1, \\cdots, r\\} \\to E_n^1\\) be a bijection as well.\nThus, \\(V(x,y)\\) is a random variable from the probability space \\(\\left( \\prod_{E_n} U(N), \\mathcal{B}(\\prod_{E_n} U(N)), du_{\\phi(1)} \\cdots du_{\\phi(a)} du_{\\psi(1)} \\cdots du_{\\psi(r)} \\right)\\) to the probability space \\((U(N), \\mathcal{B}(U(N)), \\sigma)\\), where \\(\\sigma\\) denotes Haar measure.\nFor the vertex \\(x\\), let \\[\ni_1(x), \\cdots, i_{|x|_1}(x)\n\\] be the unique list of integers from the set \\(\\{1, \\cdots, a\\}\\) such that \\[\n\\phi(i_1(x)), \\cdots, \\phi(i_{|x|_1}(x))\n\\] is the path of bonds in \\(E_n^0\\) leading from vertex \\(0 \\in B_n\\) to vertex \\(x \\in B_n\\).\nSimilary, define the lists \\[\ni_1(y), \\cdots, i_{|y|_1}(y)\n\\] and \\[\ni_1(v), \\cdots, i_{|v|_1}(v)\n\\] and \\[\ni_1(w), \\cdots, i_{|w|_1}(w).\n\\] Recall the definition from the January 15, 2026 post: \\(G_n: U(B_n) \\times B_n \\to U(N)\\) is given by \\[\nG_n(U,z) = \\begin{cases}\nI & \\text{ if }z = 0 \\\\\nu_{\\phi(i_1(z))}\\cdots u_{\\phi(i_{|z|_1}(z))} & \\text{ else},\n\\end{cases}\n\\] where \\(\\phi(i_1(z)), \\cdots, \\phi(i_{|z|_1}(z))\\) is the unique path of bonds in \\(E_n^0\\) between \\(0 \\in B_n\\) and \\(z \\in B_n\\). By the Lemma from the January 15th post as well, we have the following: \\[\\begin{align*}\nV(x,y) &= G_U(x)U(x,y)G_U(y)^* \\\\\n&= G_n(U,x) U(x,y) G_n(U,y)^* \\\\\n&= U_{\\phi(i_1(x))} \\cdots U_{\\phi(i_{|x|_1}(x))} U(x,y) U_{\\phi(i_{|y|_1}(y))}^* \\cdots U_{\\phi(i_{1}(y))}^*,\n\\end{align*}\\] and \\[\\begin{align*}\nV(v, w) &= U_{\\phi(i_1(v))} \\cdots U_{\\phi(i_{|v|_1}(v))} U(v,w) U_{\\phi(i_{|w|_1}(w))}^* \\cdots U_{\\phi(i_{1}(w))}^*.\n\\end{align*}\\]\nI’ll have to continue another day.\n\n\nJanuary 15, 2026\nFix the box size \\(n \\in \\mathbb N\\) and the dimension \\(d \\in \\mathbb N\\). Let \\(a = |E_n^0|\\) and let \\(r = |E_n^1|\\).\nLet \\(\\phi_n: \\{1, \\cdots, a\\} \\to E_n^0\\) be a bijection and let \\(\\psi_n: \\{1, \\cdots, r\\} \\to E_n^1\\) be a bijection as well, but let me suppress the \\(n\\) from the notation, leaving us with \\(\\phi\\) and \\(\\psi\\).\nA configuration is an assignment \\(E_n \\to U(N)\\), or a sequence \\((u_{\\phi(1)}, \\cdots, u_{\\phi(a)}, u_{\\psi(1)}, \\cdots, u_{\\psi(r)} )\\), where \\(u_{\\phi(l)} \\in U(N)\\) for all \\(l \\in \\{1, \\cdots, a\\}\\) and \\(u_{\\psi(l)} \\in U(N)\\) for all \\(l \\in \\{1, \\cdots, r\\}\\).\nNext we consider notation for axial gauge fixing. Recall that the set of all configurations on \\((B_n, E_n)\\) is denoted \\(U(B_n)\\). Let \\(G_n: U(B_n) \\times B_n \\to U(N)\\) be defined by \\[G_n((u_{\\phi(1)}, \\cdots, u_{\\phi(a)}, u_{\\psi(1)}, \\cdots, u_{\\psi(r)}), x) =\n\\begin{cases}\nI & \\text{if } x = 0 \\\\\nu_{\\phi(i_1(x))} \\cdots u_{\\phi(i_k(x))} & \\text{if } x \\ne 0.\n\\end{cases}\n\\] where \\(k = |x|_1\\) and \\(i_1(x), \\cdots, i_k(x)\\) is the unique sequence of integers taken from the set \\(\\{1, \\cdots, a\\}\\) such that \\(\\phi(i_1(x)), \\cdots, \\phi(i_k(x))\\) is the path of bonds in \\(E_n^0\\) leading from vertex \\(0 \\in B_n\\) to vertex \\(x \\in B_n\\).\n\n\n\n\n\n\nNoteLemma\n\n\n\n\\(G_n\\) agrees with the definition of gauge fixing given in section 9 of Chatterjee (2016).\n\n\nProof. We have fixed the box size \\(n\\). Next, fix a configuration \\(U \\in U(B_n)\\), or we may write \\(U = (u_{\\phi(1)}, \\cdots, u_{\\phi(a)}, u_{\\psi(1)}, \\cdots, u_{\\psi(r)} )\\) for some \\(u_{(\\cdot)} \\in U(N)\\).\nWe proceed by induction using the lexicographic ordering \\(\\prec\\) on the box lattice sites to show that \\[G_U(x) = G_n(U,x)\\] for all \\(x \\in B_n\\), where the left hand side is Chatterjee’s notation for axial gauge fixing.\nThe base case is \\(x = 0\\). We indeed have \\(G_U(0) = I = G_n(U,0)\\).\nNow assume the induction hypothesis that \\(G_U(y) = G_n((u_{\\phi(1)}, \\cdots, u_{\\phi(a)}, u_{\\psi(1)}, \\cdots, u_{\\psi(r)}), y)\\) for all \\(y \\prec x\\), for some \\(x = (x_1, \\cdots, x_d) \\in B_n\\).\nLet \\(j \\in \\{1, \\cdots, d\\}\\) be the largest index such that \\(x_j \\ne 0\\). Let \\(y := x - e_j\\). Then \\(x \\in B_n\\) and \\(y \\prec x\\). Hence, \\[\\begin{align*}\nG_U(x) &= G_U(y)U(y,x) \\\\\n&= G_n((u_{\\phi(1)}, \\cdots, u_{\\phi(a)}, u_{\\psi(1)}, \\cdots, u_{\\psi(r)}), y) U(y,x) \\\\\n&= u_{\\phi(i_1(y))} \\cdots u_{\\phi(i_k(y))} U(y,x)\n\\end{align*}\\] where \\(k = |y|_1\\) and \\(i_1(y), \\cdots, i_k(y)\\) is the unique sequence of integers taken from the set \\(\\{1, \\cdots, a\\}\\) such that \\(\\phi(i_1(y)), \\cdots, \\phi(i_k(y))\\) is the path of bonds in \\(E_n^0\\) leading from vertex \\(0 \\in B_n\\) to vertex \\(y \\in B_n\\). Observe that \\((y,x) \\in E_n^0\\). Hence, the sequence \\(\\phi(i_1(y)), \\cdots, \\phi(i_k(y)), (y,x)\\) is a path from \\(0 \\in B_n\\) to \\(x \\in B_n\\) in the graph \\((B_n, E_n^0)\\). But \\((B_n, E_n^0)\\) is a tree, so it is a unique path. Define \\(i_1(x) := i_1(y)\\) and so forth until \\(i_k(x) := i_k(y)\\). Then let \\(i_{k+1}(x)\\) be the unique integer in the set \\(\\{1, \\cdots, a\\} \\setminus \\{i_1(y), \\cdots, i_k(y))\\}\\) such that \\(\\phi(i_{k+1}(x)) = (y,x)\\). Based on this, we conclude that \\[\\begin{align*}\nu_{\\phi(i_1(y))} \\cdots u_{\\phi(i_k(y))} U(y,x) &= u_{\\phi(i_1(x))} \\cdots u_{\\phi(i_k(x))} u_{\\phi(i_{k+1}(x))} \\\\\n&= G_n((u_{\\phi(1)}, \\cdots, u_{\\phi(a)}, u_{\\psi(1)}, \\cdots, u_{\\psi(r)}), x)\n\\end{align*}\\] which closes the induction. \\(\\square\\)\nTomorrow, I want to use the notation \\(G_n\\) and the notion of paths to rewrite my journal entry from January 13, 2026.\n\n\nJanuary 14, 2026\n\nLiterature Reflection (aka rabbit hole reflection)\nI want to reflect in this post about one rabbbit hole I went down.\nThe story begins when I was looking for notation to describe the tree that is implicit in the \\(E_n^0\\) edges of the box \\(B_n\\), as described in Chatterjee (2016). So I visited first my old repository of pdfs from my Davidson times. In that repository I looked at Bruce Sagan’s “The Art of Counting,” which did not have what I needed in terms of describing a lattice tree, although I could use the definition of tree mentioned there. Then I visited Bonas’s “A Walk through Combinatorics,” which didn’t have anything on lattices \\(\\mathbb Z^d\\), but had the definitions of graphs and such. Then I remembered the book Prof. Brennecke recommended, namely, “Statistical Mechanics of Lattice Systems” by Friedli and Velenik. That book was written in 2019, and I noticed a general difference between the notation in Chatterjee’s 2016 paper and the Lattice Systems book. This got me a bit worried because it suggested that that notation I’m learning via the 2016 paper is outdated.\nThen the rabbit hole officially began because I was no longer looking for notation for the tree. I couldn’t help myself and I looked up Velenik’s website because I was so impressed by the lattice systems book and I wondered if Velenik was already famous. Then, fueled by a lust for fame, I looked up Dominil-Copin, who also works at the same place as Velenik. I downloaded the article: “Marginal Triviality of the scaling limits …” by Copin and Aizenman. The one upshot of this is that I discovered what Prof. Brennecke meant when he said a Field’s medal was awarded to proving the triviality of some quantum field model. Unable to stop my fame lust, I also looked up Chatterjee’s website, and saw the Yang-Mills program he has concocted together with S. Cao. At this point I was very burnt out and desperate to break the loop.\nAnyways, I think I’ll continue to look at some of the current articles for ideas on notation, but hopefully I won’t feel stuck in a loop like I did today.\n\n\nNotation\nI think it is better to make a list of the possible notations for lattice gauge theory. I’ll allow myself to write down multiple notations and see later which ones stick.\n\nFix \\(\\mathbb Z^d\\) as the d-dimensional integer lattice.\nLet \\(e_1, \\cdots, e_d\\) be the standard basis vectors in \\(\\mathbb R^d\\).\nLet \\(\\mathfrak{E}\\) (mathfrak E) be the underlying undirected edge set, which can be written \\[\n\\mathfrak{E} = \\{\\{i,j\\} \\subset \\mathbb Z^d \\mid |i-j|_1 = 1\\}.\n\\] Note that \\(|i-j|_1 = \\sum_{k=1}^d |i_k-j_k|\\), where \\(i = (i_1, \\cdots, i_d)\\) and similarly for \\(j\\). The metric \\(|\\cdot|_1\\) can be called the \\(l_1\\) norm or the taxi-cab metric. Hence, \\(\\mathfrak{E}\\) is the set of nearest neighbor edges of the lattice \\(\\mathbb Z^d\\).\nThus, \\((\\mathbb Z^d, \\mathfrak{E})\\) is a graph.\nLet \\(\\Lambda \\subset \\mathbb Z^d\\).\nLet \\[\n\\mathfrak{E}_{\\Lambda} = \\{\\{i,j\\} \\subset \\mathbb \\Lambda \\mid |i-j|_1 = 1\\}\n\\] be the nearest neighbor undirected edge set of the sublattice \\(\\Lambda\\).\nLet \\[\nE'_{\\Lambda} = \\{(i,j) \\in \\Lambda^2 \\mid |i-j|_1 = 1 \\}\n\\] denote the positively and negatively oriented nearest neighbor edges and let \\[\nE_{\\Lambda} = \\{(i,j) \\in \\Lambda^2 \\mid |i-j|_1 = 1, i &lt; j \\}\n\\] denote the positively oriented directed edge set of the lattice \\(\\Lambda\\), where \\(i &lt; j\\) denotes the lexicographic ordering on \\(\\mathbb Z^d\\).\nLet \\(B_n = \\{0, \\cdots, n-1\\}^d \\subset \\mathbb Z^d\\)\nLet \\[\\begin{align*}\nE_n^0 = \\{ (i,j) & \\in B_n \\times B_n \\mid |i-j|_1 = 1, \\text{ and for some } \\\\\n               & 1 \\le k \\le d \\text{ and for some } i_1, \\cdots, i_k: \\\\\n               & i = (i_1, \\cdots, i_k, 0, \\cdots, 0) \\text{ and } j = i + e_k \\}\n\\end{align*}\\]\nLet \\(E_n\\) be defined as in Chatterjee (2016) Section 2. Then \\(E_n = E_{B_n}\\).\nLet \\[\\begin{align*}\n\\mathfrak{E}_n^0 = \\{ \\{i,j\\} & \\subset B_n \\mid |i-j|_1 = 1, \\text{ and for some } \\\\\n               & 1 \\le k \\le d \\text{ and for some } i_1, \\cdots, i_k: \\\\\n               & i = (i_1, \\cdots, i_k, 0, \\cdots, 0) \\text{ and } j = i + e_k \\}\n\\end{align*}\\] be the undirected version of \\(E_n^0\\).\nThe pair \\((B_n, \\mathfrak{E}_n^0)\\) is a tree, or in other words, the underlying undirected graph of \\((B_n, E_n^0)\\) is a tree.\nA graph \\((V,E)\\) is a pair consisting of a vertex set \\(V\\) and an edge set \\(E \\subset \\{\\{v_1,v_2\\} \\subset V \\mid v_1 \\ne v_2 \\}\\)\nA finite sequence \\(v_1, \\cdots, v_k\\) of distinct vertices is called a path from \\(v_1\\) to \\(v_k\\) assuming \\(\\{v_i, v_{i+1} \\} \\in E\\) for all \\(i = 1, \\cdots, k-1\\). Similarly, to each path we can associate one and only one list of edges \\(v_1v_2, v_2v_3, \\cdots, v_{k-1}v_k\\), where we have used the convention of writing an edge without the set bars.\nWe say that a graph is connected if for all \\(x,y \\in V\\), there is a path from \\(x\\) to \\(y\\)\nTheorem (Sagan 1.10.2). Let \\(T\\) be a graph with \\(|V| = n\\) and \\(|E| = m\\). The following are equivalent conditions for \\(T\\) to be a tree:\n\n\\(T\\) is connected and acyclic\n\\(T\\) is acyclic and \\(n = m+1\\)\n\\(T\\) is connected and \\(n = m+1\\)\nFor every pair of vertices \\(u,v\\), there is a unique path from \\(u\\) to \\(v\\).\n\nAssume that \\((B_n, \\mathfrak{E}_n^0)\\) is connected. One has \\(|E_n^0| = n^d - 1\\) as in section 17 of Chatterjee (2016). Thus, condition 3 of the theorem is satisfied, so it follows that \\((B_n, \\mathfrak{E}_n^0)\\), or the underlying graph of \\((B_n, E_n^0)\\), is a tree.\nOne can see that \\((B_n, \\mathfrak{E}_n^0)\\) is connected in the following way. Fix some vertex \\(x \\in B_n\\). We want to find a path to \\(y \\in B_n\\). First, find a path to \\(0\\) in the following way. For each step, find the largest index \\(k\\) which is non-zero, and subtract \\(e_k\\) to obtain the next vertex. If the next vertex is \\(y\\), then stop. Else, continue to the 0 vertex. If \\(y\\) is not reached before \\(0\\), then proceed in the same way starting from \\(y\\) and concatenate the paths.\n\n\n\n\nJanuary 13, 2026\nI have written down on paper a ridiculous amount about Lemma 9.3 of Chatterjee (2016), so it’s about time I start typing some of it.\n\nOne Argument from Lemma 9.3 of Chatterjee 2016\nFirst let me cite one result about independent probabilistic objects, which in shorthand says that \\(X\\) and \\(Y\\) are independent random variables if \\(\\mathbb E (f(X)g(Y)) = \\mathbb E (f(X)) \\mathbb E (g(Y))\\) for a sufficiently robust family of functions \\(f, g\\). I couldn’t find this result in Klenke’s book (although it’s almost certainly implicit somewhere). I also couldn’t find it in Williams’s Prob. With Martingales, though looking at the Monotone class theorem looked related. I did find the result in Prof. Eberle’s Bachelor’s Probability theory notes: it is Satz 3.38.\nRecall the guage fixing procedure, which can be viewed abstractly as a map \\[\\begin{equation}\nG_U': U(B_n) \\to U_0(B_n),\n\\end{equation}\\] where I added the prime to distinguish from \\(G_U(x)\\), which is an assigment to the vertex \\(x \\in B_n\\) of a unitary matrix. I previously proved on paper that this map is onto, but let me skip a discussion of that. Given a configuration \\(U \\in U(B_n)\\) and the gauge transform \\(G_U\\) (determined by \\(U\\)), let \\(V(x,y) = G_U(x) U(x,y) G_U(y)^*\\).\nLemma 9.3 says that the matrices \\(\\{V(x,y) \\mid (x,y) \\in E_n^1, U \\in U(B_n)\\}\\) are independent and Haar-distributed. Let me skip for now the discussion of these matrices being Haar-distributed.\nLet \\(i,j \\in E_n^1\\) be distinct edges. Let \\(f, g \\ge 0\\) be measurable with respect to \\((U(N), \\mathcal{B}, \\text{Haar})\\). A first step to Lemma 9.3 is proving that \\[\\begin{equation}\n\\mathbb E [ f(V_i) g(V_j) ] = \\mathbb E [ f(V_i) ] \\mathbb E [ g(V_j) ]\n\\end{equation}\\] where the expectation is with regards to product Haar measure on \\(U(B_n)\\) because \\(V_i\\) and \\(V_j\\) are implicitly variables of the entire unitary configuration, not only the \\(E_n^1\\) edge matrices. Explicity, we want to show that \\[\\begin{equation}\n\\int_{U(B_n)} f(V_i) g(V_j) \\prod_{e \\in E_n} d\\sigma(U_e) = \\int_{U(B_n)} f(V_i) \\prod_{e \\in E_n} d\\sigma (U_e) \\times \\int_{U(B_n)} g(V_j) \\prod_{e \\in E_n} d\\sigma (U_e),\n\\end{equation}\\]\nwhere as in Chatterjee, we wrote \\(\\text{Haar} = d\\sigma\\), for compactness of notation. By the definition of conditional expectation, we have that\n\\[\n\\mathbb E [ f(V_i) g(V_j) ] = \\mathbb E [ \\mathbb E[ f(V_i) g(V_j) \\mid U_0(B_n) ]]\n\\tag{3}\\]\nIndeed, \\(Z(\\omega) = \\mathbb E[ f(V_i) g(V_j) \\mid U_0(B_n) ] (\\omega)\\) is the almost surely unique random variable such that \\[\\begin{equation}\n\\mathbb E[ 1_B( f(V_i) g(V_j) )] = \\mathbb E[ 1_B( Z(\\omega) )]\n\\end{equation}\\] for all \\(B \\in \\sigma(U_0(B_n))\\), the sigma algebra generated by \\(U_0(B_n)\\), viewed as a collection of random variables \\((U_e)_{e \\in E_n^0}\\) on the product space \\(U(B_n)\\). Of course, \\(U(B_n) \\in \\sigma(U_0(B_n))\\), so we get Equation 3.\nNext, consider the following lemma from Prof. Eberle’s notes:\n\n\n\n\n\n\nNoteLemma (Eberle)\n\n\n\nLet \\((\\Omega, \\mathcal{A}, P)\\) be a probability space and let \\(\\mathcal{F} \\subset \\mathcal{A}\\) be a sub-sigma algebra. Let \\((S,\\mathcal{S})\\) and \\((T, \\mathcal{T})\\) be measurable spaces. If \\(Y: \\Omega \\to S\\) is \\(\\mathcal{F}\\) measurable and \\(X: \\Omega \\to T\\) is independent of \\(\\mathcal{F}\\), and \\(\\psi: S \\times T \\to [0,\\infty)\\) is a product measurable map, then \\[\n\\mathbb E[\\psi(X,Y) \\mid \\mathcal{F}](\\omega) = \\mathbb E[\\psi(X,Y(\\omega))]\n\\] for almost all \\(\\omega \\in \\Omega\\).\n\n\nNote that on the right hand side in the formula in the lemma, \\(Y(\\omega)\\) is to be treated as a constant when you integrate to compute the expectation, whereas \\(X\\) is still considered as a variable: \\[\n\\mathbb E[\\psi(X,Y(\\omega))] = \\int_\\Omega \\psi(X(\\omega'), Y(\\omega)) dP(\\omega').\n\\] In essense, you have fixed the variable \\(Y\\) because you already know what it is, based on \\(\\mathcal{F}\\). In my setting, this is just a rigorous way of writing that the matrices attached to \\(E_n^0\\) edges inside the intergral can be treated as fixed. Let me demonstrate.\nLet \\(S := \\prod_{E_n^0} U(N) = U_0(B_n)\\) and \\(T := \\prod_{E_n^1} U(N)\\). Let also \\(\\Omega = \\prod_{E_n} U(N) = U(B_n)\\). Let also \\[\nY := Y(\\omega) := Y(U_e)_{e \\in E_n} = (U_e)_{e \\in E_n^0}\n\\] and \\[\nX := X(\\omega) := X(U_e)_{e \\in E_n} = (U_e)_{e \\in E_n^1}.\n\\] Note that \\(V_i\\) can really be viewed as a function of the matrix variables on all edges \\(E_n\\). We can write: \\[\nV_i(U_e)_{e \\in E_n} = U_0 \\cdots U_0 U(i) U_0 \\cdots U_0\n\\] where \\(U_0\\) is a placeholder for matrices attached to \\(E_n^0\\) edges, and \\(U(i)\\) is a matrix attached to edge \\(i \\in E_n^1\\). We can write in this way because \\(G_U(x)\\) is just a product of \\(U_0(B_n)\\) matrices for any \\(x \\in B_n\\), and \\(G_U\\) is deterministically a function of the variables \\(U_0(B_n)\\).\nFor ease of notation, let \\[U_1 := (U_e)_{e \\in E_n^1}\\] and \\[U_0 := (U_e)_{e \\in E_n^0} \\in U_0(B_n).\\] Thus, we consider the definition \\[\\begin{align*}\n\\psi(X,Y) &= \\psi(U_0,U_1)\\\\\n&= f(U_0 U(i) U_0) g(U_0 U(j) U_0).\n\\end{align*}\\]\nThen by the Lemma, we have\n\\[\\begin{align*}\n\\mathbb E[ f(V_i) g(V_j) \\mid U_0(B_n) ](\\omega) &=\n\\mathbb E[ f(V_i) g(V_j) \\mid U_0(B_n) ](U_e)_{e \\in E_n} \\\\\n&= \\mathbb E[ f(V_i(U_e)_{e \\in E_n^0}) g(V_j(U_e)_{e \\in E_n^0}) ] \\\\\n&= \\int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \\prod_{e \\in E_n} d\\sigma(U'(e))\n\\end{align*}\\]\nThen by left and right invariance of the Haar measure, we compute \\[\\begin{align*}\n\\int_{U(B_n)} f(U_0 U'(i) U_0) g(U_0 U'(j) U_0) \\prod_{e \\in E_n} d\\sigma(U'(e)) &= \\int_{U(B_n)} f(U'(i)) g(U'(j)) \\prod_{e \\in E_n} d\\sigma(U'(e))\n\\end{align*}\\]\nHence, \\[\n\\mathbb E[ f(V_i) g(V_j) \\mid U_0(B_n) ](\\omega) = \\mathbb E_{U'} [f(U'(i)) g(U'(j))].\n\\]\nThen, \\[\\begin{align*}\n\\mathbb E [ f(V_i) g(V_j) ] &= \\mathbb E [ \\mathbb E[ f(V_i) g(V_j) \\mid U_0(B_n) ]] \\\\\n&= \\int_{U(B_n)} \\mathbb E_{U'} [f(U'(i)) g(U'(j))] \\prod_{e \\in E_n} d\\sigma(U_e)\\\\\n&= \\mathbb E_{U'} [f(U'(i)) g(U'(j))] \\int_{U(B_n)} 1 \\prod_{e \\in E_n} d\\sigma(U_e),\n\\end{align*}\\] where we pulled a “constant” out of the integral. Then by Haar invariance and Fubini a couple times to split up \\(f\\) and \\(g\\), we conclude the result \\[\\begin{equation}\n\\mathbb E [ f(V_i) g(V_j) ] = \\mathbb E [ f(V_i) ] \\mathbb E [ g(V_j) ].\n\\end{equation}\\]\nI’m tired of this article for today, but one improvement would be to write out explicitly what the product \\(U_0 \\cdots U_0\\) actually is. In particular, I should first write down the tree with root at the origin in the lattice. Then I should rewrite \\(i \\in E_n^1\\) as an edge of the form \\((i_l, i_r)\\), and then \\(G_U(i_l)\\) will just be the product of \\(U_0\\) matrices indexed by the edges from the origin \\(0\\) to \\(i_l\\) along the tree. So if these tree edges are \\(t_0, \\cdots, t_{i_l}\\), then the product would be \\[\nU_0 \\cdots U_0 = U(t_0) \\cdots U(t_{i_l}).\n\\]\n\n\n\nJanuary 3, 2026\nI’m studying section 12 in Chatterjee (2016), which is titled Some Standard Results About Gaussian Measures. I’d like to understand why \\(Q\\) being a positive definite \\(n\\) by \\(n\\) real matrix implies that \\[x^TQx + v^Tx + C =: P(x) \\ge c \\lVert x \\rVert^2\\] for some positive constant \\(c\\), for all \\(\\Vert x \\rVert\\) sufficiently large, where \\(x = (x_1, \\cdots, x_n) \\in \\mathbb R^n\\). Maybe I should first mention what this is saying. When a real matrix \\(Q\\) is positive definite, it means that the polynomial \\[ \\begin{matrix}\nQ_{11} x_1^2 &+& \\cdots &+& Q_{1n} x_1 x_n\\\\\n\\vdots && \\ddots && \\vdots \\\\\nQ_{n1} x_1x_n &+& \\cdots &+& Q_{nn} x_n^2\n\\end{matrix}\\] is strictly positive for all \\(x \\ne 0\\). So the matrix \\(Q\\) is the coefficient matrix of the degree 2 monomials. Oh yeah, and \\(P(x)\\) above is just a general degree 2 polynomial in the real variables \\(x_1, \\cdots, x_n\\). I’ll mention that the variables commute, so we can just assume that \\(Q\\) is symmetric, because we can break up the coefficients into two equal halves. The main point of all this is that \\(P(x) \\ge c \\lVert x \\rVert^2\\) is a necessary and sufficient criterion for the integrability of a Gaussian density term for a Gaussian measure (see section 12), so knowing that \\(Q\\) being positive definite is equivalent allows you to later bring in stuff about smallest eigenvalues of \\(Q\\), linear algebra stuff, and other information that is not obvious from first glance.\nThis implication of \\(Q\\) positive definite implies \\(P(x) \\ge c \\lVert x \\rVert^2\\) is too hard for my little brain, so as a first step I need to consider an easier case: assume \\(v = 0\\) and \\(C = 0\\), so the polynomial \\(P\\) is just made up of the terms in \\(x^TQx\\). So I want to show that \\[P(x) = x^T Qx \\ge c \\lVert x \\rVert^2\\] for some \\(c &gt; 0\\) and for all \\(\\lVert x \\rVert &gt; r &gt; 0\\), where \\(r\\) is some radius that is big enough.\nLet’s assume that \\[R_* := \\inf_{\\lVert x \\rVert = 1} x^TQx &gt; 0.\\] Hopefully I can prove later on that \\(Q\\) being positive definite implies this infimum statement truly does hold, but I believe \\(Q\\) being symmetric is also needed.\nIf this infimum statement does truly hold, then since the infimum is a lower bound, then for any \\(\\lVert x \\rVert = 1\\), we get the inequality \\[x^TQx \\ge R_*\\] and since \\(\\lVert x \\rVert^2 = 1\\), we also have \\[x^TQx \\ge R_* \\lVert x \\rVert^2.\\]\nThen take some arbitrary \\(x \\in \\mathbb R^n \\setminus \\overline{B}(0,1)\\), that is, \\(\\lVert x \\rVert &gt; 1.\\) Scaling down \\(x\\), we see that \\(\\frac{x}{\\lVert x \\rVert}\\) is on the sphere of radius 1. So it holds that \\[\\frac{x}{\\lVert x \\rVert}^T Q \\frac{x}{\\lVert x \\rVert} \\ge R_* \\lVert \\frac{x}{\\lVert x \\rVert} \\rVert^2.\\] Then multiplying through by \\(\\lVert x \\rVert^2\\), we see that \\[x^T Q x \\ge R_* \\lVert x \\rVert^2.\\] We have thus shown that with \\(c := R_*\\) and \\(r = 1\\), then for all \\(\\lVert x \\rVert \\ge r\\), we have \\(P(x) = x^T Qx \\ge c \\lVert x \\rVert^2,\\) the desired conclusion.\nHere are some other observations that don’t fit anywhere yet:\n\nRandom Oberservations\nObservation 1: \\(c\\lVert x \\rVert^2 = cx_1^2 + \\cdots + cx_n^2.\\)\nObservation 2: Consider the expression \\(x^TQx\\) as being built of \\[x^TQx = \\text{diagonal}(Q,x) + \\text{off-diagonal}(Q,x),\\] where \\[\\text{diagonal}(Q,x) = Q_{11}x_1^2 + \\cdots + Q_{nn} x_n^2\\] and \\[\\text{off-diagonal}(Q,x) = \\sum_{i \\ne j} Q_{ij}x_ix_j.\\]\nObservation 3: I notice that the diagonal entries of \\(Q\\) must be positive. Here’s why. If \\(e_i = (0, \\cdots, 1, \\cdots, 0) \\in \\mathbb R^n\\) is the vector with \\(0\\) at every index besides the \\(i\\)th index, then \\(Q_{ii} = e_i^{T} Q e_i &gt; 0\\), by positive definiteness.\n\n\nFurther Discussion\nI wish I had time to graph some of the simpler cases. For instance, the case of \\(\\mathbb R^n = \\mathbb R^2\\), where we can name the variables \\(x\\) and \\(y\\), and then we can see the conic sections. Then the off diagonal part looks like \\[\\begin{align*}\n\\text{off-diagonal}(Q,(x,y)) = Q_{12}xy + Q_{21}xy,\n\\end{align*}\\] which, when plotted as the graph of the function \\(z(x,y) = (Q_{12}+Q_{21})xy\\) in \\(\\mathbb R^3\\), looks like a hyperbolic parabaloid. The diagonal part, being a quadratic form with positive coefficients, is always positive, and then \\(x^TQx&gt; 0\\) takes the geometric meaning that the negaative part of the hyperbolic parabaloid is dominated by the diagonal part.\n\n\nStill Needed\nNext I need to show that \\(Q\\) being positive definite (and symmetric) implies that \\[R_* := \\inf_{\\lVert x \\rVert = 1} x^TQx &gt; 0.\\]\nThen I need to tackle the case where the linear part \\(v^Tx\\) is non zero and the constant \\(C\\) is also non-zero.\n\n\n\n\n\n\nReferences\n\nBrennecke, Christian. 2026. “Introduction to Constructive Quantum Field Theory.” Lecture Notes. https://www.iam.uni-bonn.de/users/brennecke/home.\n\n\nChatterjee, Sourav. 2016. “The Leading Term of the Yang-Mills Free Energy.”\n\n\nHerbert Amann, Joachim Escher. 2009. Analysis III. Birkhäuser.\n\n\nTao, Terence. 2006. Analysis 2. Hindustan."
  },
  {
    "objectID": "phone-notes.html",
    "href": "phone-notes.html",
    "title": "Twitter-like feed",
    "section": "",
    "text": "January 6, 2025\nWhen the coupling constant of U(N) lattice gauge theory is small, then beta is big. When beta is big, the format of the Yang mills probability density forces the Wilson action to be very small if there is to be any probability mass associated to a given configuration. The Wilson action being small means that most unitary matrices are equal to the identity matrix. Matrices mostly being equal to identity matrices implies that parallel transporting along plaquettes does not do anything. So the gauge field has no effect. I think that the gauge field having little effect and the coupling constant being small are related concepts. By analogy, smallness of the coupling constant for gravity means that gravity has little role.\nI was previously confused because I noticed that when the coupling constant is large, then the Yang Mills gauge measure is just a product Haar measure. I figured product Haar measure means that there is no interaction, a contradiction to the coupling constant being large. Perhaps the reason that this is not a contradiction is that unitary matrices under Haar measure are not concentrated near the identity matrix, so during parallel transport, the gauge field plays a bigger role. And the gauge field taking stronger role fits with the coupling constant being larger.\n\n\nDecember 24, 2025\nConditional probabilities are where you fix some prior knowledge/variables and then you ask about the probability for the remaining variables.\nThat seems like the right framework for axial gauge fixing. You fix the variables for the tree edges, then figure out the rest of the variables.\n\n\nDecember 25, 2025\nThe log beta term in the formula for the free energy is related to the 1/beta smallness of the Wilson action.\nAlso, even though the math of cqft is very eclectic and broad, it also feels like a niche field. At least I get that feeling when looking at one of the early papers by Fröhlich and Brydges."
  },
  {
    "objectID": "KleinGordon.html",
    "href": "KleinGordon.html",
    "title": "Wightman Axioms for free scalar quantum field",
    "section": "",
    "text": "Article last updated: January 16, 2026\nI’m writing this article so that I can learn Theorem 3.1 of Prof. Christian Brennecke’s constructive quantum field theory notes Brennecke (2026). The statement of the theorem is as follows.\n\n\n\n\n\n\nNoteTheorem 3.1\n\n\n\nConsider the strongly continuous unitary representation \\(\\Gamma(U)\\) (gamma of U) of \\(\\mathcal{P}_{+}^{\\uparrow}\\) (P plus up) on \\(\\mathcal{H} = \\mathcal{F}_s(L^2(S_m^+))\\) (Bosonic Fock space built over \\(L^2\\) of the mass shell of mass \\(m\\)) with Fock space vacuum \\(\\Omega = (1,0,0,\\cdots) \\in \\mathcal{H}\\) and let \\(\\phi = (\\phi(f))_{f \\in \\mathcal{S}(\\mathbb R^4)}\\) (family of operators indexed by Schwartz functions on \\(\\mathbb R^4\\)) be defined as (3.14): \\[\\begin{equation}\n\\phi(f) = (2\\pi)^{1/2} \\int_{S_m^+} \\lambda_m(dp) \\tilde{f}(p) a_p + (2\\pi)^{1/2} \\int_{S_m^+} \\lambda_m(dp) \\tilde{f}(p) a_p^*.\n\\end{equation}\\] Then \\((\\mathcal{H}, \\Gamma(U), \\Omega, \\phi)\\) satisfies the Wightman axioms and \\[\\begin{equation}\n(\\square + m^2) \\phi = 0\n\\end{equation}\\] in the sense of operator-valued distributions.\n\n\n\nThe operator valued distribution (3.14)\nLet me clarify (3.14) for myself. Recall that the creation and annihilation operators depend on some function \\(f \\in L^2(\\mathcal{M})\\), where in this case \\(\\mathcal{M} = S_m^+\\). Only once you have some \\(f\\) can you talk about an operator from the infinite tensor product space \\(\\mathcal{F}_s\\) to itself. That is why I can write \\[\\begin{equation}\na(\\sqrt{ 2 \\pi } \\tilde{f} ) : \\mathcal{F}_s \\to \\mathcal{F}_s,\n\\end{equation}\\] and this object is defined in section 2.4 in Brennecke (2026).\nNext, I want to phrase (3.14) directly in terms of rigorously defined objects. I’ll use the identities \\[\\begin{equation}\na(f) =: \\int_{\\mathbb R^d} dx \\overline{f}(x) a_x\n\\end{equation}\\] and \\[\\begin{equation}\na^*(g) =: \\int_{\\mathbb R^d} dx g(x) a_x^*\n\\end{equation}\\] found in section 2.4 of Brennecke’s notes. It follows that \\[\\begin{equation}\n(2\\pi)^{1/2} \\int_{S_m^+} \\lambda_m(dp) \\tilde{f}(p) a_p := a(\\sqrt{2\\pi} \\overline{\\tilde{f}}).\n\\end{equation}\\] This is nice because now I can just refer back to section 2.4 to see how \\(a(\\sqrt{2\\pi} \\overline{\\tilde{f}})\\) acts on a tensor product on some dense subset in the Fock space.\nIn summary, (3.14) says \\[\n\\phi(f) = \\sqrt{2 \\pi } a(\\overline{\\tilde{f}}) + \\sqrt{2 \\pi } a^*( \\tilde{f}).\n\\]\n\n\nThe unitary representation Gamma\nConsider the phrase “strongly continuous unitary representation \\(\\Gamma(U)\\) of \\(\\mathcal{P}_+^\\uparrow\\) on \\(\\mathcal{H} = \\mathcal{F}_s(L^2(S_m^+))\\).” We understand this phrase through the following discussion. Given an abstract group \\(G\\) and a vector space \\(V\\), a unitary representation (of \\(G\\) on \\(V\\)) is a homomorphism \\(\\rho: G \\to \\mathcal{U}(V)\\), where \\(\\mathcal{U}(V)\\) denotes the set of all unitary maps from \\(V\\) to \\(V\\), equipped with composition as the multiplication operation (or matrix multiplication). What is a natural topology to put on \\(\\mathcal{U}(V)\\)? Perhaps an operator topology?\nEquipping \\(G\\) with a topology, we say that \\(\\rho\\) is strongly continuous if for all \\(v \\in V\\) \\[G \\ni g \\mapsto \\rho(g)v \\in V\\] is continuous. For example, if \\(G = SU(2)\\) and \\(\\iota : SU(2) \\to U(2)\\) is a strongly continuous unitary representation of \\(SU(2)\\) on \\(\\mathbb C^2\\), then strong continuity means \\[SU(2) \\ni A \\mapsto Av \\in \\mathbb C^2\\] is continuous for all \\(v \\in \\mathbb C^2\\).\nThus, \\(\\Gamma(U): \\mathcal{P}_+^\\uparrow \\to \\mathcal{U}(\\mathcal{F}_s(L^2(S_m^+)))\\) should satisfy \\[\\mathcal{P}_+^\\uparrow \\ni (a,L) \\mapsto \\Gamma(U)((a,L))v \\in \\mathcal{F}_s(L^2(S_m^+))\\] is continuous, where \\(v\\) is a fixed and arbitrary element of the Bosonic Fock space built over the mass shell. Observe that \\(\\Gamma(U)((a,L))\\) is a unitary operator on Bosonic Fock space, so it makes sense that applying it to \\(v\\) gives a Bosonic function. Observe that \\(\\Gamma(U)\\) is not a finite dimensional representation because Fock space is not finite dimensional.\nIt turns out that the unitary representation Gamma actually has a name: the second quantization of the unitary representation \\(U= (U(a,L))_{(a,L) \\in \\mathcal{P}_+^{\\uparrow}}\\). Thus, completely understanding \\(\\Gamma(U)\\) entails understanding the representation \\(U\\) of the proper Poincaré group \\(\\mathcal{P}_+^{\\uparrow}\\) on \\(L^2\\) of the mass shell \\(S_m^+\\), which can be seen in definition (3.4) in the section on the quantization of a massive relativistic particle. To this end, let me look at the following exercise, which is Probem 3.1 (a) in Brennecke (2026).\n\n\n\n\n\n\nNoteExercise 1\n\n\n\nFor \\((a,L) \\in \\mathcal{P}_+^{\\uparrow}\\), let \\(U(a,L)\\) be defined as in (3.4): for \\(\\psi \\in L^2(S_m^+, \\mathcal{B}(S_m^+), \\lambda_m)\\), we set \\[\n(U(a,L)\\psi)(p) = e^{ip^{\\mu}a_{\\mu}} \\psi(L^{-1}p)\n\\] for almost every \\(p \\in S_m^+\\) and for all \\((a,L) \\in \\mathcal{P}_+^{\\uparrow}\\). Show that \\(\\mathcal{P}_+^\\uparrow \\ni (a,L) \\mapsto U(a,L) \\in \\mathcal{U}(L^2(S_m^+))\\) defines a strongly continuous, unitary representation of \\(\\mathcal{P}_+^\\uparrow\\) on \\(L^2(S_m^+) = L^2(S_m^+, \\mathcal{B}(S_m^+), \\lambda_m)\\).\n\n\nProof. Step one is to verify the homomorphism property \\[\n(U((a,L)(a',L'))\\psi)(p) = ((U(a,L)U(a',L'))\\psi)(p)\n\\tag{1}\\] for all \\((a,L), (a',L') \\in \\mathcal{P}_+^\\uparrow\\). By definition, \\((a,L)(a',L') = (a+La', LL')\\), so we make the appropriate substitution on the left hand side of Equation 1. Then we have by (3.4) that \\[\\begin{align*}\n(U(a+La', LL')\\psi)(p) = e^{ip^\\mu(a+La')_{\\mu}}\\psi((LL')^{-1}p).\n\\end{align*}\\] We compute for the right hand side using (3.4): \\[\\begin{align*}\n((U(a,L)U(a',L'))\\psi)(p) &= (U(a,L)(U(a',L')\\psi(\\cdot)))(p) && \\text{i.e. composition law}\\\\\n&= (U(a,L)e^{i(\\cdot)^\\mu a'_{\\mu}}\\psi(L'^{-1} (\\cdot)) )(p) \\\\\n&= e^{ip^\\mu a_{\\mu}} e^{i(L^{-1}p)^\\mu a'_{\\mu}} \\psi(L'^{-1}L^{-1}(p)) \\\\\n&= e^{i (p^\\mu a_\\mu + (L^{-1}p)^\\mu a'_\\mu)} \\psi((LL')^{-1}p).\n\\end{align*}\\] For the homomorphism property, it remains to prove that \\[\np^\\mu a_\\mu + (L^{-1}p)^\\mu a'_\\mu = p^\\mu(a+La')_{\\mu}.\n\\] Note the following facts and observations about\n\nInterlude about O(1,3)\n\\(O(1,3)\\), the group of isometries on Minkowski space, of which \\(L\\) is an element by defintion of the Poincaré group. First, Minkowski space \\(\\mathbb R^4\\) is equipped with a bilinear form \\(\\eta = \\text{diag}({1,-1,-1,-1})\\), which means that \\(\\eta(x,y) = x^T \\eta y\\), where \\(x\\) and \\(y\\) are column vectors with 4 entries. Next, an isometry \\(L \\in O(1,3)\\) is a linear map \\(\\mathbb R^4 \\to \\mathbb R^4\\) that preserves the geometry in the sense that \\(\\eta(Lx,Ly) = \\eta(x,y)\\) for all \\(x,y \\in \\mathbb R^4\\). Rewriting gives the equation \\(x^TL^T\\eta Ly = x^T\\eta y\\) for all \\(x\\) and \\(y\\), and by choosing \\(x\\) and \\(y\\) as the various combinations of basis vectors in order to single out the matrix entries of \\(L^T\\eta L\\) on the left and \\(\\eta\\) on the right, the isometry condition on \\(L\\) can be expressed equivalently as \\(L^T \\eta L = \\eta\\). Another useful thing to note is that \\(a^\\mu b_\\mu = a^T \\eta b\\) for column vectors \\(a\\) and \\(b\\) in \\(\\mathbb R^4\\).\n\n\nEnd of Interlude, back to the proof.\nLet’s rewrite the equation \\(p^\\mu a_\\mu + (L^{-1}p)^\\mu a'_\\mu = p^\\mu(a+La')_{\\mu}\\) using \\(\\eta\\) as \\[\np^T \\eta a + (L^{-1}p)^T \\eta a' = p^T \\eta (a+La').\n\\] So we want to prove that this equation holds, given that \\(L \\in O(1,3)\\) and that \\(p \\in S_m^+\\). By linearity and cancellation, the equation becomes \\[\n(L^{-1}p)^T \\eta a' =  p^T \\eta La'.\n\\] But using that \\(L^{-1} \\in O(1,3)\\), which implies that \\((L^{-1})^T \\eta L^{-1} = \\eta\\), which implies \\((L^{-1})^T \\eta = \\eta L\\), gives confirmation. But why is \\(L^{-1} \\in O(1,3)?\\) Well, \\(L \\in\nO(1,3)\\) implies \\(L^T \\eta L = \\eta\\), which implies that \\(\\eta^T = L^T \\eta^T L\\) because \\(\\eta\\) is symmetric, which implies that \\(\\eta^T L^{-1} = L^T \\eta^T\\), which implies that \\((L^{-1})^T \\eta = \\eta L\\) after transposing both sides, which implies that \\((L^{-1})^T \\eta L^{-1}= \\eta\\), which implies that \\(L^{-1} \\in O(1,3)\\). This concludes step one, the homomorphism property of the representation.\nNext, I’d like to check the unitary property, which states that for all \\((a,L) \\in \\mathcal{P}_+^\\uparrow\\), that \\(U(a,L)\\) is a unitary operator on \\(L^2(S_m^+)\\), which means that \\[\n\\langle \\psi, \\phi \\rangle_{L^2(S_m^+)} = \\langle U(a,L) \\psi, U(a,L) \\phi \\rangle_{L^2(S_m^+)}\n\\] for all \\(\\psi, \\phi \\in L^2(S_m^+)\\).\nI’m not super sure if functions in \\(L^2(S_m^+)\\) are supposed to have a codomain of \\(\\mathbb C\\), but I’ll guess that for now based on the imaginary \\(i\\) in the exponent in the formula (3.4).\n\\[\\begin{align*}\n\\langle U(a,L) \\psi, U(a,L) \\phi \\rangle_{L^2(S_m^+)} &= \\int_{S_m^+} d\\lambda_m(p) \\overline{U(a,L)\\psi} (p) U(a,L) \\phi (p)  \\\\\n&= \\int_{S_m^+} d \\lambda_m(p) \\overline{e^{ip^\\mu a_\\mu} \\psi (L^{-1}p)} e^{ip^\\mu a_\\mu} \\phi(L^{-1}p) \\\\\n&= \\int_{S_m^+} d \\lambda_m(p)  e^{-ip^\\mu a_\\mu} \\overline{ \\psi (L^{-1}p)} e^{ip^\\mu a_\\mu} \\phi(L^{-1}p) \\\\\n&= \\int_{S_m^+} d \\lambda_m(p) \\overline{ \\psi (L^{-1}p)} \\phi(L^{-1}p) \\\\\n&= \\int_{L^{-1}S_m^+} d (\\lambda_m \\circ L)(h) \\overline {\\psi(h)} \\phi(h) && \\text{ change of variables }\\\\\n&= \\int_{S_m^+} d \\lambda_m (h) \\overline {\\psi(h)} \\phi(h) && \\text{ Lemma 3.1 of Brennecke's notes} \\\\\n&= \\langle \\psi, \\phi \\rangle_{L^2(S_m^+)}.\n\\end{align*}\\]\nHere is why \\(L^{-1}S_m^+ = S_m^+\\). The definition of the mass shell is \\(S_m^+ = \\{p \\in \\mathbb R^4: p^T\\eta p = m^2, p_0 &gt; 0\\}\\). Begin with an element \\(a := L^{-1}p \\in L^{-1}S_m^+\\). Then \\(L^{-1} \\in \\mathcal{P}_+^\\uparrow\\) implies that \\(L^{-1}_{00} &gt; 0\\) and also \\(p_0 &gt; 0\\),\n\n\nInterlude on proper Poincaré group\nFirst, it will be helpful to understand the following statement found on page 46 of Brennecke (2026): If \\(L \\in O(1,3)\\), then \\[\n\\eta^{\\mu \\nu} L_{\\mu \\lambda} L_{\\nu \\kappa} =\n\\eta_{\\lambda \\kappa} = \\eta^{\\mu \\nu } L_{\\lambda \\mu} L_{\\kappa \\nu}.\n\\] The conclusion uses the Einstein summation convention. As a simple example of that, consider two matrices \\(A \\in \\mathbb F^{m \\times n}\\) and \\(B \\in \\mathbb F^{n \\times p}\\). Then the formula for the \\((\\lambda,\\kappa)\\) entry, or in other words, the \\(\\lambda\\)-th row and \\(\\kappa\\)-th column of the product \\(AB\\) is \\(AB_{\\lambda \\kappa} = \\sum_{\\nu = 1}^n A^{\\lambda \\nu}B_{\\nu \\kappa} = A^{\\lambda \\nu} B_{\\nu \\kappa}\\), because entries that appear once up high and once down low in the symbolism are taken to be summed over.\nTo guide myself, let me ask: does \\((L^T \\eta L)_{\\lambda \\kappa} = \\eta^{\\mu \\nu} L_{\\mu \\lambda} L_{\\nu \\kappa}\\)? We have \\[\\begin{align*}\n(L^T \\eta L)_{\\lambda \\kappa} &= ((L^T \\eta) L)_{\\lambda \\kappa}\\\\\n&= (L^T \\eta)^{\\lambda \\nu} L_{\\nu \\kappa} \\\\\n&= ((L^T)^{\\lambda \\mu} \\eta_{\\mu \\nu}) L_{\\nu \\kappa} \\\\\n&= (L^{\\mu \\lambda} \\eta_{\\mu \\nu}) L_{\\nu \\kappa} && \\text{transpose def} \\\\\n&= (L^{\\mu \\lambda} \\eta_{\\nu \\mu}) L_{\\nu \\kappa} && \\eta \\text{ symmetric} \\\\\n&= (\\eta_{\\nu \\mu} L^{\\mu \\lambda}) L_{\\nu \\kappa} && \\text{ multiplication commutative} \\\\\n&= \\eta^{\\nu \\mu} L_{\\mu \\lambda} L_{\\nu \\kappa},\n\\end{align*}\\] so the answer is yes.\n\n\n\n\n\n\n\nReferences\n\nBrennecke, Christian. 2026. “Introduction to Constructive Quantum Field Theory.” Lecture Notes. https://www.iam.uni-bonn.de/users/brennecke/home."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website was created on December 9, 2025 to host stuff about my Master’s thesis on and (hopefully) post-Master’s exploration of YM lattice gauge theory, or more generally, the Yang Mills Millenium problem."
  },
  {
    "objectID": "about.html#some-developer-details",
    "href": "about.html#some-developer-details",
    "title": "About",
    "section": "Some developer details:",
    "text": "Some developer details:\nThis website is built using Quarto.\n\nHere are the instructions on how to render to github pages: https://quarto.org/docs/publishing/github-pages.html#render-to-docs.\non how to implement page navigation: https://quarto.org/docs/websites/website-navigation.html\n\nHere is a guide on citations using biblatex: https://www.overleaf.com/learn/latex/Bibliography_management_with_biblatex."
  }
]